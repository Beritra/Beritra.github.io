{"meta":{"title":"Beritra","subtitle":"Blogs","description":"Beritra's Blogs","author":"Beritra","url":"http://Beritra.github.com","root":"/"},"pages":[{"title":"分类","date":"2019-12-14T21:37:39.000Z","updated":"2019-12-15T14:42:02.136Z","comments":true,"path":"categories/index.html","permalink":"http://beritra.github.com/categories/index.html","excerpt":"","text":""},{"title":"分类","date":"2017-02-24T16:28:56.000Z","updated":"2019-12-14T21:39:54.400Z","comments":false,"path":"tags/index.html","permalink":"http://beritra.github.com/tags/index.html","excerpt":"","text":""},{"title":"关于我","date":"2017-02-24T16:28:26.000Z","updated":"2019-12-14T21:40:10.048Z","comments":true,"path":"about/index.html","permalink":"http://beritra.github.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"ConcurrentHashMap使用和原理","slug":"ConcurrentHashMap使用和原理","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:30:04.633Z","comments":true,"path":"2019/12/01/ConcurrentHashMap使用和原理/","link":"","permalink":"http://beritra.github.com/2019/12/01/ConcurrentHashMap%E4%BD%BF%E7%94%A8%E5%92%8C%E5%8E%9F%E7%90%86/","excerpt":"ConcurrentHashMap是在JDK1.5时，J.U.C引入的一个同步集合工具类，顾名思义，这是一个线程安全的HashMap。不同版本的ConcurrentHashMap，内部实现机制千差万别，本文所有的讨论基于JDK1.8。","text":"ConcurrentHashMap是在JDK1.5时，J.U.C引入的一个同步集合工具类，顾名思义，这是一个线程安全的HashMap。不同版本的ConcurrentHashMap，内部实现机制千差万别，本文所有的讨论基于JDK1.8。 基本结构 ConcurrentHashMap内部维护了一个Node类型的数组，也就是table\\： 1transient volatile Node[] table; 数组的每一个位置table[i]代表了一个桶，当插入键值对时，会根据键的hash值映射到不同的桶位置，table一共可以包含4种不同类型的桶：Node、TreeBin、ForwardingNode、ReservationNode。上图中，不同的桶用不同颜色表示。可以看到，有的桶链接着链表，有的桶链接着树。 TreeBin所链接的是一颗红黑树，红黑树的结点用TreeNode表示，所以ConcurrentHashMap中实际上一共有五种不同类型的Node结点。 之所以用TreeBin而不是直接用TreeNode，是因为红黑树的操作比较复杂，包括构建、左旋、右旋、删除，平衡等操作，用一个代理结点TreeBin来包含这些复杂操作，其实是一种“职责分离”的思想。另外TreeBin中也包含了一些加/解锁的操作。 节点定义Node节点Node结点的定义非常简单，也是其它四种类型结点的父类。 默认链接到table[i]——桶上的结点就是Node结点。当出现hash冲突时，Node结点会首先以链表的形式链接到table上，当结点数量超过一定数目时，链表会转化为红黑树。因为链表查找的平均时间复杂度为O(n)，而红黑树是一种平衡二叉树，其平均时间复杂度为O(logn)。 TreeNode结点TreeNode就是红黑树的结点，TreeNode不会直接链接到table[i]——桶上面，而是由TreeBin链接，TreeBin会指向红黑树的根结点。 TreeBin结点TreeBin相当于TreeNode的代理结点。TreeBin会直接链接到table[i]——桶上面，该结点提供了一系列红黑树相关的操作，以及加锁、解锁操作。 ForwardingNode结点ForwardingNode结点仅仅在扩容时才会使用 ReservationNode结点保留结点，ConcurrentHashMap中的一些特殊方法会专门用到该类结点。 常量定义1/**2 * 最大容量.3 */4private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;56/**7 * 默认初始容量8 */9private static final int DEFAULT_CAPACITY = 16;1011/**12 * The largest possible (non-power of two) array size.13 * Needed by toArray and related methods.14 */15static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;1617/**18 * 负载因子，为了兼容JDK1.8以前的版本而保留。19 * JDK1.8中的ConcurrentHashMap的负载因子恒定为0.7520 */21private static final float LOAD_FACTOR = 0.75f;2223/**24 * 链表转树的阈值，即链接结点数大于8时， 链表转换为树.25 */26static final int TREEIFY_THRESHOLD = 8;2728/**29 * 树转链表的阈值，即树结点树小于6时，树转换为链表.30 */31static final int UNTREEIFY_THRESHOLD = 6;3233/**34 * 在链表转变成树之前，还会有一次判断：35 * 即只有键值对数量大于MIN_TREEIFY_CAPACITY，才会发生转换。36 * 这是为了避免在Table建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。37 */38static final int MIN_TREEIFY_CAPACITY = 64;3940/**41 * 在树转变成链表之前，还会有一次判断：42 * 即只有键值对数量小于MIN_TRANSFER_STRIDE，才会发生转换.43 */44private static final int MIN_TRANSFER_STRIDE = 16;4546/**47 * 用于在扩容时生成唯一的随机数.48 */49private static int RESIZE_STAMP_BITS = 16;5051/**52 * 可同时进行扩容操作的最大线程数.53 */54private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;5556/**57 * The bit shift for recording size stamp in sizeCtl.58 */59private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;6061static final int MOVED = -1; // 标识ForwardingNode结点（在扩容时才会出现，不存储实际数据）62static final int TREEBIN = -2; // 标识红黑树的根结点63static final int RESERVED = -3; // 标识ReservationNode结点（）64static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash6566/**67 * CPU核心数，扩容时使用68 */69static final int NCPU = Runtime.getRuntime().availableProcessors(); 字段定义1/**2 * Node数组，标识整个Map，首次插入元素时创建，大小总是2的幂次.3 */4transient volatile Node&lt;K, V&gt;[] table;56/**7 * 扩容后的新Node数组，只有在扩容时才非空.8 */9private transient volatile Node&lt;K, V&gt;[] nextTable;1011/**12 * 控制table的初始化和扩容.13 * 0 : 初始默认值14 * -1 : 有线程正在进行table的初始化15 * &gt;0 : table初始化时使用的容量，或初始化/扩容完成后的threshold16 * -(1 + nThreads) : 记录正在执行扩容任务的线程数17 */18private transient volatile int sizeCtl;1920/**21 * 扩容时需要用到的一个下标变量.22 */23private transient volatile int transferIndex;2425/**26 * 计数基值,当没有线程竞争时，计数将加到该变量上。类似于LongAdder的base变量27 */28private transient volatile long baseCount;2930/**31 * 计数数组，出现并发冲突时使用。类似于LongAdder的cells数组32 */33private transient volatile CounterCell[] counterCells;3435/**36 * 自旋标识位，用于CounterCell[]扩容时使用。类似于LongAdder的cellsBusy变量37 */38private transient volatile int cellsBusy;394041// 视图相关字段42private transient KeySetView&lt;K, V&gt; keySet;43private transient ValuesView&lt;K, V&gt; values;44private transient EntrySetView&lt;K, V&gt; entrySet; put操作put方法内部调用了putVal这个私有方法 putVal的逻辑还是很清晰的，首先根据key计算hash值，然后通过hash值与table容量进行运算，计算得到key所映射的索引——也就是对应到table中桶的位置。 这里需要注意的是计算索引的方式：i = (n - 1) &amp; hash n - 1 == table.length - 1，table.length 的大小必须为2的幂次的原因就在这里。 读者可以自己计算下，当table.length为2的幂次时，(table.length-1)的二进制形式的特点是除最高位外全部是1\\，配合这种索引计算方式可以实现key在table中的均匀分布，减少hash冲突——出现hash冲突时，结点就需要以链表或红黑树的形式链接到table[i]，这样无论是插入还是查找都需要额外的时间。 putVal方法一共处理四种情况： 1、首次初始化table-懒加载ConcurrentHashMap在构造的时候并不会初始化table数组，首次初始化就在这里通过initTable方法完成。 2、table[i]对应的桶为空最简单的情况，直接CAS操作占用桶table[i]即可。 3、发现ForwardingNode结点，说明此时table正在扩容，则尝试协助进行数据迁移ForwardingNode结点是ConcurrentHashMap中的五类结点之一，相当于一个占位结点，表示当前table正在进行扩容，当前线程可以尝试协助数据迁移。 4、出现hash冲突,也就是table[i]桶中已经有了结点当两个不同key映射到同一个table[i]桶中时，就会出现这种情况： 当table[i]的结点类型为Node——链表结点时，就会将新结点以“尾插法”的形式插入链表的尾部。 当table[i]的结点类型为TreeBin——红黑树代理结点时，就会将新结点通过红黑树的插入方式插入。 putVal方法的最后，涉及将链表转换为红黑树 —— treeifyBin ，但实际情况并非立即就会转换，当table的容量小于64时，出于性能考虑，只是对table数组扩容1倍——tryPresize get操作get方法的逻辑很简单，首先根据key的hash值计算映射到table的哪个桶——table[i]。 如果table[i]的key和待查找key相同，那直接返回； 如果table[i]对应的结点是特殊结点（hash值小于0），则通过find方法查找； 如果table[i]对应的结点是普通链表结点，则按链表方式查找。 第二种情况下，table[i]是不同的节点，处理方式也会不同： Node结点的查找当槽table[i]被普通Node结点占用，说明是链表链接的形式，直接从链表头开始查找。 TreeBin结点的查找TreeBin的查找比较特殊，我们知道当槽table[i]被TreeBin结点占用时，说明链接的是一棵红黑树。由于红黑树的插入、删除会涉及整个结构的调整，所以通常存在读写并发操作的时候，是需要加锁的。 ConcurrentHashMap采用了一种类似读写锁的方式：当线程持有写锁（修改红黑树）时，如果读线程需要查找，不会像传统的读写锁那样阻塞等待，而是转而以链表的形式进行查找（TreeBin本身时Node类型的子类，所有拥有Node的所有字段） ForwardingNode结点的查找ForwardingNode是一种临时结点，在扩容进行中才会出现，所以查找也在扩容的table上进行 ReservationNode结点的查找ReservationNode是保留结点，不保存实际数据，所以直接返回null ConcurrentHashMap的计数size方法内部实际调用了sumCount方法： 1final long sumCount() &#123;2 CounterCell[] as = counterCells;3 CounterCell a;4 long sum = baseCount;5 if (as != null) &#123;6 for (int i = 0; i &lt; as.length; ++i) &#123;7 if ((a = as[i]) != null)8 sum += a.value;9 &#125;10 &#125;11 return sum;12&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"InnoDB和MyISAM区别和联系","slug":"InnoDB和MyISAM区别和联系","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:07:33.040Z","comments":true,"path":"2019/12/01/InnoDB和MyISAM区别和联系/","link":"","permalink":"http://beritra.github.com/2019/12/01/InnoDB%E5%92%8CMyISAM%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB/","excerpt":"s从别人博客抄来的 MySQL 中 InnoDB 引擎与 MyISAM 引擎的区别与联系。","text":"s从别人博客抄来的 MySQL 中 InnoDB 引擎与 MyISAM 引擎的区别与联系。 InnoDB：MySQL默认的事务型引擎，也是最重要和使用最广泛的存储引擎。它被设计成为大量的短期事务，短期事务大部分情况下是正常提交的，很少被回滚。InnoDB的性能与自动崩溃恢复的特性，使得它在非事务存储需求中也很流行。除非有非常特别的原因需要使用其他的存储引擎，否则应该优先考虑InnoDB引擎。 MyISAM：在MySQL 5.1 及之前的版本，MyISAM是默认引擎。MyISAM提供的大量的特性，包括全文索引、压缩、空间函数（GIS）等，但MyISAM并不支持事务以及行级锁，而且一个毫无疑问的缺陷是崩溃后无法安全恢复。正是由于MyISAM引擎的缘故，即使MySQL支持事务已经很长时间了，在很多人的概念中MySQL还是非事务型数据库。尽管这样，它并不是一无是处的。对于只读的数据，或者表比较小，可以忍受修复操作，则依然可以使用MyISAM（但请不要默认使用MyISAM，而是应该默认使用InnoDB） 1、 存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。分别为：表定义文件、数据文件、索引文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。 InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 2、 存储空间MyISAM： MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。 InnoDB： 需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 3、 可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。 InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。 4、 事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 5、 AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 6、 表锁差异MyISAM： 只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB： 支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 7、 全文索引MySql全文索引 MyISAM：支持 FULLTEXT类型的全文索引 InnoDB：不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 8、表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 9、表的具体行数MyISAM： 保存有表的总行数，如果select count() from table;会直接取出出该值。 InnoDB： 没有保存表的总行数，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 10、CRUD操作MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。 InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。 11、 外键MyISAM：不支持 InnoDB：支持 参考文章： Mysql 中 MyISAM 和 InnoDB 的区别有哪些？","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"离线环境下安装高可用FastDFS","slug":"FastDFS安装文档","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:40:24.712Z","comments":true,"path":"2019/12/01/FastDFS安装文档/","link":"","permalink":"http://beritra.github.com/2019/12/01/FastDFS%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3/","excerpt":"记录一次离线环境下安装高可用 FastDFS 的过程 本文大部分内容摘抄至《MySQL技术内幕（InnoDB存储引擎）第二版》，小部分来源于自己理解和网络博客。","text":"记录一次离线环境下安装高可用 FastDFS 的过程 本文大部分内容摘抄至《MySQL技术内幕（InnoDB存储引擎）第二版》，小部分来源于自己理解和网络博客。 FastDFS安装文档需准备2台机器（安装Centos7操作系统），A,B A：安装tracker，storage（group1），nginx B：安装tracker，storage（group2），nginx 默认选择A服务器的tracker作为leader，tracker为从 1.安装libfastcommon包 解压libfastcommon-1.0.40.tar.gz cd libfastcommon-1.0.40 | ./make.sh | ./make.sh install ibfastcommon.so 默认安装到了/usr/lib64/libfastcommon.so，但是FastDFS主程序设置的lib目录是/usr/local/lib，所以此处重新设置软链接： ​ ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so 2.安装FastDFS 解压fastdfs-5.11 cd fastdfs-5.11 | ./make.sh | ./make.sh install ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so cd /etc/fdfs 对三个配置文件进行拷贝,备用 cp client.conf.sample client.conf cp storage.conf.sample storage.conf cp tracker.conf.sample tracker.conf 进入 fastdfs-5.11/conf目录,复制以下文件，http.conf和mime.types为了跟nginx模块整合使用，client.conf是fastdfs的客户端测试文件 cp http.conf /etc/fdfs/ cp mime.types /etc/fdfs/ cp client.conf /etc/fdfs/ 3.安装tracker 新建tracker的目录数据文件和日志文件 mkdir /home/face/FastDFS/tracker 编辑tracker配置文件/etc/fdfs/tracker.conf,主要配置参数如下： port=22122 base_path= /home/face/FastDFS/tracker work_thread=4 #最好和cpu核数保持一致 store_lookup=0 #选择上传文件模式 0代表group轮询 1指定特定group 2选择空间最大的group 通过命令启动tracker: systemctl start fdfs_trackerd 查看/home/face/fastDFS/tracker目录下的data和logs目录，看是否启动成功 通过netstat命令查看端口监听情况：netstat -unltp|grep fdfs 检测22122 4.安装storage 新建storage的目录数据文件和日志文件 mkdir /home/face/FastDFS/storage | mkdir /home/face/FastDFS/storage/data 编辑storage配置文件/etc/fdfs/storage.conf，主要配置参数如下： A机器：group_name=group1 B机器：group_name=group2 port=23000 base_path=/home/face/FastDFS/storage store_path_count=1 store_path0=/home/face/FastDFS/storage/data tracker_server=A机器ip:22122 tracker_server=B机器ip:22122 通过命令启动storage: systemctl start fdfs_storaged 查看/home/face/fastDFS/storage目录下的data和logs目录，看是否启动成功 看storage服务器是否已经登记到 tracker服务器，运行以下命令：/usr/bin/fdfs_monitor /etc/fdfs/storage.conf 5.安装fastdfs-nginx-module上面的4个步骤已经可以上传和下载文件，但是fastdfs自己提供的http服务比较简洁，而且会出现数据同步不及时导致的读取数据不存在的情况，所以需要安装nginx模块来解决这个问题。 解压fastdfs-nginx-module-master包 cd fastdfs-nginx-module/src 复制文件到/etc/fdfs/目录下， cp mod_fastdfs.conf /etc/fdfs/ 进入/etc/fdfs/,编辑mod_fastdfs.conf,主要配置参数如下： base_path=/home/face/FastDFS/storage tracker_server=A机器ip:22122 tracker_server=B机器ip:22122 A机器：group_name=group1 B机器：group_name=group2 url_have_group_name = true store_path_count=1 store_path0=/home/face/FastDFS/storage/data group_count = 2 [group1]group_name=group1storage_server_port=23000store_path_count=1store_path0=/home/face/FastDFS/storage/data[group2]group_name=group2storage_server_port=23000store_path_count=1store_path0=/home/face/FastDFS/storage/data 进入fastdfs-nginx-module/src目录，编辑conf文件，修改如下： 1ngx_addon_name&#x3D;ngx_http_fastdfs_module23if test -n &quot;$&#123;ngx_module_link&#125;&quot;; then4 ngx_module_type&#x3D;HTTP5 ngx_module_name&#x3D;$ngx_addon_name6 ngx_module_incs&#x3D;&quot;&#x2F;usr&#x2F;include&#x2F;fastdfs &#x2F;usr&#x2F;include&#x2F;fastcommon&#x2F;&quot;7 ngx_module_libs&#x3D;&quot;-lfastcommon -lfdfsclient&quot;8 ngx_module_srcs&#x3D;&quot;$ngx_addon_dir&#x2F;ngx_http_fastdfs_module.c&quot;9 ngx_module_deps&#x3D;10 CFLAGS&#x3D;&quot;$CFLAGS -D_FILE_OFFSET_BITS&#x3D;64 -DFDFS_OUTPUT_CHUNK_SIZE&#x3D;&#39;256*1024&#39; -DFDFS_MOD_CONF_FILENAME&#x3D;&#39;\\&quot;&#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf\\&quot;&#39;&quot;11 . auto&#x2F;module12else13 HTTP_MODULES&#x3D;&quot;$HTTP_MODULES ngx_http_fastdfs_module&quot;14 NGX_ADDON_SRCS&#x3D;&quot;$NGX_ADDON_SRCS $ngx_addon_dir&#x2F;ngx_http_fastdfs_module.c&quot;15 CORE_INCS&#x3D;&quot;$CORE_INCS &#x2F;usr&#x2F;include&#x2F;fastdfs &#x2F;usr&#x2F;include&#x2F;fastcommon&#x2F;&quot;16 CORE_LIBS&#x3D;&quot;$CORE_LIBS -lfastcommon -lfdfsclient&quot;17 CFLAGS&#x3D;&quot;$CFLAGS -D_FILE_OFFSET_BITS&#x3D;64 -DFDFS_OUTPUT_CHUNK_SIZE&#x3D;&#39;256*1024&#39; -DFDFS_MOD_CONF_FILENAME&#x3D;&#39;\\&quot;&#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf\\&quot;&#39;&quot;18fi 6.安装nginx 解压nginx-1.14.2.tar.gz cd nginx-1.14.2 ./configure –prefix=/usr/local/nginx –add-module=/usr/src/fastdfs-nginx-module-master/srcmake &amp;&amp; make install 编辑nginx.conf配置文件，配置如下： 1upstream fdfs_group1 &#123;2 server A:8080 weight&#x3D;1 max_fails&#x3D;2 fail_timeout&#x3D;30s;3&#125; 4 5upstream fdfs_group2 &#123;6 server B:8080 weight&#x3D;1 max_fails&#x3D;2 fail_timeout&#x3D;30s;7&#125; 8 9include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;conf.d&#x2F;*.conf; 创建conf.d文件夹，进入该文件夹，创建tracker.conf和storage.conf 编辑tracker.conf 1server &#123;2 listen 80;3 server_name A;4 5 location ~ &#x2F;group1&#x2F;M00 &#123;6 proxy_next_upstream http_502 http_504 error timeout invalid_header;7 proxy_pass http:&#x2F;&#x2F;fdfs_group1;8 expires 30d;9 &#125;1011 location ~ &#x2F;group2&#x2F;M00 &#123;12 proxy_next_upstream http_502 http_504 error timeout invalid_header;13 proxy_pass http:&#x2F;&#x2F;fdfs_group2;14 expires 30d;15 &#125;16&#125; 编辑stroage.conf A机器： 1server &#123;2 listen 8080;3 server_name A;4 location ~ &#x2F;group1&#x2F;M00 &#123;5 root &#x2F;home&#x2F;face&#x2F;FastDFS&#x2F;storage&#x2F;data;6 index index.html index.htm; 7 ngx_fastdfs_module;8 &#125;9&#125; B机器： 1server &#123;2 listen 8080;3 server_name B;4 location ~ &#x2F;group2&#x2F;M00 &#123;5 root &#x2F;home&#x2F;face&#x2F;FastDFS&#x2F;storage&#x2F;data;6 index index.html index.htm;7 ngx_fastdfs_module;8 &#125;9&#125; 启动nginx：/usr/local/nginx/sbin/nginx 7.测试文件上传，浏览器访问数据 编辑3个文件: echo “1111” &gt;&gt;/opt/1.txt &amp;&amp; echo “2222” &gt;&gt;/opt/2.txt &amp;&amp; echo “3333” &gt;&gt;/opt/3.txt 编辑client.conf文件，配置如下： tracker_server=A机器ip:22122 tracker_server=B机器ip:22122 上传文件: /usr/local/bin/fdfs_test /etc/fdfs/client.conf upload 1.txt /usr/local/bin/fdfs_test /etc/fdfs/client.conf upload 2.txt /usr/local/bin/fdfs_test /etc/fdfs/client.conf upload 3.txt 返回结果中会把上传完成的url 打印出来，复制URL在浏览器打开是否正常","categories":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"FastFDS","slug":"FastFDS","permalink":"http://beritra.github.com/tags/FastFDS/"},{"name":"高可用","slug":"高可用","permalink":"http://beritra.github.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"InnoDB 表结构","slug":"InnoDB表结构","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:39:39.456Z","comments":true,"path":"2019/12/01/InnoDB表结构/","link":"","permalink":"http://beritra.github.com/2019/12/01/InnoDB%E8%A1%A8%E7%BB%93%E6%9E%84/","excerpt":"本文大部分内容摘抄至《MySQL技术内幕（InnoDB存储引擎）第二版》，小部分来源于自己理解和网络博客。","text":"本文大部分内容摘抄至《MySQL技术内幕（InnoDB存储引擎）第二版》，小部分来源于自己理解和网络博客。 索引组织表在InnoDB存储引擎中，表都是根据主键顺序组织存放的，这种存储方式的表成为索引组织表（Index Organized table）。在InnoDB存储引擎表中，每张表都有个主键（Primary Key），如果在创建表的时候没有仙式地定义主键，则InnoDB会按照以下方式选择或创建主键： 首次按判断表中是否有非空的唯一索引（Unique NOT NULL），如果有，则该列为主键。 如果没有，InnoDB存储引擎会自动创建一个6字节大小的指针。 当表中有多个非空唯一索引的时候，InnoDB存储引擎将选择建表的时候第一个定义的非空唯一索引作为主键。注意这里是定义索引的顺序，而不是建表的时候列的顺序。 InnoDB逻辑存储结构从InnoDB存储应引擎的存储结构来看，所有的数据都被逻辑的存放在一个空间中，成为表空间（tablespace）。表空间又由段（segment）、区（extent）、页（page）组成。页在一些文档中也成为块（block），InnoDB存储引擎的逻辑存储结构大致如图： 表空间表空间可以看做是InnoDB存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。默认情况下InnoDB存储引擎有一个共享表空间ibdata1，所有的数据都存放在这个表空间中。如果用户开启了参数innodb_fiule_per_table，则每张表内的数据可以单独放在一个表空间中。 即使启用了innodb_fiule_per_table，每张表的表空间内从存放的也只是数据、索引和插入缓冲Bitmap页，其他类型的数据比如回滚（undo）、插入缓冲索引页、系统事务信息、二次写缓冲（Double write buffer）等还是存放在原有共享表空间。 段上面的图已经显示了表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等。因为前面已经介绍过了InnoDB存储引擎表是索引组织（index organized）的，因此数据即索引，索引即数据。那么数据段即B+树的叶子节点（图中的Leaf node segment），索引段即B+树中的非索引节点（图中的Non-leaf node segment）。回滚段较为特殊。 区区是由连续的页组成的空间，在任何情况下，每个区的大小都为1MB。为了保证区中页的连续性，InnoDB存储引擎一次从磁盘中申请4~5个区。在默认情况下，InnoDB存储引擎页的大小为16KB，即一个区中一共有64个连续的页。 InnoDB 1.0.x 版本开始引入压缩页，即每个夜的大小可以通过KEY_BLOCK_SIZE设置为2K、4K、8K，因此每个区对应页的数量就应该是512、256、128。 InnoDB 1.2.x 版本新增了参数 innodb_page_size，通过该参数可以将默认页的大小设置为4K、8K，但是页中的数据库不是压缩。这是区中页的数量同样页是256、128。总之，不论页的大小怎么变化，区的大小总为1M。 页同大多数数据库一样，InnoDB有页（Page）的概念（也可以称之为块），页是InnoDB磁盘管理的最小单位。 在InnoDB存储引擎中，常见的页类型有： 数据页（B-tree Node） undo页（undo Log Page） 系统页（System Page） 事务数据页（Transaction system Page） 插入缓冲位图页（Insert Buffer Bitmap） 插入缓冲空闲列表页（Inser Buffer Free List） 未压缩的二进制大对象页（Uncompressed BLOB Page） 压缩的二进制大对象页（compressed BLOB Page） 行InnoDB存储引擎是面向列的（row-oriented），也就是说数据是按行进行存放的。每个页存放的行记录也是有硬性规定的，最多允许存放16KB/2-200行记录，即7992行。 InnoDB行记录格式InnoDB引擎和大多数数据库一样，记录以行的形式存储。这意味这页中保存着表中一行行的数据。在InnoDB 1.0.x 版本之前，InnoDB存储引擎提供了Compact 和 Redundant 两种格式来存放行记录数据，这也是目前使用最多的一种格式。Redundant 格式是为了兼容之前版本而保留的，MySQL 5.1之后的版本默认为 Compact 行格式。可以使用命令SHOW TABLE STATUS LIKE &#39;table_name&#39;来查看当前表使用的行格式。 Compact 行记录格式Compact行记录格式的设计目标是高效的存储数据，简单来说就是一个页中存放的行数据越多，性能就越高。下图是详细的存储格式： 可以看到，Compact 行记录的格式首部是一个非 NULL 变长字段长度列表，并且是按照列的顺序逆序放置的，长度为： 若列的长度小于255字节，则用1字节表示； 若大于255个字节，用2字节表示。 变长字段的长度不可以超过两个字节，因为 VARCHAR 类型的最大长度限制为 65535。变长字段之后的第二个部分是 NULL 标志位，该位指示了该位数据中是否有 NULL 值，有则用1表示。接下来的部分是记录头信息（record header），固定占用5字节（40位），每位的含义如下： 最后的部分就是实际存储每个列的数据。需要特别注意的是，NULL 不占用该部分任何空间，即 NULL 除了占用 NULL 标志位之外，实际存储不占用空间。另外，每行数据除了用户定义的之外，还有两个隐藏列，事务ID列和回归指针列，分别是6和7字节大小。如果 InnoDB 表没有定义主键，还会增加一个6字节的 rowid 列。 行溢出数据InnoDB 存储引擎可以将一条记录中的某些数据存储在真正的数据页面之外。一般认为 BLOB 、 LOB 这类的大对象列类型的存储会把数据放在数据页面之外。但是不是这样的，BLOB 可以不放在溢出页面，而且即便 VARCHAR 这种类型也可能被存放为行溢出数据。 MySQL 数据库的 VARCHAR 类型理论上可以存放 65535 字节，实际上创建一个 65535 长度的列的时候，会得到错误信息： 1[42000][1118] Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs. 由于还有别的开销，经过实际测试，能存放 VARCHAR 类型的最大长度是65532。 这里需要注意，65535 是说的字节数，如果你是用的是多字节的字符集，比如我用utf8mb4，没个字占用4个字节，创建会提示[42000][1074] Column length too big for column &#39;varchars&#39; (max = 16383); use BLOB or TEXT instead.，即varchar字段已经被限制到了16383的长度。 此外，还要注意，MySQL 官方手册中定义的65535长度是指所有 VARCHAR 列的长度综合，如果综合超过了这个长度，依然无法创建，同样是上面那个错误提示。 即便能够存储65535个字节，但是，InnoDB 引擎的页为 16KB，即16384字节，怎么能存放65535个字节呐？因此，在一般情况下，InnoDB引擎存储的数据都是放在页类型为B-tree node中。但是当发生行溢出的时候，数据存放在页类型为 Uncompress BLOB 页中。 通过工具可以看到，数据也只保存 VARCHAR 的前768的前缀（prefix）数据，之后都是偏移量，指向行溢出页，也就是 Uncompressed BLOB Page。因此，对于行溢出数据，存放采用下图的方式： 那么新的问题又来了，多长的 VARCHAR 是保存在单个数据页中的，多长开始会保存在 BLOB 中？ InnoDB存储引擎表是索引组织的，即 B+Tree 结构，这样每个页中至少应该有两条记录（不然就退化成链表了）。因此，如果页中只能存下一条记录，那么 InnoDB 引擎就会自动将行数据放到溢出页中。 经过测试，这个数字是 8098 ，如果少于这个长度，一个页中就可以放入至少两行数据，VARCHAR 类型的行数据就不会被放到 BLOB 页中去。 另一个问题，对于 TEXT 和 BLOB 数据类型，他们也不是永远存放在 Uncompressed BLOB Page 中的，跟 VARCHAR 类似，至少保证一个页中能存放两条记录。当然一般 BLOB 不会这么小，大多数情况下还是会发生行溢出，数据页只保存前 768 字节，实际数据还是存在 BLOB 页中。 InnoDB数据页结构页是 InnoDB 存储引擎管理数据库的最小磁盘单位。也类型为 B-tree Node 的页存放的即是表中行的实际数据了。 InnoDB 数据页由以下7个部分组成，如图所示： File Header（文件头） Page Header（页头） Infimun 和 Supremum Records User Records（用户记录，即行记录） Free Space（空闲空间） Page Directory（页目录） File Trailer（文件结尾信息） 其中 File Header、Page Header、File Trailer 的大小是固定队的，分别为38、56、8字节，这些空间用来标记也得一些信息，如 Checksum，数据页所以在的 B+ 树索引的层数等。User Records、Free Space、Page Dirctory 这些部分为实际的行记录存储空间，因此大小是动态的。 约束约束完整性关系型数据库和文件系统的一个不同点是，关系数据库本身能够保证存储数据的完整性，不需要应用程序的控制，而文件系统一般都需要在程序端进行控制。当前几乎所有的关系型数据库都提供了约束（constraint）机制，来保证数据的完整性。 对 InnoDB 存储引擎本身而言，提供了以下几种约束： Primary Key Unique Key Foreign Key Default NOT NULL 约束的创建和查找约束的创建可以用以下两种方式： 表建立的时候进行约束定义 利用ALTER TABLE命令来创建约束 对于 Unique Key （唯一索引）的约束，用户可以通过命令CREATE UNIQUE INDEX来建立。对于主键约束而言，默认约束名为 PRIMARY 。 约束和索引的区别创建约束的方法通常就是创建索引的方法，的确，当用户创建了一个唯一索引就创建了一个唯一约束。但是约束和索引的概念还是有所不同的，约束更是一个逻辑的概念，用来保证数据的完成性，而索引是一个数据结构，既有逻辑上的概念，在数据库中还代表着物理存储的方式。 对错误数据的约束在某些设置下，MySQL 允许非法的或不正确的数据的插入或更新，又或者可以在数据库内部将其转化为一个合法的值，如向 NOT NULL 的字段中插入一个 NULL值，MySQL 数据库会将其改为0再插入，因此数据库本身没有对数据的正确性进行约束。而通过设置 sql_mode参数，MySQL 数据库又可以堆输入的合法值进行约束。详细设置可以参考MySQL官方手册中对 sql_mode 的说明。 外键约束MyISAM 存储引擎本身不支持外键，而 InnoDB 则完整支持外键约束。现在有一定规模的项目都会不建议甚至强制不允许使用外键，具体原因可以再专门探讨，这里就不详细说明外键的使用方法和原理了，只记录几个知识要点。 可以定义的字表操作： CASCADE：父表变化时子表与父表进行一样的操作 SET NULL：父表变化时子表设为 NULL NO ACTION：父表抛出错误，阻止操作 RESTRICT：同上, 都是立即检查外键约束 数据库默认的设置是 RESTRICT。 视图在MySQL数据中，视图（View）是一个命名的虚表，它由一个 SQL 查询来定义，可以当做表使用。与持久表（permanent table）不同的是，视图中的数据没有实际的物理存储。 视图的作用视图的主要用途之一是被用作一个抽象装置，特别是对于一些应用程序，程序本身不需要关心基表（base table）的结构，只需要按照视图定义来取数据或者更新数据，因此，视图层在一定程度上起到一个安全层的作用。 创建视图的语法如下： 1CREATE2 [OR REPLACE]3 [ALGORITHM &#x3D; &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]4 [DEFINER &#x3D; user]5 [SQL SECURITY &#123; DEFINER | INVOKER &#125;]6 VIEW view_name [(column_list)]7 AS select_statement8 [WITH [CASCADED | LOCAL] CHECK OPTION] 虽然视图是基于基表的虚拟表，但是用户可以对某些视图进行更新操作，其本质就是通过视图的定义来更新基表。 一般称可以进行更新操作的视图成为可更新视图（updatable view）。视图定义中的 WITH CHECK OPTION 就是针对于可更新的视图的，即更新的值是否需要检查。 分区表分区概述分区功能并不是在存储引擎层完成的，因此不是只有 InnoDB 存储引擎支持分区，常见的存储引擎 MyISAM、NDB等都支持。但也并不是所有的存储引擎都支持，如 CSV、FEDORATED、MERGE 等就不支持。在使用分区功能前，应该对选择的存储引擎对分区的支持有所了解。 MySQL 数据库在5.1版本时就添加了对分区的支持。分区的过程是将一个表或索引分解为多个更小、更可管理的部分。就访问数据库的应用而言，从逻辑上讲，只有一个表或一个索引，但是在物理上，这个表或索引可能由数十个物理分区组成。每个分区都是独立的对象，可以独自处理，也可以作为一个更大对象的一部分进行处理。 MySQL 数据库支持的分区类型为水平分区，并不支持垂直分区。此外，MySQL 数据的分区是局部分区索引，一个分区中即存放了数据又存放了索引。而全局分区是指，数据存放在各个分区中，但是所有数据的索引放在一个对象中。目前，MySQL 数据库还不支持全局分区。 MySQL 数据库支持以下几种分区： RANGE 分区：行数据基于属于一个给定连续区间的列值被放入分区。MySQL 5.5 开始支持 RANGE COLUMNS 的分区。 LIST 分区：和 RANGE 分区类似，只是 LIST 分区面向的是离散的值。 MySQL 5.5 开始支持 LIST COLUMNS 的分区。 HASH 分区：根据用户自定义的表达式的返回值来进行分区，返回值不能为负数。 KEY 分区：根据 MySQL 数据库提供的哈希函数来进行分区。 无论创建何种类型的分区，如果分区表中存在主键或唯一索引时，分区列必须是唯一索引的一个组成部分。 唯一索引是允许 NULL 值的，并且分区列只要是唯一索引的一个组成部分，不需要是整个唯一索引列都是分区列。 另外，如果建表时没有指定主键，唯一索引，可以指定任何一个列为分区列。 分区类型RANGE 分区RANGE分区是最常用的一种分区。下面的CREATE TABLE语句创建了一个 id 列的区间分区表。当 id 小于10时，数据插入 p0 分区。当 id 等于10 小于 20时，数据插入 p1 分区。 1CREATE TABLE t&#123;2id INT3&#125;ENGINE&#x3D;INNODB4PARTITION BY RANGE(id)&#123;5PARTITION p0 VALUES LESS THAN(10),6PARTITION p1 VALUES LESS THAN(20));7&#125; 这时候查看磁盘上的物理文件，启用分区表之后，文件不再是由一个 ibd 文件组成了，而是由建立分区时的各个分区 ibd 文件组成。 可以通过查询information_cheme架构下的 PARATITIONS 表来查看每个分区的具体信息： 1SELECT * FROM information_scheme.PARTITIONS where table_scheme&#x3D;database() and table_name&#x3D;&#39;t&#39;\\G; RANGE 分区主要用于日期列的分区，例如销售类的表，按年进行分区存放销售记录。这样的好处就是，便于对表进行管理。比如要删除某一年的数据，不需要在 where 里面写大段的时间参数，只需删除对应年份所在的分区即可。另一个好处就是可以加快某些查询操作，比如查询某一年的销售额，使用预先设定好的分区可以加快查询速度。 LIST 分区LIST 分区个 RANGE 分区非常相似，只是分区列的值是离散的，而非连续的。比如： 1CREATE TABLE t(2a INT,3b INT)ENGINE&#x3D;INNODB4PARTITION BY LIST(b)(5PARTITION p0 VALUES IN(1,3,5,7,9),6PARTITION p1 VALUES IN(0,2,4,6,8)7); 不同于 RANGE 分区中定义的VALUES LESS TAHN语句，LIST 分区使用 VALUES IN。因为每个分区的值是离散的，因此只能定义值。 HASH 分区HASH分区的目的是将数据均匀的分不到预先定义的各个分区中，保证各个分区的数据量大致都一样的。在 RANGE 和 LIST 分区中，必须明确指定一个给定的列值或列值集合应该保存在哪个分区中；而在 HASH 分区中，MySQL 自动完成这些工作，用户需要做的只是基于将要进行哈希分区的列值指定一个列值或者表达式，以及指定被分区的表将要被分割的分区数量。 要使用 HASH 分区来分割一个表，要在 CREATE TABLE 语句上添加一个PARTITION BY HASH（expr）子句，其中expr是一个返回一个整数的表达式。它可以仅仅是字段类型为 MySQL 整型的列名。此外，用户很可能需要在后面再添加一个PARTITIONS num子句，其中 num 是一个非负的整数，它表示将要被分割成分区的数量。如果没有包含一个 PARTITIONS 字句，那么分区数量将默认为1。 下面的例子创建了一个 HASH 分区的表 t，分区按日期列 b 进行： 1CREATE TABLE t_hash(2a INT,3b DATETIME)ENGINE&#x3D;INNODB4PARTITION BY HASH(YEAR(b))5PARTITIONS 4; MySQL 数据库还支持一种成为 LINEAR HASH的分区，它使用一个更加复杂的算法来确定新行插入到已经分区的表中的位置。它的语法和 HASH 分区的语法相似，只是将关键字 HASH 改为 LINEAR HASH。下面创建的是一个 LINEAR HASH 的分区表： 1CREATE TABLE t_linear_hash(2a INT,3b DATETIME)ENGINE&#x3D;INNODB4PARTITION BY LINEAR HASH(YEAR(b))5PARTITIONS 4; LINEAR HASH 分区的优点在于，增加、删除、合并和拆分分区将变得更加快捷，这有利于处理含有大量数据库的表。缺点在于，与使用 HASH 分区得到的数据分布相比，各个分区间数据的分布可能不太均衡。 KEY 分区KEY 分区和 HASH 分区相似，不同之处在于 HASH 分区使用用户定义的函数进行分区，KEY 分区使用 MySQL 数据库提供的函数进行分区，对于 NDB Cluster 引擎，MySQL 数据库使用 MD5 函数来分区；对于其他存储引擎， MySQL 数据库使用其内部的哈希函数，这些函数基于与 PASSWORD() 一样的运算法则。 COLUMNS 分区前面介绍的4种分区，分区得条件是：数据必须是整形（integer），如果不是整型，那应该需要通过函数化为整型。MySQL 5.5 版本开始支持 COLUMNS 分区，可以视为 RANGE 分区和 LIST 分区的一种进化。COLUMNS 分区可以直接使用非整形的数据进行分区，分区根据类型直接比较而得，不需要转化为整形。此外 RANGE COLUMNS 分区可以对多个列的值进行分区。 COLUMNS 分区支持以下的数据类型： 所有整形类型，如 INT、SMALLINT、TINYINT、BIGINT。FLOAT 和 DECIMAL 则不予支持。 日期类型，如 DATE 和 DATETIME。其余的日期类型则不予支持。 字符串类型，如 CHAR、VARCHAR、BINARY 和 VARBINARY。BLOB 和 TEXT 类型不予支持。 对于日期类型的分区，我们不再需要YEAR()和TO_DODAY()函数了，而可以直接使用 COLUMNS。 子分区子分区（subpartitioning）是在分区的基础上在进行分区，有时也称这种分区为复合分区（composite partitioning）。MySQL 数据库允许在 RANGE 和 LIST 的分区上在进行 HASH 和 KEY 的子分区。 但是子分区的建立需要注意以下几个问题： 每个子分区的数量必须相同。 要在一个分区表的任何分区上使用 SUBPARTITION 来明确定义任何子分区，就必须定义所有的子分区。 每个 SUBPARTITION 子句必须包括子分区的一个名字。 子分区的名字必须是唯一的。 分区中的NULL值MySQL数据库允许堆 NULL 值进行分区，但是处理方法和其他数据库可能完全不同。MySQL 数据库的分区总是视 NULL 值小于任何一个非 NULL 值，这和 MySQL 数据库中处理 NULL 值的ORDER BY操作是一样的。因此对于不同的分区类型，MySQL 数据库对于 NULL 值的处理也是各不相同。 对于 RANGE 分区，如果向分区中插入了 NULL 值，则 MySQL 数据库会将该值放入最左边的分区。 LIST 分区必须显式指出向哪个分区中放入 NULL 值，否则会报错。 HASH 分区和 KEY 分区跟上面又不相同，任何分区函数都会将含有 NULL 值的记录返回为0。 分区和性能在合理使用分区之前，必须了解分区的使用环境。 数据库的应用分为两类：一类是 OLTP（在线事务处理），如 Blog、电子商务、网络游戏等；另一类是 OLAP（在线分析处理），如数据仓库、数据集市。 对于 OLAP 的应用，分区的确是能够很好地提高查询性能，因为 OLAP 应用大多数查询需要频繁的扫描一张很大的表。 然而对于 OLTP 的应用，分区应该非常小心。在这种应用下，通常不可能获取一张表中超过 10%，大部分通过索引返回几条记录即可。而根据 B+ 树索引的原理，一般的 B+ 树需要2~3次的磁盘 IO ，因此 B+ 树已经可以很好地完成操作，不需要分区的帮助，而且设计不好的分区会带来严重的性能问题。 在表和分区间交换数据在MySQL 5.6 开始支持ALTER TABLE ... EXCHANGE PARTITION语法。该语句允许分区或子分区中的数据与另一个非分区的表中的数据进行交换。如果非分区表中的数据为空，那么相当于将分区中的数据移动到非分区表中。若分区表中的数据为空，则相当于将外部表中的数据导入到分区中。 要使用ALTER TABLE ... EXCHANGE PARTITION语句，必须满足： 要交换的表和分区表有相同的结构，但是不能含有分区 在非分区表中的数据必须在交换的分区定义中 被交换的表中不能含有外键，或者其他的表含有对该表的外键引用 用户除了需要 ALTER、INSERT 和 CREATE 权限外，还需要 DROP 的权限 此外，还有两个小的细节需要注意： 使用该语句的时候，不会触发交换表和被交换表上的触发器 AUTO_INCREMENT 列将被重置","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"InnoDB","slug":"InnoDB","permalink":"http://beritra.github.com/tags/InnoDB/"}]},{"title":"Java中的各种集合类","slug":"Java中的各种集合类","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:27:27.944Z","comments":true,"path":"2019/12/01/Java中的各种集合类/","link":"","permalink":"http://beritra.github.com/2019/12/01/Java%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E9%9B%86%E5%90%88%E7%B1%BB/","excerpt":"记录 Java 中的各种集合类和底层原理。","text":"记录 Java 中的各种集合类和底层原理。 Java中的集合主要分为以下集合类：Map、List、Set、Queue和concurrent包里面供多线程环境下使用的以上几种集合类。 Mapjava.util包中提供的常见Map类包括以下几种。这里乱入了个ConcurrentHashMap，放到下面和其他concurrent包的集合一起讲。 HashMap HashMap是老生常谈的集合了，学习HashMap主要关注点是哈希算法、rehash、数据存储、扩容方式、性能区别和结合ConcurrentHashMap了解为什么线程不安全，后者怎么解决线程安全问题。 哈希算法先看一下JDK中hashCode的生成方式，JDK8以后都是如下方式： 1static final int hash(Object key) &#123;2 int h;3 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);4&#125; 这里可以看到，key不是null的情况下，都是取key.hashCode()之后无符号右移16位，然后取异或。这里的key.hashCode()是native方法，直接在JVM中返回int型散列值。 无符号右移&gt;&gt;&gt; ：不管正负标志位为0还是1，将该数的二进制码整体右移，左边部分总是以0填充，右边部分舍弃。 位与：第一个操作数的的第n位于第二个操作数的第n位如果都是1，那么结果的第n为也为1，否则为0 为什么要这么做？ 理论上散列值是一个int型，如果直接拿散列值作为下标访问HashMap主数组的话，考虑到2进制32位带符号的int表值范围从-2147483648到2147483648。前后加起来大概40亿的映射空间。只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。 但问题是一个40亿长度的数组，内存是放不下的。你想，HashMap扩容之前的数组初始大小才16。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来访问数组下标。 顺便说一下，这也正好解释了为什么HashMap的数组长度要取2的整次幂。因为这样（数组长度-1）正好相当于一个“低位掩码”。“与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。以初始长度16为例，16-1=15。2进制表示是00000000 00000000 00001111。和某散列值做“与”操作如下，结果就是截取了最低的四位值。 1 10100101 11000100 001001012&amp; 00000000 00000000 000011113----------------------------------4 00000000 00000000 00000101 &#x2F;&#x2F;高位全部归零，只保留末四位 但这时候问题就来了，这样就算我的散列值分布再松散，要是只取最后几位的话，碰撞也会很严重。更要命的是如果散列本身做得不好，分布上成等差数列的漏洞，恰好使最后几个低位呈现规律性重复，就无比蛋疼。 这时候“扰动函数”的价值就体现出来了，说到这里大家应该猜出来了。看下面这个图: 右位移16位，正好是32bit的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。 扩容HashMap两个关键的初始化参数： 1static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 162static final float DEFAULT_LOAD_FACTOR = 0.75f; 前者是初始化容量16，即新建一个HashMap的时候，如果不指定长度，则容量为16。 后者是加载因子，即当实际长度除以容量高于该因子的时候，进行扩容操作。默认为0.75，所以HashMap空间占用大于3/4的时候就开始扩容了。扩容后的容量是原来的两倍。 HashMap的resize不是简单的把长度扩大，而是有下面两个步骤： 扩容：创建一个新的Entry空数组，长度是原数组的2倍。 reHash:遍历原Entry数组，把所有的Entry重新hash到新数组。为什么要重新hash呢？因为长度扩大以后，hash的规则也随之改变。 让我们回顾一下hash公式： index = hashCode(key) &amp; (length - 1) 当原数组长度为8时，hash运算是和111B做与运算；新数组长度为16，hash运算是和1111B做与运算。Hash结果显然不同。 那么这里为什么要用map容量减去1这个数字哪？好处有两个： 分布均匀 速度更快 在HashMap的源码中。get和put方法会根据key的hash值找到这个entry在hash表数组中的位置，源码如下： 1if ((p &#x3D; tab[i &#x3D; (n - 1) &amp; hash]) &#x3D;&#x3D; null)2 tab[i] &#x3D; newNode(hash, key, value, null); 按照我们理想的状况，hashMap的存取就是O(1)，也就是直接根据hashcode就可以找到它，每个bucket只存储一个节点，链表指向都是null,这样就比较开心了，不要出现一个链表很长的情况。 所以我们希望它能分布的均匀一点，如果让我们设计的话，我们肯定是直接对长度取模：hashcode % length,但HashMap的设计者却不是这样写的，它写成了2进制运算，因为当容量一定是2^n时，h &amp; (length - 1) == h % length，并且位运算的速度要高于取模。 另外，元素在重新计算hash之后，因为n变为2倍，新的index的二进制就是在前面多了一位，比如原来的容量为8的时候，元素下标为5，扩容到16之后，根据多的那一位是0还是1，元素下标只需要+8或者在原位置就可以了，也就是说resize操作也会更快。 存储方式HashMap实际是一种“数组+链表”数据结构。在put操作中，通过内部定义算法寻止找到数组下标，将数据直接放入此数组元素中，若通过算法得到的该数组元素已经有了元素（俗称hash冲突，链表结构出现的实际意义也就是为了解决hash冲突的问题）。将会把这个数组元素上的链表进行遍历，将新的数据放到链表末尾。 通过哈希算法从寻止上能够高效的找到对应的下标，但是随着数据的增长，哈希冲突碰撞过多。在寻找数据上，找到该来链表，会通过遍历在寻找对应数据，如此将会使得get数据效率越来越低。在jdk1.8中，链表元素数量大于等于8将会重组该链表结构形成为“红黑树结构”，这种结构使得在hash冲突碰撞过多情况下，get效率比链表的效率高很多。 性能在没有哈希冲突的情况下，HashMap存取元素的时间复杂度是O(1)，但是这只是理想情况。当冲突不多的时候，重复元素以链表形式存储，时间复杂度是O(N)，当数据量大的时候，链表转换为红黑树，时间复杂度变成O(LogN) 线程安全和其他局限HashMap不是线程安全的，另外如果HashMap的key是自定义类，需要重写hashCode()方法，并且由于HashMap的效率高度依赖hashCode()，需要保证散列分布尽量均匀。 都知道HashMap不是线程安全的，那么在哪些环节导致了他线程不安全？ 插入数据的时候 1tab[i] = newNode(hash, key, value, null); 假如A线程和B线程同时添加元素，然后计算出了相同的哈希值对应了相同的数组位置，因为此时该位置还没数据，然后对同一个数组位置添加，B的写入操作就会覆盖A的写入操作造成A的写入操作丢失。 删除数据的时候 跟上面同样，多个线程同时删除数据，可能产生错误。 扩容的时候 线程1执行put时，因为元素个数超出threshold而导致rehash，线程2此时执行get，有可能导致这个问题。 因为在resize的时候，是计算新的容量和threshold，在创建一个新hash表，最后将旧hash表中元素rehash到新的hash表中。如果在这个期间，另一个线程执行读取操作，有可能get到null。 那么ConcurrentHashMap如何保证线程安全？下面解析ConcurrentHashMap的时候一起说。 LinkedHashMapLinkedHashMap是继承自HashMap的，跟HashMap最大的区别是，他是基于Hash表和链表的实现，并且依靠着双向链表保证了迭代顺序是插入的顺序。 来看看HashMap和LinkedHashMap的结构图，是不是秒懂了。LinkedHashMap其实就是可以看成HashMap的基础上，多了一个双向链表来维持顺序。 可以用LinkedHashMap实现最近访问算法，即最近访问过的元素在最前面，LinkedHashMap有这么一个构造方法。 public LinkedHashMap(int initialCapacity, float loadFactor,boolean accessOrder) accessOrder为true的时候按照元素最后访问时间排序（LRU算法：最近最久使用），为false则是按照插入顺序排序，默认为false. TreeMapTreeMap是基于红黑树的实现，具有如下特点： 不允许出现重复的key； 可以插入null键，null值； 可以对元素进行排序； 无序集合（插入和遍历顺序不一致）； 由于是基于红黑树，TreeMap在插入、删除、搜索的时候，时间复杂度都是O（LogN） EnumMapEnumMap是专门为枚举类型量身定做的Map实现。虽然使用其它的Map实现（如HashMap）也能完成枚举类型实例到值得映射，但是使用EnumMap会更加高效：它只能接收同一枚举类型的实例作为键值，并且由于枚举类型实例的数量相对固定并且有限，所以EnumMap使用数组来存放与枚举类型对应的值。这使得EnumMap的效率非常高。EnumMap在内部使用枚举类型的ordinal()得到当前实例的声明次序，并使用这个次序维护枚举类型实例对应值在数组的位置。 在key是枚举类的时候，EnumMap可以用来代替HashMap，并且由于是数组实现，性能更好。 HashTableHashtable 与 HashMap 的简单比较 HashTable 基于 Dictionary 类，而 HashMap 是基于 AbstractMap。Dictionary 是任何可将键映射到相应值的类的抽象父类，而 AbstractMap 是基于 Map 接口的实现，它以最大限度地减少实现此接口所需的工作。 HashMap 的 key 和 value 都允许为 null，而 Hashtable 的 key 和 value 都不允许为 null。HashMap 遇到 key 为 null 的时候，调用 putForNullKey 方法进行处理，而对 value 没有处理；Hashtable遇到 null，直接返回 NullPointerException。 Hashtable 方法是同步，而HashMap则不是。我们可以看一下源码，Hashtable 中的几乎所有的 public 的方法都是 synchronized 的，而有些方法也是在内部通过 synchronized 代码块来实现。所以有人一般都建议如果是涉及到多线程同步时采用 HashTable，没有涉及就采用 HashMap，但是在Collections 类中存在一个静态方法：synchronizedMap()，该方法创建了一个线程安全的 Map 对象，并把它作为一个封装的对象来返回。 HashMap的初始容量为16，Hashtable初始容量为11，两者的填充因子默认都是0.75。 两者计算hash的方法不同 Hashtable计算hash是直接使用key的hashcode对table数组的长度直接进行取模 1int hash &#x3D; key.hashCode();2int index &#x3D; (hash &amp; 0x7FFFFFFF) % tab.length; HashMap计算hash对key的hashcode的前后16为进行了异或操作，以获得更好的散列值，然后对table数组长度取模(实际上是位操作，增加效率) 1 static final int hash(Object key) &#123;2 int h;3 return (key &#x3D;&#x3D; null) ? 0 : (h &#x3D; key.hashCode()) ^ (h &gt;&gt;&gt; 16);4 &#125;5 6static int indexFor(int h, int length) &#123;7 return h &amp; (length-1);8 &#125; IdentityHashMapIdentityHashMap是一致性哈希表，使用引用相等，而不是equals方法来比较两个对象的相等性。因此，IdentityHashMap中，如果存在两个键key1和key2，当且仅当key1==key2时，两个键相等，而其他大部分的哈希表，当且仅当k1 == null ? k2 == null : k1.equals(k2)时，两个键才认为是相等的。 IdentityHashMap使用System.identityHashCode来确定对象的哈希码，该方法返回对象的地址。 看下IdentityHashMap的存储原理图，和HashMap不同，HashMap是通过数组+拉链法存储元素并解决哈希冲突的。IdentityHashMap将所有的key和value都存储到Object[]数组table中，并且key和value相邻存储，当出现哈希冲突时，会往下遍历数组，直到找到一个空闲的位置。注意，数组第一个位置存储的是key，第二个位置存储的是value。因此奇数位置处存储的是key，偶数位置处存储的是value。 IdentityHashMap同样允许空的键和值，但是不保证map中的顺序，尤其是不保证顺序会恒定不变。 WeakHashMap和HashMap一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是null。不过WeakHashMap的键是“弱键”。 当弱引用指向的对象只能通过弱引用（没有强引用或弱引用）访问时，GC会清理掉该对象，之后，引用对象会被放到ReferenceQueue中。在Entry的构造函数中可以得知，通过super(key, queue)将key保存为弱引用，通过this.value = value将value保存为强引用。当key中的引用被gc掉之后，在下次访问WeakHashMap（调用expungeStaleEntries函数）时相应的entry也会自动被移除。 WeakHashMap 并不是你什么也不干它就能自动释放内部不用的对象的，而是在你访问它的内容的时候释放内部不用的对象。 Listjava.util包中提供的常见List类包括以下几种。 从刚学Java的前几天起，大概就会见到这个问题：LinkedList和ArrayList有什么共同点和区别？ 共同点： 二者都是继承自AbstractList抽象类，AbstractList实现了List接口中除了size()、get(int location)之外的方法。 二者都是线程不安全的。 区别： ArrayList是实现了基于动态数组的数据结构，而LinkedList是基于链表的数据结构； 数据更新和查找时，ArrayList可以直接通过数组下标访问，所以效率更高。 数据增加和删除的时候，ArrayList需要移动其他元素的位置，而LinkedList只需要修改一个指针，所以后者效率更高。 VectorVector是同样继承于AbstractList的一个列表，而它是线程安全的，实现方式是对所有数据操作的方法添加了synchronized关键字。 StackStack栈是Vector的一个子类，它实现了一个标准的后进先出的栈。 他的方法很简单，只有empty()、peek()、pop()、push(Object element)、search(object element)这几个。其中peek和pop的返回值都是堆栈顶部的对象，但是前者只是查看，后者是移除。 Setjava.util包中提供的常见Set类包括以下几种。 HashSet没什么好说的，其实就是把HashMap封装了一层，从HashSet的构造方法可以看出，就是维护了一个HashMap，数据的增伤改查也是调用的HashMap的方法。 TreeSet也是一样，其实就是TreeMap套了个皮。 EnumSet就不一样了，跟EnumMap其实没有什么关系。EnumSet 是一个 Set 集合的抽象类，其有两个实现类 JumboEnumSet 和 RegularEnumSet，在使用的时候放入的必须是枚举类型，其特点是速度非常快。 EnumSet 的默认子类 RegularEnumSet 和 JumboEnumSet 实现原理都是基于位运算向量，位运算向量的原理就是用一个位表示一个元素的状态（元素的状态只有两种），用一组位表示一个集合的状态，每个位对应一个元素，譬如一个枚举类 DemoEnum 有6个枚举值，则 EnumSet 集合就可以通过一个 byte 字节从右到左（二进制低到高位）来表示，不用的位上用 0 填充，用的位上每个 bit 位代表一个枚举值，1 表示包含该枚举值，0 表示不含该枚举值。因此位向量能表示的枚举值个数与向量长度有关，上面例子中一个 byte 类型最多能表示 8 个枚举值，所以 EnumSet 抽象类的两个实现类 RegularEnumSet 和 JumboEnumSet 分别定义了不同的向量长度。RegularEnumSet 使用 64 位的 long 类型变量作为位向量，而 JumboEnumSet 使用一个 long 类型数组作为向量（数组内存连续），故当我们通过 EnumSet 的工厂方法创建 EnumSet 集合时 EnumSet 会通过判断枚举类的枚举值数量决定使用两个子类的哪一个，如果枚举值个数小于等于 64 就用 RegularEnumSet，大于 64 就用 JumboEnumSet。 简单来说EnumSet就是一个高效的枚举类集合。 Queue 队列(Queue)可以当做一种特殊的线性表，遵循先进先出原则。而双向队列(Deque),是Queue的一个子接口，双向队列是指该队列两端的元素既能入队(offer)也能出队(poll),如果将Deque限制为只能从一端入队和出队，则可实现栈的数据结构。 PriorityQueue有一种特殊的队列，叫做优先队列。优先队列的作用是能保证每次取出的元素都是队列中权值最小的（Java的优先队列每次取最小元素，C++的优先队列每次取最大元素）。这里牵涉到了大小关系，元素大小的评判可以通过元素本身的自然顺序（natural ordering），也可以通过构造时传入的比较器（Comparator，类似于C++的仿函数）。 Java中PriorityQueue实现了Queue接口，不允许放入null元素；其通过堆实现，具体说是通过完全二叉树（complete binary tree）实现的小顶堆（任意一个非叶子节点的权值，都不大于其左右子节点的权值），也就意味着可以通过数组来作为PriorityQueue的底层实现。 最小堆的完全二叉树有一个特性是根节点必定是最小节点，子女节点一定大于其父节点。还有一个特性是叶子节点数量=全部非叶子节点数量+1。 每次增删元素都有可能对树结构进行调整，所以PriorityQueue队列不适合进场出队入队的频繁操作，但是他的优先级特性非常适合一些对顺序有要求的数据处理场合。 concurrent包ConcurrentHashMap 上面HashMap已经说到了HashMap在多个线程同时存取或者触发扩容的时候，都有可能出现错误，导致操作被覆盖或者丢失，那么怎么解决这个问题呐？ 第一反应当然是加锁，HashTable就是这么做的，使用了synchronized关键字。虽然解决了并发访问的安全性问题，但是性能不怎么样。HashTable中的增删改、甚至equals、toString方法等等都是方法级的锁，所以同时只能一个线程去操作，导致效率问题。 在JDK1.7及之前版本，ConcurrentHashMap采用的是Segment分段锁，即将数据分为一段一段的存储，然后给每一段数据加一把锁。当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 在JDK1.8以后，ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。 数据结构与HashMap1.8的结构类似，数组+链表/红黑二叉树(链表长度&gt;8时，转换为红黑树)。 通过 JDK 的源码和官方文档看来， 他们认为的弃用分段锁的原因由以下几点： 加入多个分段锁浪费内存空间。 生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。 为了提高 GC 的效率。 在JDK11下对HashMap和ConcurrentHashMap进行了简单测试，生成5000万条随机数然后插入，分别消耗16348毫秒和19194毫秒。其中包括随机数生成、插入和扩容的时间消耗，可见两者之间性能差距不大。 然后使用HashTable在单线程下插入，同样的数据量时间在17秒所有，跟HashMap差别不大，可以当做是误差范围内。然后使用20个线程插入，消耗时间在15秒左右，提升并不明显。奇怪的是ConcurrentHashMap却使用了45秒。然后缩小数据量，在1000万以下的时候，ConcurrentHashMap的插入速度又好于HashTable了。这个现象很有意思，有空了详细研究一下产生这个问题的原因。 ConcurrentHashMap的整体性能要优于HashTable，但是某些场景不能替代HashTable，例如强一致性的场景，ConcurrentHashMap的get、size等方法都没有加锁，ConcurrentHashMap是弱一致性的。 ConcurrentSkipListMap concurrentHashMap与ConcurrentSkipListMap性能测试在4线程1.6万数据的条件下，ConcurrentHashMap 存取速度是ConcurrentSkipListMap 的4倍左右。 但ConcurrentSkipListMap有几个ConcurrentHashMap 不能比拟的优点： 1、ConcurrentSkipListMap 的key是有序的。 2、ConcurrentSkipListMap 支持更高的并发。ConcurrentSkipListMap 的存取时间是log（N），和线程数几乎无关。也就是说在数据量一定的情况下，并发的线程越多，ConcurrentSkipListMap越能体现出他的优势。 ArrayBlockingQueue ConcurrentLinedQueue ConcurrentLinkedDeque ConcurrentSkipSet CopyOnWriteArrayList CopyOnWriteArraySet DelayQueue LinkedBlockingDeque LinkedBlockingQueue LinkedTransferQueue PriorityBlockingQueue SynchronousQueue 参考资料： JDK 源码中 HashMap 的 hash 方法原理是什么？ HashMap底层实现原理 【java并发】造成HashMap非线程安全的原因 IdentityHashMap源码详解 WeakHashMap实现原理及源码分析 EnumSet 原理相关","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"集合","slug":"集合","permalink":"http://beritra.github.com/tags/%E9%9B%86%E5%90%88/"}]},{"title":"Java多线程","slug":"Java多线程","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:27:33.213Z","comments":true,"path":"2019/12/01/Java多线程/","link":"","permalink":"http://beritra.github.com/2019/12/01/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"Java 多线程相关的基础知识。","text":"Java 多线程相关的基础知识。 概念线程和进程 何为线程？ 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 何为进程？ 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程和进程有何不同？ 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 生命周期 一个线程有五个基本状态 新建状态：当线程对象创建后，即进入新建状态，如：Thread t = new MyThread(); 就绪状态：当调用线程对象的start()方法时，线程即进入就绪状态。处于就绪状态的线程只是说明此线程已经做好准备，随时等待CPU调度执行，并不是说执行了start()方法就立即执行。 运行状态：当CPU开始调度处于就绪状态的线程时，此时线程才得以真正执行，即进入到运行状态。 阻塞状态：处于运行状态中的线程由于某种原因，暂时放弃对CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才有机会再次被CPU调用以进入到运行状态。阻塞状态分为三种： 等待阻塞 同步阻塞 其他阻塞 死亡状态：线程执行完毕或者异常退出，该线程结束生命周期。 关键字synchronizedsynchronized 是 Java 中的关键字，是利用锁的机制来实现同步的。 锁机制有如下两种特性： 互斥性：即在同一时间只允许一个线程持有某个对象锁，通过这种特性来实现多线程中的协调机制，这样在同一时间只有一个线程对需同步的代码块(复合操作)进行访问。互斥性我们也往往称为操作的原子性。 可见性：必须确保在锁被释放之前，对共享变量所做的修改，对于随后获得该锁的另一个线程是可见的（即在获得锁时应获得最新共享变量的值），否则另一个线程可能是在本地缓存的某个副本上继续操作从而引起不一致。 作用域： 对象一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞。 1public void run() &#123;2 synchronized (this)&#123;3 try &#123;4 System.out.println(Thread.currentThread().getId() + \" running\");5 Thread.sleep(2000);6 System.out.println(Thread.currentThread().getId() + \" complete\");7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 &#125;11&#125; 同一时刻只有一个线程可以执行run()方法 或者锁一个对象 1public Integer count=0;23public void add()&#123;4 synchronized (count)&#123;5 try &#123;6 System.out.println(Thread.currentThread().getId() + \" running\");7 Thread.sleep(2000);8 count++;9 System.out.println(Thread.currentThread().getId() + \" complete\");10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 &#125;14&#125; 起到同样的作用，count这个对象同一时间只能一个线程访问。 方法1public synchronized void run() &#123;2 try &#123;3 System.out.println(Thread.currentThread().getId() + \" running\");4 Thread.sleep(2000);5 System.out.println(Thread.currentThread().getId() + \" complete\");6 &#125; catch (InterruptedException e) &#123;7 e.printStackTrace();8 &#125;9&#125; 代码块synchronized关键字同样可以同步代码块，但是只有代码块内部的代码被锁，同一个方法里的其他代码仍然可以并行执行。 1public void synchronizedCodeBlock() &#123;2 System.out.println(Thread.currentThread().getId() + \":同一个方法里没有被锁的部分可以同步执行\");3 synchronized (this) &#123;4 try &#123;5 Thread.sleep(100);6 &#125; catch (InterruptedException e) &#123;7 e.printStackTrace();89 &#125;10 System.out.println(Thread.currentThread().getId() + \":被锁住的代码部分必须顺序执行\");11 &#125;12&#125; 内部类静态变量、方法、代码块synchronize关键字如果修饰的是一个静态变量、静态方法或者静态代码块的时候，同步的是这个类的所有实例。 锁静态方法、变量的话，会作用于该类的所有实例。 1public synchronized static void synchronizedStaticFunction() &#123;2 try &#123;3 Thread.sleep(200);4 &#125; catch (InterruptedException e) &#123;5 e.printStackTrace();6 &#125;7 System.out.println(Thread.currentThread().getId() + \":静态方法\");8&#125; locklock关键字的使用方式跟synchronized类似 总结来说，Lock和synchronized有以下几点不同： Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 Lock可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 几种锁可中断锁lock是可中断锁，而synchronized 不是可中断锁 线程A和B都要获取对象O的锁定，假设A获取了对象O锁，B将等待A释放对O的锁定， 如果使用 synchronized ，如果A不释放，B将一直等下去，不能被中断 如果 使用ReentrantLock，如果A不释放，可以使B在等待了足够长的时间以后，中断等待，而干别的事情 公平锁/非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁。非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。 乐观锁/悲观锁乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。悲观锁在Java中的使用，就是利用各种锁。乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。 独享锁/共享锁独享锁是指该锁一次只能被一个线程所持有。共享锁是指该锁可被多个线程所持有。对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReentrantReadWriteLock，其读锁是共享锁，其写锁是独享锁。读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。对于Synchronized而言，当然是独享锁。 可重入锁/不可重入锁 可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 读写锁ReentrantLock属于排他锁，这些锁在同一时刻只允许一个线程进行访问，而读写锁在同一时刻可以允许多个线程访问，但是在写线程访问时，所有的读和其他写线程都被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 自旋锁阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。 在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。 而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。 volatile被volatile修饰的共享变量，就具有了以下两点特性： 保证了不同线程对该变量操作的内存可见性; 禁止指令重排序 JMM主要就是围绕着如何在并发过程中如何处理原子性、可见性和有序性这3个特征来建立的，通过解决这三个问题，可以解除缓存不一致的问题。而volatile跟可见性和有序性都有关。olatile不保证原子性。 一个简单的测试 1public class MyVolatileClass &#123;2 int a = 0;3 4 public void add() &#123;5 a = a+1;6 &#125;7&#125; 开启多个线程执行一万次add()方法之后，可以看到a的结果并不是一万，而是要少于一万。无论int a前面加不加volatile结果都一样。所以volatile不能保证操作的原子性。那么volatile的作用是什么？ 1static volatile boolean stop = false;2public static void main(String[] args) &#123;3 new Thread(() -&gt; &#123;4 while(!stop) &#123;5 &#125;6 System.out.println(\"停止了\");7 &#125;).start();8 try &#123;9 Thread.sleep(1000);10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 stop = true;14 System.out.println(\"赶快停止吧 \" + stop);15&#125; 如果不带volatile关键字，那么循环很可能不会终止。 基本使用方式继承Thread类1public class MyThread extends Thread &#123;2 @Override3 public void run() &#123;4 System.out.println(\"MyThread is running\");5 &#125;6&#125; 实现Runnable接口1public class MyRunnable implements Runnable &#123;2 @Override3 public void run() &#123;4 System.out.println(\"MyRunnable is running\");5 &#125;6&#125; 实现Callable接口1public class MyCallable implements Callable &#123;2 @Override3 public Object call() &#123;4 System.out.println(\"MyCallable is running\");5 return null;6 &#125;7&#125; 三者区别 继承Thread不方便共享变量，由于Java的单继承机制，继承了Thread类之后不能再继承别的类。 Runnable和Callable功能比较相似，主要区别有： Runnable是自从java1.1就有了，而Callable是1.5之后才加上去的 Callable规定的方法是call(),Runnable规定的方法是run() Callable的任务执行后可返回值，而Runnable的任务是不能返回值(是void) call方法可以抛出异常，run方法不可以 运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果。 加入线程池运行，Runnable使用ExecutorService的execute方法，Callable使用submit方法 执行run方法和start方法的区别start()的作用是启动一个新的线程。 通过start()方法来启动的新线程，处于就绪（可运行）状态，并没有运行，一旦得到cpu时间片，就开始执行相应线程的run()方法，这里方法run()称为线程体，它包含了要执行的这个线程的内容，run方法运行结束，此线程随即终止。start()不能被重复调用。用start方法来启动线程，真正实现了多线程运行，即无需等待某个线程的run方法体代码执行完毕就直接继续执行下面的代码。这里无需等待run方法执行完毕，即可继续执行下面的代码，即进行了线程切换。 run()就和普通的成员方法一样，可以被重复调用。 如果直接调用run方法，并不会启动新线程！程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到多线程的目的。 总结：调用start方法方可启动线程，而run方法只是thread的一个普通方法调用，还是在主线程里执行。 FutureFuture就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。 在Future接口中声明了5个方法，下面依次解释每个方法的作用： cancel方法用来取消任务，如果取消任务成功则返回true，如果取消任务失败则返回false。参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务，如果设置true，则表示可以取消正在执行过程中的任务。如果任务已经完成，则无论mayInterruptIfRunning为true还是false，此方法肯定返回false，即如果取消已经完成的任务会返回false；如果任务正在执行，若mayInterruptIfRunning设置为true，则返回true，若mayInterruptIfRunning设置为false，则返回false；如果任务还没有执行，则无论mayInterruptIfRunning为true还是false，肯定返回true。 isCancelled方法表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true。 isDone方法表示任务是否已经完成，若任务完成，则返回true； get()方法用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回； get(long timeout, TimeUnit unit)用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null。 也就是说Future提供了三种功能： 判断任务是否完成； 能够中断任务； 能够获取任务执行结果。 FutureTask FutureTask类实现了RunnableFuture接口，我们看一下RunnableFuture接口的实现： 1`public` `interface` `RunnableFuture ``extends` `Runnable, Future &#123;`` ``void` `run();``&#125;` 可以看出RunnableFuture继承了Runnable接口和Future接口，而FutureTask实现了RunnableFuture接口。所以它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。 FutureTask提供了2个构造器： 1`public` `FutureTask(Callable callable) &#123;``&#125;``public` `FutureTask(Runnable runnable, V result) &#123;``&#125;` 事实上，FutureTask是Future接口的一个唯一实现类。 底层实现方式可能遇到的问题和解决办法死锁有可能产生死锁的情况： 系统资源的竞争 通常系统中拥有的不可剥夺资源，其数量不足以满足多个线程运行的需要，使得线程在 运行过程中，会因争夺资源而陷入僵局，如磁带机、打印机等。只有对不可剥夺资源的竞争 才可能产生死锁，对可剥夺资源的竞争是不会引起死锁的。 线程推进顺序非法 线程在运行过程中，请求和释放资源的顺序不当，也同样会导致死锁。例如，并发线程 P1、P2分别保持了资源R1、R2，而线程P1申请资源R2，线程P2申请资源R1时，两者都 会因为所需资源被占用而阻塞。 信号量使用不当也会造成死锁。线程间彼此相互等待对方发来的消息，结果也会使得这 些线程间无法继续向前推进。例如，线程A等待线程B发的消息，线程B又在等待线程A 发的消息，可以看出线程A和B不是因为竞争同一资源，而是在等待对方的资源导致死锁。 死锁是由四个必要条件导致的，所以一般来说，只要破坏这四个必要条件中的一个条件，死锁情况就应该不会发生。 如果想要打破互斥条件，我们需要允许进程同时访问某些资源，这种方法受制于实际场景，不太容易实现条件； 打破不可抢占条件，这样需要允许进程强行从占有者那里夺取某些资源，或者简单一点理解，占有资源的进程不能再申请占有其他资源，必须释放手上的资源之后才能发起申请，这个其实也很难找到适用场景； 进程在运行前申请得到所有的资源，否则该进程不能进入准备执行状态。这个方法看似有点用处，但是它的缺点是可能导致资源利用率和进程并发性降低； 避免出现资源申请环路，即对资源事先分类编号，按号分配。这种方式可以有效提高资源的利用率和系统吞吐量，但是增加了系统开销，增大了进程对资源的占用时间。 面试题线程池Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 在阿里巴巴Java开发手册中也明确指出，是『不允许』使用Executors创建线程池。 避免使用Executors创建线程池，主要是避免使用其中的默认实现，那么我们可以自己直接调用ThreadPoolExecutor的构造函数来自己创建线程池。在创建的同时，给BlockQueue指定容量就可以了。 线程间通信在调用wait(), notify()或notifyAll()的时候，必须先获得锁，且状态变量须由该锁保护，而固有锁对象与固有条件队列对象又是同一个对象。也就是说，要在某个对象上执行wait，notify，先必须锁定该对象，而对应的状态变量也是由该对象锁保护的。 调用一个Object的wait与notify/notifyAll的时候，必须保证调用代码对该Object是同步的，也就是说必须在作用等同于synchronized(obj){……}的内部才能够去调用obj的wait与notify/notifyAll三个方法，否则就会报错： 1java.lang.IllegalMonitorStateException:current thread not owner wait/notify/notifyAll一个简单的等待、唤醒示例： 1public class WaitAndNotify &#123;2 Object lock = new Object();34 public void prevFunc() &#123;5 try &#123;6 Thread.sleep(2000);7 synchronized (lock) &#123;8 lock.notifyAll();9 &#125;1011 &#125; catch (InterruptedException e) &#123;12 e.printStackTrace();13 &#125;14 &#125;1516 public void nextFunc() &#123;17 try &#123;18 synchronized (lock) &#123;19 System.out.println(Thread.currentThread().getId() + \"等待任务执行,线程wait\");20 lock.wait();21 System.out.println(Thread.currentThread().getId() + \"任务执行完毕,线程notify\");22 &#125;2324 &#125; catch (InterruptedException e) &#123;25 e.printStackTrace();26 &#125;2728 &#125;2930 public static void main(String[] args) &#123;31 WaitAndNotify waitAndNotify = new WaitAndNotify();32 new Thread(waitAndNotify::nextFunc).start();33 new Thread(waitAndNotify::nextFunc).start();34 new Thread(waitAndNotify::nextFunc).start();35 new Thread(waitAndNotify::prevFunc).start();36 &#125;37&#125; notify和notifyAll的区别是前者只随机唤醒一个线程，后者唤醒所有。 join1public class ThreadJoin &#123;2 void funcA() &#123;3 try &#123;4 System.out.println(Thread.currentThread().getId() + \"开始耗时操作\");5 Thread.sleep(2000);6 System.out.println(Thread.currentThread().getId() + \"耗时操作完成\");7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 &#125;1112 public static void main(String[] args) &#123;13 ThreadJoin threadJoin = new ThreadJoin();14 Thread thread = new Thread(threadJoin::funcA);15 thread.start();16 try &#123;17 System.out.println(\"主线程等待结果\");18 thread.join();19 System.out.println(\"主线程结束\");20 &#125; catch (InterruptedException e) &#123;21 e.printStackTrace();22 &#125;23 &#125;24&#125; 应用java.util.concurrent包 数据结构：ConcurrentHashMap, BlockingQueue 系列 线程池：Executor, ExecutorService, ThreadPoolExecutor, ScheduledThreadPoolExecutor, Executors（工厂类）, Callable, Runnable, Future 锁：ReentrantLock, ReentrantReadWriteLock, Condition 线程同步：CountDownLatch, CyclicBarrier 参考链接 Java多线程学习（一）Java多线程入门 java-线程中start和run的区别 Java多线程学习（吐血超详细总结） Java并发编程：Callable、Future和FutureTask Java 之 synchronized 详解 Java并发编程：Lock java中几种锁，分别是什么？","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"MySQL原理和优化","slug":"MySQL原理和优化","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:27:12.392Z","comments":true,"path":"2019/12/01/MySQL原理和优化/","link":"","permalink":"http://beritra.github.com/2019/12/01/MySQL%E5%8E%9F%E7%90%86%E5%92%8C%E4%BC%98%E5%8C%96/","excerpt":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。","text":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。 存储引擎MySQL 5.0 支持的存储引擎包括 MyISAM、 InnoDB、 BDB、 MEMORY、 MERGE、 EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED 等,其中 InnoDB 和 BDB 提供事务安全表,其他存储引擎都是非事务安全表。 使用show engines命令可以查看支持的存储引擎。我在MySQL 8.0中执行命令得到的结果如下： 一般使用的都是InnoDB引擎，所以主要还是研究InnoDB的特性和原理。 自动增长列InnoDB 表的自动增长列可以手工插入，但是插入的值如果是空或者 0，则实际插入的将是自动增长后的值。如果插入一个大于当前自增id的数的时候，自增id会变成这个数字，中间的就被跳过了。 可以通过“ALTER TABLE *** AUTO_INCREMENT = n;”语句强制设置自动增长列的初识值，默认从 1 开始，但是该强制的默认值是保留在内存中的，如果该值在使用之前数据库重新启动，那么这个强制的默认值就会丢失，就需要在数据库启动以后重新设置。 对于 InnoDB 表,自动增长列必须是索引。如果是组合索引,也必须是组合索引的第一列。 外键约束MySQL 支持外键的存储引擎只有 InnoDB，在创建外键的时候,要求父表必须有对应的索引，子表在创建外键的时候也会自动创建对应的索引。 在创建索引的时候，可以指定在删除、更新父表时，对子表进行的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。其中 RESTRICT 和 NO ACTION 相同，是指限制在子表有关联记录的情况下父表不能更新； CASCADE 表示父表在更新或者删除时，更新或者删除子表对应记录；SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被 SET NULL。选择后两种方式的时候要谨慎，可能会因为错误的操作导致数据的丢失。 《阿里巴巴Java规范》中强制要求不能使用外键，所有表之间的关联逻辑应该在业务逻辑中实现。 存储方式在8.0里面，InnoDB把数据字典进行重构，大家应该知道数据字典是什么，就是表结构，你的用户定义，所有的跟DDL相关放到数据字典里面去。在8.0之前，数据字典有两份，一份是存储在.frm文件里，另一份是InnoDB的数据表里。在8.0之后.frm被干掉了，只保留了InnoDB中的数据。 索引索引概述所有 MySQL 列类型都可以被索引，对相关列使用索引是提高 SELECT 操作性能的最佳途径。根据存储引擎可以定义每个表的最大索引数和最大索引长度，每种存储引擎(如 MyISAM、InnoDB、BDB、MEMORY 等)对每个表至少支持 16 个索引，总索引长度至少为 256 字节。大多数存储引擎有更高的限制。 InnoDB存储引擎支持以下几种常见的索引： B+ 树索引 全文索引 哈希索引 B+树索引就是传统意义上的索引，这是目前关系型数据库系统中查找最为常用和最有效的索引。B+树索引的构造类似与二叉树，根据键值（Key Value）快速找到数据。 设计索引的原则和技巧 搜索的索引列，不一定是所要选择的列。换句话说，最适合索引的列是出现在 WHERE子句中的列，或连接子句中指定的列，而不是出现在 SELECT 关键字后的选择列表中的列。 使用惟一索引。考虑某列中值的分布。索引的列的基数越大，索引的效果越好。例如，存放出生日期的列具有不同值，很容易区分各行。而用来记录性别的列，只含有“ M”和“F”，则对此列进行索引没有多大用处，因为不管搜索哪个值，都会得出大约一半的行。 使用短索引。如果对字符串列进行索引，应该指定一个前缀长度，只要有可能就应该这样做。 利用最左前缀。在创建一个 n 列的索引时，实际是创建了 MySQL 可利用的 n 个索引。多列索引可起几个索引的作用，因为可利用索引中最左边的列集来匹配行。这样的列集称为最左前缀。另外，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任调整。 不要过度索引。不要以为索引“越多越好”，什么东西都用索引是错误的。 对于 InnoDB 存储引擎的表，记录默认会按照一定的顺序保存，如果有明确定义的主键,则按照主键顺序保存。如果没有主键，但是有唯一索引，那么就是按照唯一索引的顺序保存。如果既没有主键又没有唯一索引，那么表中会自动生成一个内部列，按照这个列的顺序保存。 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 索引的操作创建索引的详细定义如下： 1CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name2 [index_type]3 ON tbl_name (key_part,...)4 [index_option]5 [algorithm_option | lock_option] ...67key_part: &#123;col_name [(length)] | (expr)&#125; [ASC | DESC]89index_option:10 KEY_BLOCK_SIZE [&#x3D;] value11 | index_type12 | WITH PARSER parser_name13 | COMMENT &#39;string&#39;14 | &#123;VISIBLE | INVISIBLE&#125;1516index_type:17 USING &#123;BTREE | HASH&#125;1819algorithm_option:20 ALGORITHM [&#x3D;] &#123;DEFAULT | INPLACE | COPY&#125;2122lock_option:23 LOCK [&#x3D;] &#123;DEFAULT | NONE | SHARED | EXCLUSIVE&#125; 索引的删除语法为： 1DROP INDEX index_name ON tbl_name B+树索引的原理关于B树和B+树的结构，可以参考这篇博文：B树和B+树的插入、删除图文详解 B+树相比B树，优点包括： B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快； B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定; B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。 B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。 而B树相对于B+树的优点是，如果经常访问的数据离根节点很近，而B树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比B+树快。 聚簇索引 &amp; 非聚簇索引聚集索引与非聚集索引的区别是：叶节点是否存放一整行记录 InnoDB 主键使用的是聚簇索引，MyISAM 不管是主键索引，还是二级索引使用的都是非聚簇索引。 下图形象说明了聚簇索引表(InnoDB)和非聚簇索引(MyISAM)的区别： 对于非聚簇索引表来说（右图），表数据和索引是分成两部分存储的，主键索引和二级索引存储上没有任何区别。使用的是B+树作为索引的存储结构，所有的节点都是索引，叶子节点存储的是索引+索引对应的记录的数据。 对于聚簇索引表来说（左图），表数据是和主键一起存储的，主键索引的叶结点存储行数据(包含了主键值)，二级索引的叶结点存储行的主键值。使用的是B+树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录(数据内容)。 聚簇索引的优点 当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。 当通过聚簇索引查找目标数据时理论上比非聚簇索引要快，因为非聚簇索引定位到对应主键时还要多一次目标记录寻址,即多一次I/O。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。 聚簇索引的缺点 插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键。 更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新。 二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。二级索引的叶节点存储的是主键值，而不是行指针（非聚簇索引存储的是指针或者说是地址），这是为了减少当出现行移动或数据页分裂时二级索引的维护工作，但会让二级索引占用更多的空间。 采用聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多，因为插入要保证主键不能重复，判断主键不能重复，采用的方式在不同的索引下面会有很大的性能差距，聚簇索引遍历所有的叶子节点，非聚簇索引也判断所有的叶子节点，但是聚簇索引的叶子节点除了带有主键还有记录值，记录的大小往往比主键要大的多。这样就会导致聚簇索引在判定新记录携带的主键是否重复时进行昂贵的I/O代价。 联合索引联合索引是指对表上多个列进行索引，创建方法和单个索引一样，不同之处在于有多个索引列。 需要注意的是，如果 索引优化和最佳实践参考文章： MySQL索引原理及慢查询优化","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"优化","slug":"优化","permalink":"http://beritra.github.com/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"MySQL基础知识总结","slug":"MySQL基础知识总结","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:27:18.340Z","comments":true,"path":"2019/12/01/MySQL基础知识总结/","link":"","permalink":"http://beritra.github.com/2019/12/01/MySQL%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","excerpt":"总结 MySQL 中的一些基础知识点。","text":"总结 MySQL 中的一些基础知识点。 本文尽量按照MySQL 8.0版本的规则，但是很多参考书籍和资料还是基于5.*版本，可能有疏漏。 SQL基础SQL全称是Structure Query Language（结构化查询语言）。主要分为三个类别： DDL（Data Definiton Languages）语句：数据定义语言，这些语句定义的不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的关键字包括create、drop、alter等。 DML（Data Manipulation Language）语句：数据操纵语句、用于添加、删除、更新和查询数据库记录，并检查数据完整性，常见的语句关键字包括insert、delete、update和select等。 DCL（Data Control Language）语句：数据控制语句，用于控制不同的数据段直接的许可和访问级别的语句。这些语句定义的数据库、表、字段、用户的访问权限和安全级别。主要语句关键字包括grant、revoke等。 DDL语句数据库增删改查数据库创建的格式如下： 1CREATE DATABASE db_name2 [[DEFAULT] CHARACTER SET charset_name]3 [[DEFAULT] COLLATE collation_name] 常用的字符集参数为： 1create database test_database default CHARACTER SET utf8mb4 collate utf8mb4_unicode_ci; 建议使用utf8mb4代替utf8，后者在存储比如emoji表情的时候会出错。collate关键字代表的是字符排序规则，会影响结果展示的顺序和order by等排序参数。 删除数据库语句： 1drop database db_name; 要修改字符集参数的话： 1ALTER DATABASE db_name2 [[DEFAULT] CHARACTER SET charset_name]3 [[DEFAULT] COLLATE collation_name] MySQL似乎是没有提供对数据库重命名的方法。 如果想查看一共有多少个数据库，可以用： 1show databases; 会显示包含系统库在内的所有数据库。 表的增删改查创建表的语句比较复杂，直接把官网文档上的完整表述贴了过来： 1CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name2 (create_definition,...)3 [table_options]4 [partition_options]56CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name7 [(create_definition,...)]8 [table_options]9 [partition_options]10 [IGNORE | REPLACE]11 [AS] query_expression1213CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name14 &#123; LIKE old_tbl_name | (LIKE old_tbl_name) &#125;1516create_definition:17 col_name column_definition18 | &#123;INDEX|KEY&#125; [index_name] [index_type] (key_part,...)19 [index_option] ...20 | &#123;FULLTEXT|SPATIAL&#125; [INDEX|KEY] [index_name] (key_part,...)21 [index_option] ...22 | [CONSTRAINT [symbol]] PRIMARY KEY23 [index_type] (key_part,...)24 [index_option] ...25 | [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY]26 [index_name] [index_type] (key_part,...)27 [index_option] ...28 | [CONSTRAINT [symbol]] FOREIGN KEY29 [index_name] (col_name,...)30 reference_definition31 | check_constraint_definition3233column_definition:34 data_type [NOT NULL | NULL] [DEFAULT &#123;literal | (expr)&#125; ]35 [AUTO_INCREMENT] [UNIQUE [KEY]] [[PRIMARY] KEY]36 [COMMENT &#39;string&#39;]37 [COLLATE collation_name]38 [COLUMN_FORMAT &#123;FIXED|DYNAMIC|DEFAULT&#125;]39 [STORAGE &#123;DISK|MEMORY&#125;]40 [reference_definition]41 [check_constraint_definition]42 | data_type43 [COLLATE collation_name]44 [GENERATED ALWAYS] AS (expr)45 [VIRTUAL | STORED] [NOT NULL | NULL]46 [UNIQUE [KEY]] [[PRIMARY] KEY]47 [COMMENT &#39;string&#39;]48 [reference_definition]49 [check_constraint_definition]5051data_type:52 (see Chapter 11, Data Types)5354key_part: &#123;col_name [(length)] | (expr)&#125; [ASC | DESC]5556index_type:57 USING &#123;BTREE | HASH&#125;5859index_option:60 KEY_BLOCK_SIZE [&#x3D;] value61 | index_type62 | WITH PARSER parser_name63 | COMMENT &#39;string&#39;64 | &#123;VISIBLE | INVISIBLE&#125;6566check_constraint_definition:67 [CONSTRAINT [symbol]] CHECK (expr) [[NOT] ENFORCED]6869reference_definition:70 REFERENCES tbl_name (key_part,...)71 [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE]72 [ON DELETE reference_option]73 [ON UPDATE reference_option]7475reference_option:76 RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT7778table_options:79 table_option [[,] table_option] ...8081table_option:82 AUTO_INCREMENT [&#x3D;] value83 | AVG_ROW_LENGTH [&#x3D;] value84 | [DEFAULT] CHARACTER SET [&#x3D;] charset_name85 | CHECKSUM [&#x3D;] &#123;0 | 1&#125;86 | [DEFAULT] COLLATE [&#x3D;] collation_name87 | COMMENT [&#x3D;] &#39;string&#39;88 | COMPRESSION [&#x3D;] &#123;&#39;ZLIB&#39;|&#39;LZ4&#39;|&#39;NONE&#39;&#125;89 | CONNECTION [&#x3D;] &#39;connect_string&#39;90 | &#123;DATA|INDEX&#125; DIRECTORY [&#x3D;] &#39;absolute path to directory&#39;91 | DELAY_KEY_WRITE [&#x3D;] &#123;0 | 1&#125;92 | ENCRYPTION [&#x3D;] &#123;&#39;Y&#39; | &#39;N&#39;&#125;93 | ENGINE [&#x3D;] engine_name94 | INSERT_METHOD [&#x3D;] &#123; NO | FIRST | LAST &#125;95 | KEY_BLOCK_SIZE [&#x3D;] value96 | MAX_ROWS [&#x3D;] value97 | MIN_ROWS [&#x3D;] value98 | PACK_KEYS [&#x3D;] &#123;0 | 1 | DEFAULT&#125;99 | PASSWORD [&#x3D;] &#39;string&#39;100 | ROW_FORMAT [&#x3D;] &#123;DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT&#125;101 | STATS_AUTO_RECALC [&#x3D;] &#123;DEFAULT|0|1&#125;102 | STATS_PERSISTENT [&#x3D;] &#123;DEFAULT|0|1&#125;103 | STATS_SAMPLE_PAGES [&#x3D;] value104 | TABLESPACE tablespace_name [STORAGE &#123;DISK|MEMORY&#125;]105 | UNION [&#x3D;] (tbl_name[,tbl_name]...)106107partition_options:108 PARTITION BY109 &#123; [LINEAR] HASH(expr)110 | [LINEAR] KEY [ALGORITHM&#x3D;&#123;1|2&#125;] (column_list)111 | RANGE&#123;(expr) | COLUMNS(column_list)&#125;112 | LIST&#123;(expr) | COLUMNS(column_list)&#125; &#125;113 [PARTITIONS num]114 [SUBPARTITION BY115 &#123; [LINEAR] HASH(expr)116 | [LINEAR] KEY [ALGORITHM&#x3D;&#123;1|2&#125;] (column_list) &#125;117 [SUBPARTITIONS num]118 ]119 [(partition_definition [, partition_definition] ...)]120121partition_definition:122 PARTITION partition_name123 [VALUES124 &#123;LESS THAN &#123;(expr | value_list) | MAXVALUE&#125;125 |126 IN (value_list)&#125;]127 [[STORAGE] ENGINE [&#x3D;] engine_name]128 [COMMENT [&#x3D;] &#39;string&#39; ]129 [DATA DIRECTORY [&#x3D;] &#39;data_dir&#39;]130 [INDEX DIRECTORY [&#x3D;] &#39;index_dir&#39;]131 [MAX_ROWS [&#x3D;] max_number_of_rows]132 [MIN_ROWS [&#x3D;] min_number_of_rows]133 [TABLESPACE [&#x3D;] tablespace_name]134 [(subpartition_definition [, subpartition_definition] ...)]135136subpartition_definition:137 SUBPARTITION logical_name138 [[STORAGE] ENGINE [&#x3D;] engine_name]139 [COMMENT [&#x3D;] &#39;string&#39; ]140 [DATA DIRECTORY [&#x3D;] &#39;data_dir&#39;]141 [INDEX DIRECTORY [&#x3D;] &#39;index_dir&#39;]142 [MAX_ROWS [&#x3D;] max_number_of_rows]143 [MIN_ROWS [&#x3D;] min_number_of_rows]144 [TABLESPACE [&#x3D;] tablespace_name]145146query_expression:147 SELECT ... (Some valid select or union statement) 可以看到创建表的时候，主要分为三个部分，包括创建定义、表选项、分区选项等，实际上大多数时候用不到这么多功能。 不手动指定的话，模式使用InnoDB作为数据库引擎，InnoDB最大支持40亿张表。 一个简单的创建语句： 1create table test_table2(3 id int not null auto_increment primary key,4 name varchar(20),5 class varchar(20),6 age int7)engine&#x3D;InnoDB character set utf8mb4; 创建完成后，可以使用desc关键字查看表结构： 1desc tablename; desc命令输出的信息相对比较简单，可以使用如下语句查看详细的建表语句： 1show create table tablename; 修改表结构使用alter table语句： 修改表类型：ALTER TABLE table_name MODIFY [COLUMN] column_definition [FIRST|AFTER colname] 增加表字段：ALTER TABLE table_name ADD [COLUMN] column_definition [FIRST|AFTER col_name] 删除表字段：ALTER TABLE table_name DROP [COLUMN] col_name 修改字段名：ALTER TABLE tablename CHANGE [COLUMN] old_col_name column_definition [FIRST|AFTER col_name] 修改字段排列顺序：前面介绍的的字段增加和修改语法(ADD/CNAHGE/MODIFY)中,都有一个可选项 first|aftercolumn_name,这个选项可以用来修改字段在表中的位置,默认 ADD 增加的新字段是加在表的最后位置,而 CHANGE/MODIFY 默认都不会改变字段的位置。 重命名表，老版本的MySQL一般使用：ALTER TABLE test_table RENAME to test_table02; MySQL 8.0中支持rename关键字：rename table test_table02 to test_table; DML语句插入记录表创建好后,就可以往里插入记录了,插入记录的基本语法如下: 1INSERT INTO tablename (field1,field2,......fieldn) VALUES(value1,value2,......valuesn); 更新记录对于表里的记录值,可以通过 update 命令进行更改,语法如下: 1UPDATE tablename SET field1&#x3D;value1,field2.&#x3D;value2,......fieldn&#x3D;valuen [WHERE CONDITION] 删除记录如果记录不再需要,可以用 delete 命令进行删除,语法如下: 1DELETE FROM tablename [WHERE CONDITION] 查询记录SELECT 的语法很复杂,所有这里只介绍最基本的语法: 1SELECT * FROM tablename [WHERE CONDITION] 太复杂的就不重复了，这些都是基础内容，网上的教程一大把。 DCL语句DCL 语句主要是 DBA 用来管理系统中的对象权限时所使用,一般的开发人员很少使用。只记录几条经常用的吧。 这里经常遇到的就是MySQL 8.0和之前版本的不一致，往往在网上搜到一个命令，拿过来不能用，MySQL 8.0的变动以下几个方面： 验证插件和密码加密方式的变化在 MySQL 8.0 中，caching_sha2_password 是默认的身份验证插件而不是之前版本的 mysql_native_password，默认的密码加密方式是 SHA2 。 用户授权和修改密码之前版本： 1GRANT ALL PRIVILEGES ON *.* TO &#96;mike&#96;@&#96;%&#96; IDENTIFIED BY &#39;000000&#39; WITH GRANT OPTION; MySQL 8.0版本中正确的授权语句： 1CREATE USER &#39;mike&#39;@&#39;%&#39; IDENTIFIED BY &#39;000000&#39;;2GRANT ALL ON *.* TO &#39;mike&#39;@&#39;%&#39; WITH GRANT OPTION; 可以看到，创建用户和授权是分开的，不再能一个语句搞定。 修改密码： 1ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;;2FLUSH PRIVILEGES; 也可以使用mysqladmin命令: 1mysqladmin -u root -p password 新密码 还有很多其他内容，作为开发不用完全掌握，可以详见这个博文：MySQL 8.0用户和角色管理 MySQL数据类型数值类型MySQL 支持所有标准 SQL 中的数值类型,其中包括严格数值类型(INTEGER、SMALLINT、DECIMAL 和 NUMERIC),以及近似数值数据类型(FLOAT、REAL 和 DOUBLE PRECISION),并在此基础上做了扩展。扩展后增加了TINYINT、MEDIUMINT 和 BIGINT 这 3 种长度不同的整型,并增加了 BIT 类型,用来存放位数据。 整型数据对于整型数据,MySQL 还支持在类型名称后面的小括号内指定显示宽度,例如 int(5)表示当数值宽度小于 5 位的时候在数字前面填满宽度,如果不显示指定宽度则默认为 int(11)，一般配合 zerofill 使用。 设置了宽度限制后,如果插入大于宽度限制的值,不会对插入的数据有任何影响,还是按照类型的实际精度进行保存。 小数对于小数的表示,MySQL 分为两种方式:浮点数和定点数。浮点数包括 float(单精度)和 double(双精度),而定点数则只有 decimal 一种表示。定点数在 MySQL 内部以字符串形式存放,比浮点数更精确,适合用来表示货币等精度高的数据。 浮点数和定点数都可以用类型名称后加“(M,D)”的方式来进行表示,“(M,D)”表示该值一共显示 M 位数字(整数位+小数位),其中 D 位位于小数点后面,M 和 D 又称为精度和标度。 由于float和double分别以32位和64位存储，所以实际上是可能产生精度丢失的。比如一个float类型的列，插入0.123456789的时候，实际存储到数据库的只是近似值0.123457,，当位数更多的时候这个问题更明显，比如分别向float和double类型的列中插入987654321.123456789，会得到987654000和987654321.1234568。 所以想要存储高精度、准确的数字，还是需要用decimal类型。 日期时间类型MySQL中的事件类型有：DATE、DATETIME、TIMESTAMP、TIME、YEAR 这些数据类型的主要区别如下: 如果要用来表示年月日,通常用 DATE 来表示。 如果要用来表示年月日时分秒,通常用 DATETIME 表示。 如果只用来表示时分秒,通常用 TIME 来表示。 如果需要经常插入或者更新日期为当前系统时间,则通常使用 TIMESTAMP 来表示。TIMESTAMP 值返回后显示为“YYYY-MM-DD HH:MM:SS”格式的字符串,显示宽度固定为 19 个字符。如果想要获得数字值,应在 TIMESTAMP 列添加+0。 如果只是表示年份,可以用 YEAR 来表示,它比 DATE 占用更少的空间。YEAR 有 2 位或4 位格式的年。默认是 4 位格式。在 4 位格式中,允许的值是 19012155 和 0000。在2 位格式中,允许的值是 7069,表示从 1970~2069 年。 MySQL 以 YYYY 格式显示 YEAR值。 字符串类型MySQL 包括了 CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM 和 SET 等多种字符串类型。下面的表详细列出了这些字符类型的比较。 CHAR和VCHAR类型CHAR 和 VARCHAR 很类似,都用来保存 MySQL 中较短的字符串。二者的主要区别在于存储方式的不同：CHAR 列的长度固定为创建表时声明的长度，长度可以为从 0255 的任何值；而 VARCHAR 列中的值为可变长字符串，长度可以指定为065535( 5.0.3以后)之间的值。在检索的时候，CHAR 列删除了尾部的空格，而 VARCHAR 则保留这些空格。 BINARY 和 VARBINARY 类型BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR,不同的是它们包含二进制字符串而不包含非二进制字符串。 ENUM 类型ENUM 中文名称叫枚举类型，它的值范围需要在创建表时通过枚举方式显式指定，对 1255 个成员的枚举需要 1 个字节存储；对于 25565535 个成员，需要 2 个字节存储。最多允许有 65535 个成员。 SET 类型Set 和 ENUM 类型非常类似，也是一个字符串对象，里面可以包含 0~64 个成员。根据成员的不同，存储上也有所不同。 1~8 成员的集合,占 1 个字节。 9~16 成员的集合,占 2 个字节。 17~24 成员的集合,占 3 个字节。 25~32 成员的集合,占 4 个字节。 33~64 成员的集合,占 8 个字节。 其他类型MySQL 8中还支持多种空间类型，应该是用来存储地理信息之类的结构化信息的，但是没找到太多中文资料，平时也没用过，就不误人子弟了。 另外还有新增的JSON类型支持。 创建JSON字段创建JSON类型的字段很简单，跟其他数据结构没什么区别： 1CREATE TABLE t1 (jdoc JSON); 插入JSON1INSERT INTO t1 VALUES(&#39;&#123;&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;&#125;&#39;); 数据库会对JSON的格式进行校验，如果插入错误的格式会报错。 1mysql&gt; INSERT INTO t1 VALUES(&#39;[1, 2,&#39;);2ERROR 3140 (22032) at line 2: Invalid JSON text:3&quot;Invalid value.&quot; at position 6 in value (or column) &#39;[1, 2,&#39;. 查询JSON查询 json 中的数据用 column-&gt;path 的形式，其中对象类型 path 这样表示 $.path, 而数组类型则是 $[index] 查询testproject表student字段中json对象id为1的记录： 1SELECT * FROM testproject WHERE student-&gt;&#39;$.id&#39;&#x3D; 1; 查询testproject表student字段中json对象id为1或者5的记录： 1SELECT * FROM testproject WHERE student-&gt;&#39;$.id&#39; in (1,5);2SELECT * FROM testproject WHERE student-&gt;&#39;$.id&#39; &#x3D; 1 or student-&gt;&#39;$.id&#39; &#x3D; 5; 更新数据MySQL 并不支持 column-&gt;path 的形式进行更新操作。如果是整个 json 更新的话，和插入时类似的。如果需要更新JSON中的某个值，需要用系统提供的函数： json_array_append：在json后面追加 json_array_insert：在指定下标插入 json_replace：只替换已经存在的旧值，不存在则忽略 json_set：替换旧值，并插入不存在的新值 json_insert：插入新值，但不替换已经存在的旧值 json_remove：删除元素 这部分详细操作可以参考官方文档The JSON Data Type和博客mysql支持原生json使用说明","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL中的锁","slug":"MySQL中的锁","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:25:36.956Z","comments":true,"path":"2019/12/01/MySQL中的锁/","link":"","permalink":"http://beritra.github.com/2019/12/01/MySQL%E4%B8%AD%E7%9A%84%E9%94%81/","excerpt":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。","text":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。 开发多用户、数据库驱动的应用时，最大的一个难点是：一方面要最大程度地利用数据库的并发访问，另一方面还要确保每个用户能以一致的方式读取和修改数据。为此就有了锁（locking）的机制，同时这也是数据库系统区别于文件系统的一个关键特性。 什么是锁锁机制用于管理对共享资源的并发访问。InnoDB 存储引擎会在行级别上堆表数据上诉哟，这固然不错。不过InnoDB 存储引擎也会在数据库内部其他多个地方使用锁，从而允许堆多种不同资源提供并发访问。例如，操作缓冲池 LRU 列表，删除、添加、移动 LRU 列表中的元素，为了保证一致性，必须有锁的介入。 lock 与 latch在数据库中，lock 与 latch 都可以被称为“锁”，但是两者有着截然不同的含义，本文主要关注的是lock。 latch一般称为闩锁（轻量级的锁），因为其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差。在 InnoDB 存储引擎中，latch 又可以分为 mutex（互斥锁）和 rwlock（读写锁）。其目的是用来保证并发线程操作临界资源的正确性，并且通常没有死锁检测的机制。 lock 的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般lock的对象仅在事务 commit 或 rollback 后进行释放（不同事务隔离级别释放的时间可能不同）。此外，lock，正如在大多数数据库中一样，是有死锁机制的。下图显示了 lock 与 latch 的不同： 对于 InnoDB 存储引擎中的 latch，可以通过命令SHOW ENGINE INNODB MUTEX来进行查看。具体的数据结果说明如下： 上述信息是比较底层的，一般仅供开发人员参好。但是用户还是可以通过这些参数调优。 相对于 latch 的查看，lock 的信息就显得直观多了。用户可以通过命令SHOW ENGING INNODB STATUS及infomation_schema架构下的表INNODB_TRX、INNODB_LOCKS、INNODB_LOCK_WAITS来观察锁的信息。 InnoDB 存储引擎中的锁锁的类型InnoDB 存储引擎实现了如下两种标准的行级锁： 共享锁（S Lock），允许事务读一行数据。 排它锁（X Lock），允许事务删除或更新一行数据。 如果一个事务 T1 已经获得了行 r 的共享锁，那么另外的事务 T2 可以立即获得行 r 的共享锁，因为读取并没有改变行 r 的数据，称这种情况为锁兼容（Lock Compatible）。但若有其他事务 T3 想获得行 r 的排它锁，则其必须等待事务 T1、T2 释放行 r 上的共享锁——这种情况称为锁不兼容。下表显示了共享锁和排他锁的兼容性。 可以看出，X 锁与任何所都不兼容，而 S 锁仅和 S 锁兼容。需要特别注意的是，S 和 X 锁都是行锁，兼容是指对同一记录（row）锁的兼容性情况。 此外，InnoDB 存储引擎支持多粒度（granular）锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB 存储引擎支持一种额外的锁方式，称之为意向锁（Intention Lock）。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度（fine granularity）上进行加锁。 怼细粒度的对象上锁，那么首先需要对粗粒度的对象上锁。如图所示，如果要对页上的对象 r 上 X 锁，那么分别需要堆数据库A、表、页上 IX 锁，最后对记录 r 上 X 锁。若其中任何一部分导致等待，那么操作都需要等粗粒度锁的完成。 InnoDB 引擎支持意向锁设计的比较简练，其意向锁即为表级别的锁。设计目的主要是为了在一个事务中揭示下一行将被请求的锁类型。其支持两种意向锁： 意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁。 意向排他锁（IX Lock），事务想要获得一张表中某几行的排它锁。 由于 InnoDB 存储引擎支持的是行级别的锁，因此意向锁不会阻塞除全表扫描以外的任何请求。表级意向锁和行级锁的兼容性如图所示： 用户可以通过命令SHOW ENGINE INNODB STATUS命令查看当前锁请求的信息 从 InnoDB 1.0 开始，在INFORMATION_SCHEMA架构下添加了表INNODB_TRX、INNODB_LOCKS、INNODB_LOCKS_WAITS。通过这三张表，用户可以更简单地监控当前事务并分析可能存在的锁问题。INNODB的定义如下图所示，一共8个字段（在MySQL 8.0 版本进行测试，发现已经扩充到了24个字段）： 这个表可以显示当前运行的 InnoDB 事务，并不能判断锁的一些情况。如果需要查看锁，则还需要访问表INNODB_LOCKS，该表的字段如下所示： 再通过表INNODB_LOCKS查看了每张表上锁的情况后，用户就可以判断由此引发的等待状况了。当事务较小时，用户就可以人为地、直观地进行判断了。但是当事务量非常大，其中锁和等待也时常发生，这个时候就不那么容易判断。但是通过表INNODB_LOCK_WAIT，可以很直观地反应当前事务的等待。表INNODB_LOCK_WAITS由四个字段组成，如下图所示。 注意：在MySQL 8.0 版本中，INFORMATION_SCHEMA下的INNODB_LOCKS和INNODB_LOCK_WAITS表已被删除。 用Performance Schema data_locks和data_lock_waits表替代。 一致性非锁定读一致性的非锁定读（consistent nonlocking read）是指 InnoDB 存储引擎通过行多版本控制（multi versioning）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行 DELETE 或者 UPDATE 操作，这时读取操作不会因此去等待行上锁的释放。相反的，InnoDB 存储引擎会去读取行的一个快照存储。 之所以称其为非锁定读，因为不需要等待访问的行上 X 锁的释放。快照数据是指该行的之前版本的数据，该实现是通过 undo 段来完成，而 undo 用来在事务中回滚数据，因此快照数据本身是没有额外的开销的。此外，读取快照数据是不需要上锁的吗，因为没有事务需要对历史的数据进行修改操作。 可以看出，非锁定读机制极大地提提高了数据库的并发性。在 InnoDB 存储引擎的默认设置下，这是默认的读取方式，即读取不会占用和等待表上的锁。但是在不同事务隔离级别下，读取的方式不同，并不是在每个事务隔离级别下都是采用的非锁定的一致性读。此外，即使都是采用的非锁定的一致性读，但是对于快照数据的定义也各不相同。 在事务隔离级别READ COMMITTED和REPEATABLE READ（InnoDB存储引擎的默认事务隔离级别）下，InnoDB 存储引擎使用非锁定的一致性读。然而，对于快照数据的定义却不相同。在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据。而在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本。 举个栗子，首先在当前 MySQL 数据库的连接回话 A 中执行以下 SQL 语句： 1mysql&gt; begin;2Query OK, 0 rows affected (0.01 sec)34mysql&gt; select * from parent where id &#x3D;1;5+------+6| id |7+------+8| 1 |9+------+101 row in set (0.00 sec) 在会话 A 显式的开启了一个事务，并读取了表 parent 中 id 为 1 的数据，但是事务并没有结束。与此同时，用户再开启另一个回话B，这样可以模拟并发的情况，然后对 B 做如下的操作： 1mysql&gt; begin;2Query OK, 0 rows affected (0.00 sec)34mysql&gt; update parent set id&#x3D;3 where id&#x3D;1;5Query OK, 1 row affected (0.00 sec)6Rows matched: 1 Changed: 1 Warnings: 0 在会话 B 中将事务表 parent 中 id 为 1 的记录修改为 id=3 ，但是事务同样没有提交。这样 id=1 的行实际上加了一个 X 锁。这时如果在会话 A 中再次读取 id 为 1 的记录，根据 InnoDB 引擎的特性，即在READ COMMITTED和REPEATETABLE READ的事务隔离级别下会使用非锁定的一致性读。回到之前的会话 A ，接着上次未提交的事务，执行 SQL 语句 select * from parent where id=1的操作，这时不管使用READ COMMITTED还是REPEATETABLE READ的事务隔离级别，显示的数据应该都是： 1mysql&gt; select * from parent where id &#x3D;1;2+------+3| id |4+------+5| 1 |6+------+71 row in set (0.00 sec) 由于当前 id=1 的数据被修改了1次，因此只有一个行办本的记录。接着，在会话 B 中提交上次的事务： 1mysql&gt; commit;2Query OK, 0 rows affected (0.00 sec) 在会话 B 提交事务之后，在会话 A 中再次运行select * from parent where id=1语句，在READ COMMITTED和REPEATETABLE READ的事务隔离级别下得到的结果就不一样了。对于READ COMMITTED的事务隔离级别，它总是能够读取行的最新版本，如果行被锁定了，则读取最新的一个快照（fresh snapshot）。在上述例子中，因为会话 B 已经提交了事务，所以READ COMMITTED事务隔离级别下会得到如下结果： 1mysql&gt; select @@transaction_isolation\\G;2*************************** 1. row ***************************3@@transaction_isolation: REPEATABLE-READ41 row in set (0.00 sec)56mysql&gt; select * from parent where id &#x3D;1;7Empty set (0.00 sec) 而对于REPEATABLE READ的事务隔离级别，总是读取事务开始时的行数据。因此对于REPEATABLE REPEAD事务隔离级别，得到的结果如下： 1mysql&gt; select @@transaction_isolation\\G;2*************************** 1. row ***************************3@@transaction_isolation: REPEATABLE-READ41 row in set (0.00 sec)56mysql&gt; select * from parent where id &#x3D;1;7+------+8| id |9+------+10| 1 |11+------+121 row in set (0.00 sec) 注意，MySQL 8.0 之前查询当前事务隔离级别的语句是select @@tx_isolation，之后是上面用的select @@transaction_isolation 下面将从时间的角度上展现上述演示的示例过程，如下表所示。需要特别注意的是，对于READ COMMITTED的事务隔离级别而言，从数据库理论的角度来看，其违反了事务 ACID 中的 I 的特性，即隔离性。 一致性锁定锁在前一节讲到，默认配置下，事务隔离级别是REPEATABLE READ模式，InnoDB 存储引擎的SELECT操作使用一致性非锁定锁。但是在某些情况下，用户需要显式得堆数据库读取操作进行加锁以保证数据逻辑的一致性。InnoDB 存储引擎对于SELECT语句支持两种一致性的锁定读（locking read）操作： SELECT … FOR UPDATE SELECT … LOCK IN SHARE MODE SELECT ... FOR UPDATE堆读取的行记录加一个 X 锁，其他事务不能对已锁定的行加上任何锁。SELECT ... LOCK IN SHARE MODE对读取的行加一个 S 锁，其他事务可以向被锁定的行加 S 锁，但是如果加 X 锁，就会被阻塞。 以上这两个语句必须在同一个事务中，当事务提交了，锁也就释放了。因此在使用两个SELECT 锁的时候，务必记得带上BEGIN，START TRANSACTION或者SET AUTOCOMMIT=0。 自增长与锁自增长是非常常用的一个属性，在 InnoDB 引擎的内存结构中，对每个含有自增长值的表都有一个自增长计数器（auto-increment counter）。当对含有自增长的计数器的表进行插入操作时，这个计数器会被初始化，执行如下的语句来得到计数器的值： 1SELECT MAX(auto_inc_col) FROM t FOR UPDATE; 插入操作会依据这个自增的计数器的值加 1 赋予自增长列。这个实现方式称作AUTO-INC Locking。这个锁其实是采用一种特殊的表锁机制，为了提高插入的性能，锁不是在一个事务完成后才释放，而是在完成对自增长值插入 SQL 语句后立即释放。 虽然AUTO-INC Locking从一定程度上提高了并发插入的效率，但是还存在一定性能上的问题。首先对于自增长值得咧并发插入性能较差，事务必须等待前一个插入完成。其次，对于INSERT ... SELECT的大数据量插入会影响插入性能。因为另一个事务中的插入会被阻塞。 从 MySQL 5.1.22 开始，InnoDB 存储引擎中提供了一种轻量级互斥锁的自增长实现机制，这种机制大大提高了自增值的插入性能。从该版本开始，InnoDB 存储引擎提供了一个参数innodb_autoinc_lock_mode来控制自增长的模式，该参数的默认值为 1 。我们对自增长的插入进行分类的话，如图所示。 接着来分析参数innodb_autoinc_lock_mode以及各个设置下堆自增的印象，总共有三个有效值可供供设定，即0、1、2，具体说明如下表所示： 在 InnoDB 存储引擎中，自增长的列必须是索引，同时必须是索引的第一个列。如果不是第一个列，则 MySQL 数据库会抛出异常。 外键和锁在 InnoDB 存储引擎中，对于一个外键列，如果没有显式的对这个列加索引，InnoDB 引擎会自动加一个索引，因为这样可以避免表锁。对于外键的插入或更新，首先需要查询父表中的记录，即 SELECT 父表。单是对于父表的 SELECT 操作，不是使用一致性非锁定读的方式，因为这样会发生数据不一致的问题。因此这时使用的是SELECT ... LOCK IN SHARE MODE方式，即主动对父表加一个 S 锁。如果这时候父表已经加上了一个 X 锁，子表上的操作会被阻塞。 锁的算法行锁的 3 种算法InnoDB 存储引擎有三种行锁的算法，分别是： Record Lock：单个行记录上的锁 Rap Lock：间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock：Gap Lock+Record Lock，锁定一个范围，并且锁定记录本身 Record Lock 总是回去锁住索引记录，如果 InnoDB 存储引擎表在建立的时候没有设置任何一个索引，那么这是 InnoDB 存储引擎会使用隐式的主键来进行锁定。 Next-Key Lock 是结合了 Gap Lock 和 Record Lock 的一种锁定算法，在 Next-Key Lock 算法下，InnoDB 对于行的查询都是采用这种锁定算法。例如一个索引有10，11，13和20这四个值，那么该索引可能被 Next-Key Lock 的区间为： 1(-∞，10]2(10，11]3(11，13]4(13，20]5(20，+∞) 采用 Next-Key Lock 的锁定技术称为 Next-Key Locking。其设计的目的是为了解决 Phantom Problem，这将在下一个小节介绍。利用这种技术，锁定的不是单个值，而是一个范围，是谓词锁（predict lock）的一种改进。除了 Next-Key Locking，还有 Previous-Key Locking 技术。 同样上述索引10，11，13和20，如果采用 Previous-Key Locking 技术，可锁定的区间会变成： 1(-∞，10)2[10，11)3[11，13)4[13，20)5[20，+∞) 若事务 T1 已经通过 Next-Key Locking 锁定了如下范围： (10,11]、(11,13] 当插入新的记录 12 的时候，锁定的范围会变成： (10,11]、(11,12]、(12,13] 当查询的索引含有唯一属性的时候，InnoDB 存储引擎会对 Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围，从而提高应用的并发性。 使用下列代码创建测试表 t ： 1create table t(a int primary key);2insert into t select 1;3insert into t select 2;4insert into t select 5; 然后执行下列语句： 表 t 一共只有1、2、5 三个值。在上面的例子中，会话 A 首先对 a=5 进行 X 素哟定。而由于 a 是主键且唯一，因此锁定的仅仅是 5 这个值，而不是（2,5）这个范围，这样在会话 B 中插入值 4 而不会阻塞，可以立即插入并返回。 若是辅助索引，情况会完全不同。根据下列代码创建测试表 z： 1mysql&gt; create table z(a int,b int, primary key(a),key(b));2mysql&gt; insert into z select 1,1;3mysql&gt; insert into z select 3,1;4mysql&gt; insert into z select 5,3;5mysql&gt; insert into z select 7,6;6mysql&gt; insert into z select 10,8; 表 z 的列 b 是辅助索引，若在会话 A 中开启事务，执行下面的 SQL 语句： 1select * from z where b&#x3D;3 for update; 这时 SQL 语句将通过索引列 b 进行查询，因此使用传统的 Next-Key Locking技术进行加锁，并且由于有两个索引，需要对其分别加锁。对于聚集索引，仅对列 a 等于 5 的索引加上 Record Lock。而对于辅助索引，其加上的是 Next-Key Lock，锁定范围是（1,3），需要特别注意的，InnoDB 存储引擎还会对辅助索引下一个键值加上 gap lock，即还有一个辅助索引范围为（3,6）的锁，因此，若在新回话 B 中运行下面的 SQL 语句，都会被阻塞。 1select * from z where a&#x3D;5 lock in share mode;2insert into z select 4,2;3insert into z select 6,5; 原因如下： 第一个 SQL 语句不能执行，因为在会话 A 中执行的 SQL 语句已经对聚集索引中列 a=5 的值加上 X 锁，因此执行会被阻塞。 第二个 SQL 语句，主键插入 4 ，没有问题，但是插入的辅助索引 2 在锁定的范围（1,3）中，所以同样会阻塞 第三个 SQL 语句，插入的主键 6 没有被锁定，5 也不在范围（1,3）之间，但是插入的值 5 在另一个锁定的范围（3,6）中，所以同样需要等待。 上面的例子可以看出，Gap Lock 的作用是为了阻止多个事务将记录插入到同一个范围内，而这会导致 Phantom Problem 问题的产生。例如上面的例子中，会话 A 用户锁定了 b=3 的记录，如果此时没有锁定（3,6），那么用户可以插入 b 列为 3 的记录，这会导致会话 A 中的用户再次执行查询时返回不同的记录，这就导致了 Phantom Problem 问题的产生。 用户可以通过以下两种方式来显式的关闭 Gap Lock： 将事务的隔离级别设置为READ COMMITTED 将参数innodb_locks_unsafe_for_binlog设置为 1 在上述配置下，除了外键约束和唯一性检查仍然需要 Gap Lock，其余情况仅使用 Record Lock 进行锁定。但需要牢记的是，上述设置破坏了事务的隔离性，并且对于 replication，可能会导致主从数据的不一致。因此从性能上看，READ COMMITTED也不会优于默认的事务隔离级别READ REPEATABLE。 需要再次提醒的是，对于唯一键值的锁定，Next-Key Lock 降级为 Record Lock 仅存在于查询所有的唯一索引列。若唯一索引由多个列组成，而查询仅是查找多个唯一索引列中的其中一个，那么查询其实是 range 类型查询，而不是 point 类型查询，故 InnoDB 存储引擎依然使用 Next-Key Lock 进行锁定。 解决 Phantom Problem在默认的事务隔离级别下，即REPEATABLE READ下，InnoDB 存储引擎采用 Next-Key Locking 机制来避免 Phantom Problem（幻想问题）。 Phantom Problem 是指在同一事务下，连续执行两次同样的 SQL 可能导致不同的结果，第二次的 SQL 语句可能会返回之前不存在的行。 还是以上一节创建的表 t 为例，表 t 由 1、2、5 三个值组成，如果事务 T1 执行如下 SQL 语句： 1select * from t where a&gt;2 for update; 注意这时候 T1 并没有进行提交操作，上述应该返回 5 这个结果。如果与此同时，另一个事务 T2 插入了 4 这个值，并且数据库允许该操作，那么事务 T1 再次执行查询 SQL 语句的时候，就会得到结果 4 和 5 。这与第一次查询的结果不同，违反了事务的隔离性，即当前事务可以看到其他事务的结果。 InnoDB 引擎采用了 Next-Key Locking 算法避免出现上面的情况。对于上面的 SQL，实际上锁住的不只是 5 这个单个值，而是对（2,+∞）范围都加了 X 锁，因此对于这个范围内的插入都是不允许的，从而避免 Phantom Problem。 InnoDB 引擎默认的事务隔离级别是REPEATABLE READ，在该隔离级别下，采用 Next-Key Locking 来加锁。而在事务隔离级别READ COMMITTED级别下，仅采用 record Lock，因此在上述示例中，会话 A 需要将事务的隔离级别设置为READ COMMITTED。 此外，用户可以通过 InnoDB 存储引擎的 Next-Key Locking 机制在应用层面实现唯一性检查： 1select * from table where col&#x3D;xxx lock in share mode; 如果用户通过索引查询一个值，并对该行加上一个 SLock，那么即使查询的值不存在，锁定的也是一个范围，因此若此行没有任何返回，那么新插入的值一定是唯一的。 这里如果在第一步select ... lock in share mode操作的回收，有多个事务并发操作，这种唯一性检查机制是否存在问题呐？答案是不会，因为这时候会导致死锁，只有一个事务操作成功，其他会抛出死锁的错误提示。 锁问题通过锁定机制可以实现事务的隔离性要求，使得事务可以并发的工作。锁提高了并发，但是也有潜在的问题。不过好在事务隔离性的要求，锁只会带来以下三种问题。 脏读在理解脏读（Dirty Read）之前，需要理解脏数据的概念。脏数据是指未提交的数据，如果读到了脏数据，即一个事务可以读到另外一个事务未提交的数据，则显然违反了数据的隔离性。 下面的表格显示了一个脏读的例子： 表 t 还是上面创建的那个，不过不同于上述例子，这里把事务隔离级别改成了READ UNCOMMITTED。因此在会话 A 未提交的前提下，事务 B 两次 SELECT 取得了不同结果，即产生了脏读。 脏读现象在生产环境并不经常发生，由上面例子可以看出来，脏读需要隔离级别为READ UNCOMMITTED，实际上大多数数据库至少都是READ COMMITTED，而 InnoDB 默认的都是READ REPEATABLE。 脏读隔离看似毫无用处，但是一些特殊的情况下还可以将事务的隔离级别放开到READ UNCOMMITTED。例如 replication 环境下的 slave 节点，并且在该 slave 节点上的查询并不需要特别精确的返回值。、 不可重复读不可重复读是指在一个事务内多次读取同一个数据集合。在这个事务还没有结束时，另一个事务也访问该同一个数据集合，并做了一些 DML 操作。因此在第一个事务两次读数据之间，由于第二个事务的修改，那么第一个事务读到的数据可能是不一样的。这种情况成为不可重复读。 不可重复读和脏读的区别：脏读读到的是未提交的数据，不可重复读读到的却是提交过的数据，但是其违反了数据库事务一致性的要求。下面的表格展示了一个例子： 这个例子的前提是，两个事务的隔离级别都调整为READ COMMITTED。会话 B 的事务提交之后，事务 A 进行读取，读到的是 1 和 2 两条记录。 一般情况下，不可重复读是可以接受的，因为读到的是已经提交的数据，本身不会带来太大的问题，因此很多数据库的默认级别设置为READ COMMITTED，允许不可重复读。 在 InnoDB 存储引擎中，使用 Next-Key Lock 算法来避免不可重复读的问题。在 Next-Key Lock 算法下，对于索引的扫描，不仅是锁住了扫描到的索引，还锁着了索引覆盖的范围（gap）。因此在这个范围内的插入都是不被允许的。这样就避免了其他事务在这个范围内插入数据导致不可重复读的问题。 丢失更新丢失更新是另一个锁导致的问题，简单来说就是一个事务的更新操作会被另一个事务的更新操作覆盖，从而导致数据不一致。例如： 事务 T1 将行记录 r 更新为 v1，但是事务 T1 尚未提交。 与此同时，事务 T2 将行记录 r 更新为 v2，事务 T2 未提交。 事务 T1 提交。 事务 T2 提交。 虽然在数据库的任何隔离级别下，都不会导致理论意义上的丢失更新问题，即使是READ UNCOMMITTED的事务隔离级别，对于任何行的 DML 操作，需要对行或者其他粗粒度级别的对象加锁。因此上述步骤中，T2 的更新操作不能顺利执行，会被阻塞。 但是，生产中还会出现类似的另一个逻辑意义上的丢失更新问题，尤其是多用户的计算机系统环境下： 事务 T1 查询一行数据，放到本地内存，并显示给一个终端用户 User1。 与此同时，事务 T2 也查询了数据，将数据展示给另一个用户 User2。 User1 修改了这行记录，更新数据库提交。 User2 也修改了记录，更新数据库提交。 显然，这个过程中 User1 的更新操作“丢失”了。为了避免这种问题，需要让事务操作串行化，而不是并行。即在操作步骤 1 中，对用户读取的记录加上一个排他锁 X 。同样，在步骤 2 中，同样需要加入排它锁 X 。通过这种方式，步骤 2 就必须等待步骤 1 和 3 完成。下面的表格演示了如何避免上述逻辑上丢失更新问题的产生。 阻塞由于不同锁之间的兼容关系，有时候一个事务中的锁需要等待另一个事务中的锁释放它占用的资源，这就是阻塞。 在 InnoDB 存储引擎中，参数innodb_lock_wait_timeout控制等待时间，innodb_rollback_on_timeout用来设定是否在等待超时时堆进行中的事务进行回滚操作。参数innodb_lock_wait_timeout是动态的，可以在数据库运行时动态调整，而innodb_rollback_on_timeout是静态的，不能在运行中修改。 死锁死锁的概念死锁是指两个或两个以上事务在执行过程中，因争夺锁资源而造成的一种相互等待的现象。 解决死锁最简单的方式是超时，当两个事务相互等待的时候，当其中一个等待超过阈值，进行回滚，另一个事务就可以继续进行。超时机制虽然简单，但是仅仅通过超时后事务回滚的方式处理，或者根据 FIFO 的顺序选择回滚对象。但如果超时的事务所占权重比较大，如事务操作更新很多航，占用了较多的 undo log，这时候采用 FIFO 的方式就不合适了。 因此，除了超时机制，当前数据库还普遍采用 wait-for graph（等待图）的方式来进行死锁检测。 wait-for graph 要求数据库保存以下两种信息： 锁的信息链表 事务等待链表 通过上述链表可以构造一张图，如果这张图中存在回路，就代表的存在死锁。 wait-for graph 是一种比较主动的死锁检测机制，InnoDB 一般会选择回滚 undo 量最小的事务。 死锁概率死锁的概率推导过程就不抄了，直接截图放这里： 可以看出死锁发生概率与以下几点有关： 系统中事务数量（n），数量越多死锁概率越大。 每个事务的操作量，数量越多死锁概率越大。 操作数据的集合（R），越小则死锁的概率越大。 死锁的示例下面的表演示了一个死锁的经典情况： 上面的例子中，会话 B 抛出了 1213 这个错误提示，即表示事务发生了死锁。大多数死锁 InnoDB 引擎本身可以侦测到，不需要人为进行干预。 还有另外一种情况，即当前事务持有了待插入记录的下一个记录的 X 锁，但是在等待队列中存在一个 S 锁的请求，则可能发生死锁。举个栗子，先创建一个表： 1CREATE TABLE t( a INT PRIMARY KEY)ENGINE&#x3D;INNODB;2INSERT INTO t VALUES (1),(2),(4),(5); 然后运行下表所示的查询： 可以看到，会话 A 中已经对记录 4 持有了 X 锁，但是会话 A 中插入记录 3 时会导致死锁发生。这个问题的产生是由于会话 B 中请求记录 4 的 S 锁而发生等待，但之前请求的锁对于主键值记录 1、2 都已经成功，若在时间点 5 能插入记录，那么会话 B 在获得记录 4 持有的 S 锁之后，还需要向后获得记录 3 的记录，这样就显得不合理。因此 InnoDB 引擎在这里主动选择了死锁，而回滚的是 undo log 记录大的事务，这与 AB-BA 死锁的处理方式又有所不同。 锁升级锁升级（Lock Escalation）是指将当前锁的粒度降低。举例来说，数据库可以吗一个表的 1000 个行锁升级为一个页锁，或者页锁升级为一个表锁。如果数据库设计中认为锁是稀有资源，想要尽量避免锁的开销，就会频繁出现锁升级现象。 InnoDB 存储引擎不存在锁升级的问题。因为其不是根据每个记录来产生行锁的，而是采用位图。不管一个事务锁住页中的一个记录还是多个记录，开销通常都是一致的。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"锁","slug":"锁","permalink":"http://beritra.github.com/tags/%E9%94%81/"}]},{"title":"MySQL中的事务","slug":"MySQL中的事务","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T14:27:13.936Z","comments":true,"path":"2019/12/01/MySQL中的事务/","link":"","permalink":"http://beritra.github.com/2019/12/01/MySQL%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1/","excerpt":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。","text":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。 事务（Transaction）是数据库区别于文件系统的重要特性之一。事务会把数据库从一种一致性状态转换为另一种一致状态。在数据库提交工作时，可以确保要么所有修改都已经保存了，要么所有修改都不保存。 InnoDB 存储引擎中的事务完全符合 ACID 的特性。ACID 是以下 4 个词的缩写： 原子性（atomicity） 一致性（consistency） 隔离性（isolation） 持久性（durability） 本文主要关注事务的原子性这一概念，并说明怎么正确使用事务及编写正确的事务应用程序，避免在事务方面养成一些不好的习惯。 认识事务概述事务可由一条非常简单的 SQL 语句组成，也可以由一组复杂的 SQL 语句组成。事务是访问并更新数据库中各种数据项的一个程序执行单元。在事务操作中，哟啊么都做修改，要么都不做，这就是事务的目的，也是事务模型区别于文件系统的重要特征之一。 理论上说，事务有严格的定义，必须同时满足四个特性，即 ACID 特性。但是数据库厂商出于各种目的，并没有严格去满足事务的 ACID 标准。对于 InnoDB 存储引擎而言，其默认的事务隔离级别为READ REPEATABLE 完全遵循和满足事务的 ACID 特性。这里具体介绍事务的 AICD 特性，并给出相关概念。 A（Atomicity）原子性。在计算机系统中，每个人都将原子性视为理所当然。例如在 C 语言中调用SQRT函数，要么返回正确的平方根值，要么返回错误的代码，而不会在不可预知的情况下改变任何的数据结构和参数。如果SQRT函数被多个程序同时调用，一个程序的返回值也不会是其他程序需要计算的平方根。 然而在数据的事务中实现调用操作的原子性，就没那么简单了，例如一个 ATM 机取款的流程： 登录 ATM 机平台，验证密码。 从远程银行的数据库中，取得账户信息。 用户在 ATM 机上输入欲提取的金额。 从远程银行的数据库中，更新账户信息。 ATM 机出款。 用户取钱。 整个取款流程应当视作原子操作，即要么都做，要么不做。不能用户钱未从 ATM 机上取得，但是银行卡上的钱已经被扣除了，相信这时任何人都不能接受的一种情况。而通过事务模型，可以保证操作的原子性。 原子性是指整个数据库事务是不可分割的工作单位。只有使事务中所有的数据库操作都执行成功，才算整个事务成功。任何一个 SQL 语句执行失败，已经执行成功的 SQL 语句也必须撤销，数据库状态应该退回到执行事务前的状态。 C（consistency），一致性。一致性是指事务将数据库从一种状态转变为下一种一致的状态。在事务开始之前和结束之后，数据库的完整性约束没有被破坏。事务是一致性的单位，如果事务中某个动作失败了，系统可以自动撤销事务，返回初始化的状态。 I（isolation），隔离性。隔离性还有其他称呼，如并发控制（consurrency control）、可串行化（serializability）、锁（locking）等。事务的隔离性要求每个读写事务的对象对其他事务的操作对象能相互分离，即该事务提交前对其他事务都不可见，通常这使用锁来实现。当前数据库系统中都提供了一种粒度锁（granular lock）的策略，允许事务仅锁住一个实体对象的子集，以此来提高事务之间的并发度。 D（durability），持久性。事务一旦提交，其结果就是永久性的。即使发生宕机等故障，数据库也能将数据恢复。需要注意的是，只能从事务本身的角度来保证结果的永久性。例如，在事务提交后，所有的变化都是永久的。即使当数据库发生崩溃而需要恢复数据时，也能保证恢复后提交的数据都不会丢失。但若不是数据库本身的问题，那么数据有可能丢失。因此保证事务系统的高可靠性（High Reliability），而不是高可用性（High Availability）。对于高可用性的实现，事务本身并不能保证，需要一些系统共同配合来完成。 分类从事务理论的角度来说，可以把事务分为以下几种类型： 扁平事务（Flat Transactions） 带有保存点的扁平事务（Flat Transaction with Savepoints） 链事务（Chained Transactions） 嵌套事务（Nested Transaction） 分布式事务（Distributed Transactions） 扁平事务（Flat Transaction）是事务类型中最简单的一种，但在实际生产环境中，可能是最频繁的事务。在扁平事务中，所有操作都处于同一层次，其由BEGIN WORK开始，由COMMIT WORK或ROLLBACK WORK结束，其间的操作是原子的，要么都执行，要么都回滚。因此扁平事务是应用程序成为原子操作的基本组成模块。 下图显示了扁平事务的三种不同结果： 扁平事务的主要限制是不能提交或者回滚事务中的某一部分，或者分步骤提交。下面给出了一个扁平事务不足以支持的例子。例如用户在旅行网站上进行自己的旅行度假计划。用户摄像从杭州道意大利的佛罗伦萨，这两个城市之间没有直达的班级，需要用户预定并转乘航班，或者需要搭火车等待。用户旅行度假的事务为： 1BEGIN WROK2S1：预定杭州到上海的高铁3S2：上海浦东国际机场坐飞机，预定区米兰的航班4S3：在米兰坐火车前往佛罗伦萨，预定去佛罗伦萨的火车 但是当用户执行到 S3 的时候，发现由于飞机到达米兰的时间台湾，已经没有当天的火车。这时用户希望在米兰当地住一晚，第二天出发去佛罗伦萨。这时如果事务为扁平事务，则需要回滚之前 S1、S2、S3 的三个操作，这个代价就显得有点大。因为当再次进行该事务的时候，S1、S2 的执行计划是不变的。也就是说，如果有支持计划的回滚操作，那么就不需要终止整个事务。因此就出现了带有保存点的扁平事务。 带有保存点的扁平事务（Flat Transaction with Savepoint），除了支持扁平事务支持的操作外，允许在事务执行过程中回滚到同一事务中较早的一个状态。这是因为某霞事务可能在执行过程中出现的错误并不会导致所有的操作都无效，放弃整个事务不合乎要求，开销页太大。保存点（Savepoint）用来通知系统应该记住事务当前的状态，以便当以后发生错误时，事务能回到保存点当时的状态。对于扁平的事务来说，其隐式的设置了一个保存点。而在整个事务中，只有一个保存点，因此，回滚只能回滚到事务开始时的状态。保存点用SAVE WORK函数来建立，通知系统记录当前的处理状态。当出现问题的时候，保存点能用作内部的重启动点，根据应用逻辑，决定是回到最近一个保存点还是其他更早的保存点。下图显示了在事务中使用了保存点： 上图显示了如何在事务中使用保存点。灰色背景部分的操作表示由ROLLBACK WORK而导致的部分回滚，实际并没有执行的操作。当用 BEGIN WORK 开启一个事务时，隐式地包含一个保存点，当事务通过ROLLBACK WORK：2 发出部分回滚命令的时候，事务回滚到保存点 2，接着一次执行，并再次执行到ROLLBACK WORK：7，知道最后的COMMIT WORK操作，这时表示事务结束，除灰色阴影部分的操作外，其他的操作都已经执行，并且提交。 另一点需要注意的是，保存点在事务内部是递增的，这从上图中也可以看出。有人可能会想，返回保存点 2 之后，下一个保存点可以是 3，因为之前的动作都终止了。然而新的保存点编号为 5，这意味着ROLLBACK不影响保存点的计数，并且单调递增的编号能保持事务执行的整个历史过程，包括在执行过程中想法的改变。 此外，当事务通过ROLLBACK WORK：2 命令部分回滚命令时，要记住事务并没有完全被回滚，只是回滚到了保存点 2 而已。这代表当前事务还是活跃的，如果想要完全回滚事务，还需要再执行命令ROLLBACK WORK。 链事务（Chained Transaction）可以视为是保存点模式的一种变种。带有保存点的扁平事务，当事务发生崩溃的时候，所有保存点都将小时，因为其保存点是易失的（volatile），而非持久的（persistent）。这意味着当进行恢复时，事务需要从开始处重新执行，而不能从最近的一个保存点继续执行。 链事务的思想是：在提交一个事务时，释放不需要的数据对象，将必要的数据处理上下文隐式地传给下一个要开始的事务。注意，提交事务操作和开始下一个事务操作将合并为一个原子操作。这意味着下一个事务将看到上一个事务的结果，就好像在一个事务中进行的一样。下图显示了链事务的工作方式： 链事务与带有保存点的扁平事务不同的是，带有保存点的扁平事务能回滚到任意正确的保存点。而链事务中的回滚仅限于当前事务，即只能恢复到最近的一个保存点。对于锁的处理，两者也不相同。链事务在执行COMMIT后即释放了当前事务所持有的锁， 而带有保存点的扁平事务不影响迄今为止所持有的锁。 嵌套事务（Nested Transaction）是一个层次结构框架。由一个顶层事务（top-level transaction）控制着各个层次的事务。顶层事务之下嵌套的事务被称为子事务（subtransaction），其控制着每一个局部的变量。嵌套事务的层次结构如图所示： 下面是 Moss 对嵌套事务的定义： 嵌套事务是由若干事务组成的一颗树，子树既可以是嵌套事务，也可以是扁平事务。 处在叶节点的事务是扁平事务。但是每个子事务从根到叶节点的距离可以是不同的。 处于根节点的事务成为顶层事务，其他事物称为子事务。事务的前驱（predecessor）称为父事务（parent），事务的下一层成为儿子事务（child）。 子事务既可以提交也可以回滚。但是它的提交操作并不会马上生效，除非其父事务已经提交。由此可以推论出，任何子事务都在顶层事务提交后才真正的提交。 树中任何一个事务的回滚都会引起它所有的子事务一同回滚，故子事务仅保留 A、C、I 特性，不具有 D 的特性。 根据 Moss 的理论，实际的工作是交由叶子节点来完成的，即只有叶子节点的事务才能访问数据库、发送消息、获取其他类型的资源。而高层的事务仅负责逻辑控制，决定何时调用相关的子事务。即使一个系统不支持嵌套事务，用户也可以通过保存点技术来模拟嵌套事务，如图所示： 从图中也可以发现，在恢复时采用保存点技术比嵌套查询有更大的灵活性。例如在完成 Tk3 这个事务的时候，可以回滚到保存点 S2 的状态。而在嵌套查询的层次结构中，这是不被允许的。 但是用保存点技术来模拟嵌套事务在锁的持有方面还是与嵌套查询有些区别。当通过保存点技术来模拟嵌套事务时，用户无法选择哪些锁需要被子事务继承，哪些需要被父事务保留。就是说：无论有多少个保存点，所有被锁住的对象都可以被得到和访问。而在查询嵌套中，不同的子事务在数据库对象上持有的锁是不同的。 例如有一个父事务 P1，其持有对象 X 和 Y 的排它锁，现在要开始调用子事务 p11，那么父事务 P1 可以不传递锁，也可以传递所有的锁，也可以只传递一个排他锁。如果子事务 P11 中还有持有对象 X、Y、Z 的排他锁。如果这时又再次调用了一个子事务 P12，那么它可以传递那里已经持有的锁。 然而，如果系统支持在嵌套事务中并行地执行各个事务，在这种情况下，采用保存点的扁平事务来模拟嵌套事务就不切实际了。者从另一个方面反映出，想要实现事务键的并行性，需要真正支持的嵌套事务。 分布式事务（Distributed Transaction）通常是在一个分布式环境下运行的扁平事务，因此需要根据数据所在位置访问网络中的不同节点。 假设一个用户在 ATM 机上进行银行的转账操作，例如持卡人从招商银行的储蓄卡转账 10000 元到工商银行的储蓄卡。在这汇总情况下，可以将 ATM 机视为节点 A，招商银行的后台数据库视为节点 B，工商银行的后台数据库视为 C，这个转账操作可以分解为以下的步骤： 节点 A 发出转账命令。 节点 B 执行储蓄卡中的余额值减去 10000。 节点 C 执行储蓄卡中的余额值加上 10000。 节点 A 通知用户完成或者节点 A 通知用户操作失败。 这里需要使用分布式事务，因为节点 A 不能通过调用一台数据库就完成任务，其需要访问网络中两个节点的数据库，而在每个节点中的数据库执行的事务操作都又是扁平的。对于分布式事务，其同样需要满足 ACID 特性，要么都发生，要么都失效。对于上述例子，如果 2、3 步中任何一个操作失败，都会导致整个分布式事务回滚。 对于 InnoDB 存储引擎来说，支持扁平事务、带有保存点的事务、链事务、分布式事务。对于嵌套事务，其并不原生支持，因此，对于有并行事务需求的用户来说，MySQL 数据库或 InnoDB 存储引擎就显得无能为力了。然而用户仍可以通过带有保存点的事务来模拟串行的嵌套事务。 事务的实现事务的隔离性由锁来实现。原子性、一致性、持久性通过数据库的 redo log 或 undo log 来完成。redo log 又称为重做日志，用来保证事务的原子性和持久性。undo log 用来保证事务的一致性。 有人可能会认为 undo 是 redo 的逆过程，其实不然。redo 和 undo 的作用都可以视为是一种恢复操作，redo 恢复提交事务修改的页操作，而 undo 回滚行记录到某个特定版本。因此两者记录的内容不同，redo 通常是物理日志，记录的是页的屋里操作改动，undo 是逻辑日志，根据每行记录进行记录。 redo重做日志用来实现事务的持久性，即事务 ACID 中的 D。其由两部分组成：一是内存中的重做日志缓冲（redo log buffer），其是易失的；二是重做日志文件（redo log file），其是持久的。 InnoDB 是事务的存储引擎，通过Force Log at Commit机制实现事务的持久性，即当事务提交（COMMIT）的时候，必须先把事务的所有日志写入到重做日志文件进行持久化，等事务的提交操作完成之后才算完成。这里的日志是指重做日志，在 InnoDB 存储引擎中，由两部分组成，即 redo log 和 undo log。redo log 用来保证事务的持久性，undo log 用来帮助事务回滚以及 MVCC 的功能。redo log 基本上都是顺序写的，在数据库运行时不需要对 redo log 的文件进行读取操作。而 undo log 是需要进行随机读写的。 为了确保每次日志都写入重做日志文件，在每次日志缓冲写入重做日志文件之后，InnoDB 存储引擎都需要调用一次fsync操作。由于重做日志文件打开并没有使用O_DIRECT选项，因此重做日志缓冲先写入文件系统缓存。由于fsync的效率取决于磁盘性能，因此磁盘的性能决定了事务提交的性能，也就是数据库的性能。 InnoDB 存储引擎允许用户手动设置非持久性的情况发生，因此提高数据库的性能。当事务提交的时候，日志不写入重做日志文件，而是等待一个时间周期后再执行fsync操作。由于并非强制在事务提交的时候进行一次fync操作，显然可以提高数据库性能。但是在数据库发生宕机的时候，由于部分日志未发刷新到磁盘，因此会丢失最后一段时间的事务。 参数inndob_flush_log_at_trx_commit用来控制重做日志刷新到磁盘的策略，默认值是 1，表示每次都会调用fsync操作。这个参数还可以设置为 0 或者 2。0 表示事务提交时不进行写入重做日志操作，这个操作仅在 mater thread 中完成，而在 master thread 中每一秒会进行一次fsync操作。2 表示事务提交时将重做日志写入重做日志文件，但仅写入文件系统的缓存中，不进行 fsync 操作。在这个设置下，当 MySQL 数据库发生宕机而操作系统不发生宕机时，并不会导致事务的丢失。但是当操作系统宕机时，重启数据库后会丢失未从文件系统缓存刷新到重做日志文件那部分事务。 下图是模拟插入 50 万行数据，参数为 1 是每插入一条就显示执行一次 COMMIT 操作，跟其他参数设置之间的数据库效率对比： 虽然用户可以通过设置参数innodb_flush_log_at_trx_commit为 0 或 2 来提高事务提交的性能，但是需要牢记的事，这种设置方法丧失了事务的 ACID 特性。而针对上述存储过程，为了提高事务的提交性能，应该在将 50 万行数据插入表后进行一次 COMMIT 操作，而不是在每插入一条记录之后进行一次 COMMIT 操作。 在 MySQL 数据库中还有一种二进制日志（binlog），其用来进行 POINT-TIME（PIT）的恢复及主从复制（Replication）环境的建立。从表面上看其和重做日志非常相似，都是记录了对于数据库操作的日志。然而，从本质上来看，二者有着非常大的不同。 首先，重做日志是在 InnoDB 存储引擎层产生，而二进制日志是在 MySQL 数据库的上层产生的，并且二进制日志不仅仅对于 InnoDB 引擎，MySQL 数据库中的任何存储引擎对于数据库的更改都会产生二进制日志。 其次，两种日志记录的内容形式不同。MySQL 数据库上层的二进制日志是一种逻辑日志，其记录的是对应的 SQL 语句。而 InnoDB 存储引擎的重做日志在事务进行中不断地被写入，这表现为日志并不是随事务提交的顺序进行写入的。 undo重做日志记录的事务的行为，可以很好地通过其对页进行“重做”操作。但是事务有时候还会进行回滚操作，这时就需要 undo。因此在对数据库在进行修改的时候，InnoDB 存储引擎不但会产生 redo，还会产生一定量的 undo。这样如果用户执行的事务或语句由于某种原因失败了，又或者用户用一条ROLLBACK语句请求回滚，就可以利用这些 undo 信息将数据回滚到修改之前的样子。 redo 存放在重做日志文件中，与 redo 不同，undo 存放在数据库内部的一个特殊段（segment）中，这个段称为 undo 段（undo segment）。undo 段位于共享表空间内。可以通过py_innodb_page_info.py工具来查看当前共享表空间中 undo 的数量。 用户通常对 undo 有这样的误解：undo 用于将数据库物理的恢复到执行语句或事务之前的样子——但事实并非如此。undo是逻辑日志，因此只将数据库逻辑的恢复到原来的样子。所有修改都被逻辑地取消掉了，但是数据结构和页本身在回滚之后可能大不相同。这是因为在多用户并发系统中，可能会有数十、数百甚至数千个并发事务。数据库的主要任务就是协调堆数据记录的并发访问。比如，一个事务在修改当前一个页中某几条记录，同时还有别的事务在对同一个页中另几条记录进行修改。因此，不能将一个页回滚到事务刚开始的样子，因为这样会影响其他事务正在进行的工作。 例如，用户执行了一个 INSERT 10w 条记录的事务，这个事务会导致分配一个新的段，即表空间会增大。在用户执行ROLLBACK时，会将插入的事务进行回滚，但是表空间的大小不会因此收缩。因此，当 InnoDB 存储引擎回滚时，它实际上做的是与先前相反的工作。对于每个 INSERT ，InnoDB 存储引擎会完成一个 DELETE；对于每个 DELETE，InnoDB 存储引擎会执行一个 INSERT；对于每个 UPDATE，InnoDB 引擎会执行一个相反的 UPDATE，将修改前的行放进去。 除了回滚操作，undo 的另一个作用是 MVCC，即在 InnoDB 存储引擎中 MVCC 的实现是通过 undo 来完成。当用户读取一行记录时，若该记录已经被其他事务占用，当前事务可以通过 undo 读取之前的行版本信息，以此实现非锁定读取。 最后也是最重要的一点是，undo log 会产生redo log，也就是 undo log 的产生会伴随着 redo log 的产生，这是因为 undo log 也需要持久性的保护。 prugedelete 和 update 操作可能不会直接删除原有的数据。假设有如下表 t： 1CREATE TABLE t(2a INT，3b VARCHAR(32)，4PRIMARY KEY(a)，5KEY(n)6)ENGINE&#x3D;Innodb; 对其执行 如下 SQL 语句： 1DELETE FROM t WHERE a&#x3D;1; 表 t 上列 a 有聚集索引，列 b 上有辅助索引。对于上述 delete 操作，仅仅是将主键列等于 1 的记录delete flag设置为 1，记录并没有被删除，即记录还存在于 B+ 树中。其次，对辅助索引上 a 等于 1，b等于 1 的记录同样没有做任何处理，甚至没有产生 undo log。而真正删除这样记录的操作其实被“延时”了，最终在 purge 操作中完成。 purge 用于最终完成 delete 和 update 操作。这样设计是因为 InnoDB 存储引擎支持 MVCC，所以记录不能在事务提交时立即进行处理。这时其他事务可能正在引用这行，故 InnoDB 存储引擎需要保存记录之前的版本。而是否可以删除改条记录通过 purge 来进行判断。若改行记录已经不被其他任何事务引用，那么就可以进行真正的 delete 操作。可见，purge 操作是清理之前的 delete 和 update 操作，故上述操作“最终”完成。而实际执行的操作为 delete 操作，清理之前行记录的版本。 group commit若事务为非只读事务，则每次事务提交时需要进行一次fsync操作，以保证重做日志都已经写入磁盘。当数据库发生宕机时，可以通过重做日志进行恢复。虽然固态硬盘的出现提高了磁盘的性能，然而磁盘的fsync性能是有限的。为了提高磁盘fsync的效率，当前数据库都提供了group commit的功能，即一次fsync可以刷新确保多个事务日志被写入文件。对于InnoDB 存储引擎来说，事务提交时会进行两个阶段的操作： 修改内存中事务对应的信息，并且将日志写入重做日志缓冲。 调用fsync将确保日志都从重做日志缓冲个写入磁盘。 步骤 2 相对于步骤 1 是一个较慢的过程，这是因为存储引擎需要与磁盘打交道。但当有事务进行这个过程时，其他事务可以进行步骤 1 的操作，正在提交的事务完成提交操作后，再次执行步骤 2 时，就可以将多个事务的重做日志通过一次fsync刷新到磁盘，这样就大大减少了磁盘的压力，从而提高了数据库的整体性能。对于写入或更新较为频繁的操作，group commit的效果尤为明显。 事务控制语句在 MySQL 命令行的默认设置下，事务都是自动提交（auto commit）的，即执行 SQL 语句之后就会马上执行 COMMIT 操作。因此要显式的开启一个事务需要使用命令BEGIN、START TRANSACTION，或者执行命令SET AUTOCOMMIT=0，禁用当前会话的自动提交。每个数据库厂商自动提交的设置都不相同，每个 DBA 或开发人员需要非常明白这一点，这对之后的 SQL 编程会有非凡的意义，因此用户不能以之前的经验来判断 MySQL 数据库的运行方式。在具体介绍其含义之前，先看看有哪些事务控制语句： START TRANSACTION|BEGIN：显式地开启一个事务。 COMMIT：提交事务，等价于COMMIT WORK。 ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。等价于ROLLBACK WORK SAVEPOINT identifier：SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个保存点。 ROLLBACK TO[SAVEPOINT] identifier：这个语句与SAVEPOINT命令一起使用。可以把事务回滚到标记点，而不回滚在此标记点之前的任何工作。例如可以发出两条UPDATE语句，后面跟一个SAVEPOINT，然后又是两条DELETE语句。如果执行DELETE语句期间出现了某种异常情况，并且捕获到了这个异常，同时发出了ROLLBACK TO SAVEPOINT命令，事务就会回滚到指定的SAVEPOINT，撤销DELETE完成的所有工作，而UPDATE语句完成的工作不受影响。 SET TRANSACTION：这个语句用来设置事物的隔离级别。InnoDB 存储引擎提供的事务隔离级别有：READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ、SERIALISZABLE。 START TRANSACTION、BEGIN语句都可以在 MySQL 命令行下显式地开启一个事务。但是在存储过程中，MySQL数据库的分析器会自动将BEGIN识别为BEGIN END，因此在存储过程中只能使用START TRANSACTION语句来开启一个事务。 COMMIT和COMMIT WORK语句基本是一致的，都是用来提交事务。不同之处在于COMMIT WORK用来控制事务结束后的行为是CHAIN还是RELEASE的。如果是CHAIN方式，那么事务就变成了链事务。 InnoDB 存储引擎中的事务都是原子的，这说明下述两种情况：构成事务的每条语句都会提交（成为永久），或者所有语句都回滚。这种保护还延伸到单个的语句。一套语句要么完全成功，要么完全回滚（注意，这里说的是语句回滚）。因此一条语句失败并抛出异常时，并不会导致先前已经执行的语句自动回滚。所有的执行都会得到保留，必须由用户自己来决定是否对其进行提交或回滚的操作。 另一个容易犯的错误是ROLLBACK TO SAVEPOINT，虽然有ROLLBACK，但其实并不真正的结束一个事务，因此即使执行了ROLLBACK TO SAVEPOINT，之后也需要显式地运行COMMIT或ROLLBACK命令。 隐式提交的 SQL 语句以下 SQL 语句会产生一个隐式的提交操作，即执行完这些语句后，会有一个隐式的COMMIT操作。 DDL 语句：ALTER DATABASE...UPGRADE DATA DIRECTORY NAME，ALTER EVENT，ALTER PROCEDURE，ALTER TABLE，ALTER VIEW，CREATE DATABASE，CREATE ENENT，CREATE INDEX，CREATE PROCEDURE，CREATE TABLE，CREATE TRIGGER，CREATE VIEW，DROP DATABASE，DROP EVENT，DROP INDEX，DROP PROCEDURE，DROP TABLE，DROP TRIGGER，DROP VIEW，RENAME TABLE，TRAUNCATE TABLE 用来隐式地修改 MySQL 架构的操作：CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD 管理语句：ANALYZE TABLE、CACHE INDEX、CHECK TABLE、LOAD INDEX INTO CACHE、OPTIMEIZE TABLE、REPAIR TABLE、 另外需要注意的是，TRUNCATE TABLE语句是 DDL，因此虽然和对整张表执行DELETE的结果是一样的，但它不能回滚的。 对于事务操作的统计对于 InnoDB 存储引擎是支持事务的，因此 InnoDB 存储引擎的应用需要在考虑每秒请求数（Question Per Second，QPS）的同时，应该关注每秒事务处理的能力（Transaction Per Second，TPS）。 计算 TPS 的方法是（com_commit+com_rollback）/time。但是利用这种方法进行计算的前提是：所有的事务必须都是显式的提交，如果存在隐式地提交和回滚（默认 autocommit=1），不会计算到com_commit和com_rollback中。 事务的隔离级别SQL 标准定义的四个隔离级别： READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE InnoDB 存储引擎默认支持的隔离级别是 REPEATABLE READ，但与标准 SQL 不同的是，InnoDB 存储引擎在 REPEATABLE READ 事务隔离级别下，使用 Next-Key Lock 锁的算法，因此避免幻读的产生。所以说，InnoDB 存储引擎在默认的 REPEATABLE READ的事务隔离级别下，已经能完全保证事务的隔离性要求，即达到 SQL 标准的 SERIALIZABLE 隔离级别。 隔离级别越低，事务请求的锁越少或保持锁的时间就越短，这也是为什么大多数数据库的默认隔离级别是 READ COMMITTED。 大部分用户质疑 SERIALIZABLE 隔离级别带来的性能问题，但是两者的开销几乎是一样的，甚至SERIALIZABLE 可能更优！因此在 InnoDB 存储引擎中选择 REPEATABLE READ 的事务隔离级别并不会有任何性能的损失。同样的，即使使用 READ COMMITTED 的隔离级别，用户也不会得到性能的大幅提升。 在 InnoDB 存储引擎中，可以使用以下命令来设置当前会话或全局的事务隔离级别： 1SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL&#123;2READ UNCOMMITTED3READ COMMITTED4REPEATABLE READ5SERIALIZABLE6&#125; 如果想在 MySQL 数据库启动时就设置事务的默认隔离级别，那就需要修改 MySQL 的配置文件，在[mysqld]中添加如下行： 1[mysqld]2transaction-isolation &#x3D; READ-COMMITTED 查看当前会话的事务隔离级别，可以使用： 1SELECT @@transaction_isolation\\G; 查看全局的事务隔离级别，可以使用： 1SELECT @@global.transaction_isolation\\G; 在 SERIALIABLE 的隔离级别下，InnoDB 引擎会堆没个 SELECT 语句后自动加上一个LOCK IN SHARE MODE，即为没个读取操作加一个共享锁。因此在这个事务隔离级别下，读占用了锁，对一致性的非锁定读不再予以支持。这时，事务隔离级别 SERIALIZABLE 符合数据库理论上的要求，即事务是 well-formed 的，并且是 two-phrased 的。 分布式事务MySQL 数据库分布式事务InnoDB 存储引擎提供了对 XA 事务的支持，并通过 XA 事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transaction resources）参与到一个全局事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都会滚，这对事务原有的 ACID 要求又有了提高。另外，在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE。 XA 事务允许你不同数据库之间的分布式事务，如一台数据库是 MySQL 数据库的，另一台是 Oracle 的，有可能还有一台是 SQL SERVER 数据库的，只要参与到全局事务中的每个节点都支持 XA 事务。 XA 事务由一个或多个资源管理器（Resource MAanagers）、一个事务管理器（Transaction Manager）以及一个应用程序（Application Program）组成。 资源管理器：提供访问事务资源的方法。通常一个数据库就是一个资源管理器。 事务管理器：协调参与全局事务中的各个事务。需要和参与全局事务的所有资源管理器进行通信。 应用程序：定义事务的边界，指定全局事务中的操作。 在 MySQL 数据库的分布式事务中，资源管理器就是 MySQL 数据库，事务管理器为连接 MySQL 服务器的客户端。下图显示了一个分布式事务的模型： 分布式事务使用两段式提交（two-phase commit）的方式。在第一阶段，所有参与全局事务的节点都开始准备（PREPARE），告诉事务管理器他们准备号提交了。在第二阶段，事务管理器告诉资源管理器执行ROLLBACK还是COMMIT。如果任何一个节点显示不能提交，则所有的节点都被告知需要回滚。可见与本地事务不同的是，分布式事务需要多一次 PREPARE 的操作，待收到所有节点的同意信息后，再进行COMMIT或是ROLLBACK操作。 内部 XA 事务之前讨论的分布式事务是外部事务，即资源管理器是 MySQL 本身。在 MySQL 数据库中还存在另一种分布式事务，其在存储引擎与插件之间，又或者在存储引擎与存储引擎之间，称之为内部 XA 事务。】 最常见的内部 XA 事务存在于 binlog 与 InnoDB 存储引擎之间。由于复制的需要，因此目前绝大多数的数据库都开启了 binlog 功能。在事务提交时，先写二进制日志，再写 InnoDB 存储引擎的重做日志。对上述两个操作的要求也是原子的。即二进制日志和重做日志必须同时写入。若 二进制日志先写了，而在写入 InnoDB 存储引擎时发生了宕机，那么 slave 可能会接收到 master 传过去的二进制日志并执行，最终导致了主从不一致的情况。 为了解决这个问题，MySQL 数据库在 binlog 与 InnoDB 存储引擎之间采用 XA 事务。当事务提交的时候，InnoDB 存储引擎回西安做一个 PREPARE 操作，将事务的 xid 写入，接着二进制日志写入，入股偶在 InnoDB 存储引擎提交前，MySQL 数据库宕机了，那么 MySQL 数据库在重启后会先检查准备的 UXID 事务是否已经提交，若没有，则在存储引擎层再进行一次提交操作。如下图所示： 不好的事务习惯在循环中提交有的开发人员习惯再循环中进行事务的提交，比如可能常写的一个存储过程： 这个里面的COMMIT命令其实并不关键。因为 InnoDB 存储引擎默认自动提交，这也是经常被开发人员忽视的问题： 其实无论上面哪个存储过程都存在一个问题，当发生错误的时候，数据库会停留在一个位置的位置。例如，用户需要插入 10000 条记录，但是当插入 5000 条时，发生了错误，这时前 5000 条记录都已经存放在数据库中，那应该怎么处理呐？另外一个问题性能问题，上面两个存储过程都不会比下面的存储过程更快，因为下面的存储过程将所有的INSERT都放在了一个事务中： 第三种方法要快的多！这是因为每次提交都要写一次重做日志，存储过程 load1 和 load2 实际上写了 10000 次重做日志文件，而对存储过程 load3 来说，实际上只写了 1 次。 所以无论从何种角度出发，都不应该在一个循环中反复进行提交操作，无论是显示还是隐式的提交。 使用自动提交使用自动提交不是一个好习惯，因为这会使初级 DBA 容易犯错，另外还可能是一些开发人员产生错误的理解。在编写程序开发时，最好把事务的控制权限交给开发人员，即在程序端进行事务的开始和结束。同时，开发人员必须了解是自动提交可能带来的问题。 使用自动回滚InnoDB 存储引擎支持通过定义一个 HANDLER 来进行自动事务的回滚操作，如在一个存储过程中发生了错误会自动对其进行回滚操作。因此很多开发人员喜欢在应用程序的存储过程中使用自动回滚操作。 在存储过程中使用自动回滚很容易丢失错误信息，所以应该在程序中控制而不是数据库中。在程序中控制事务的好处是用户可以得知发生错误的原因，然后根据发生的原因进一步调试程序。 长事务长事务（Loing-Lived Transaction），顾名思义，就是执行时间较长的事务，比如，对于银行系统中的数据库，每过一个阶段可能需要更新对应账户的利息。如果对应账户的数量非常大，例如对有 1 亿用户的表进行更新操作，可能需要非常长的时间来完成，可能需要一个小时，也可能需要 4、5 个小时，这取决于数据库的硬件配置。然而，由于事务的 ACID 特性，这个操作被封装在一个事务中完成。这就产生了一个问题，在执行过程中，当数据库、操作系统或者硬件发生问题的时候，重新开始事务的代价变得不可接受。数据库需要回滚所有已经发生的变化，而这个过程可能比产生这些变化的时间还要长。因此，对于长事务的问题，有时候可以通过转化为小霹雳（mini batch）的事务来进行处理。当事务发生错误时，只需要回滚一部分数据，然后接着上次已完成的事务继续进行。 由一个小地方还需要注意，要保证事务在处理工程中，没有其他的事务来更新表中的数据，需要人工加一个共享锁。 小结我们了解了 InnoDB 存储引擎管理事务的许多方面。了解了事务如何工作以及如何使用。 事务必须遵循 ACID 特性，即 Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）和 Durability（持久性） 。隔离性通过锁来完成；原子性、一致性、隔离性通过 redo 和 undo 来完成。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"事务","slug":"事务","permalink":"http://beritra.github.com/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"Keepalived+MySQL双主高可用配置","slug":"离线环境下Keepalived+MySQL 8.0安装和双主高可用配置","date":"2019-01-01T04:12:12.000Z","updated":"2019-12-15T14:37:31.604Z","comments":true,"path":"2019/01/01/离线环境下Keepalived+MySQL 8.0安装和双主高可用配置/","link":"","permalink":"http://beritra.github.com/2019/01/01/%E7%A6%BB%E7%BA%BF%E7%8E%AF%E5%A2%83%E4%B8%8BKeepalived+MySQL%208.0%E5%AE%89%E8%A3%85%E5%92%8C%E5%8F%8C%E4%B8%BB%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"记录一个Keepalived+MySQL 双主高可用配置的配置过程。","text":"记录一个Keepalived+MySQL 双主高可用配置的配置过程。 系统环境准备两台服务器IP： 10.17.55.201主要的 10.17.55.202次要的 两台系统版本均为 CentOS Linux release 7.4.1708 (Core) MySQL官网上下载mysql-8.0.18-1.el7.x86_64.rpm-bundle.tar，即适用于CentOS的MySQL 8.0.18 组件安装CentOS可能带有默认的Mariadb，如果不想用的话可以执行rpm -qa|grep mariadb找到所有安装包，然后依次执行rpm -r 每个文件名卸载Mariadb。 然后从官网上下载相应的MySQL安装文件。 安装顺序为 1rpm -ivh mysql-community-common-5.7.17-1.el7.x86_64.rpm 2rpm -ivh mysql-community-libs-5.7.17-1.el7.x86_64.rpm 3rpm -ivh mysql-community-client-5.7.17-1.el7.x86_64.rpm 4rpm -ivh mysql-community-server-5.7.17-1.el7.x86_64.rpm 5rpm -ivh mysql-community-devel-5.7.17-1.el7.x86_64.rpm 安装最后一个的时候有可能出现依赖openssl的问题，重新安装openssl无效，输入参数-e --nodeps可以解决，暂不清楚原理 数据库配置先从主备模式开始进行配置，方便排错。 主实例修改配置文件/etc/my.cnf 1[mysqld]2log-bin&#x3D;mysql-bin #日志名称3server-id&#x3D;1 #数据库id,两个实例不能一样 然后创建用户，用于数据同步，MySQL 8.0之后的用户创建方式和之前不一样 1CREATE USER &#39;repl&#39;@&#39;10.17.55.202&#39; IDENTIFIED WITH mysql_native_password BY &#39;1&#39;;2GRANT REPLICATION SLAVE ON *.* TO &#39;repl&#39;@&#39;10.17.55.202&#39;; 然后刷新权限 1flush privileges; 查看主节点的binary log文件名和位置： 1show master status; 记录下来File和Position两个字段，下面要用。 备用实例在从节点上设置参数： 1CHANGE MASTER TO MASTER_HOST&#x3D;&#39;10.17.55.201&#39;,MASTER_USER&#x3D;&#39;repl&#39;,MASTER_PASSWORD&#x3D;&#39;1&#39;,MASTER_LOG_FILE&#x3D;&#39;mysql-bin.000002&#39;,MASTER_LOG_POS&#x3D;1141; 然后启动从节点start slave，并且查看节点状态show slave status\\G 如果看到节点的状态信息中包含一下的两条，证明连接成功。 1Slave_IO_State: Waiting for master to send event2Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates 然后验证一下： 切换回主节点，新建一个数据库或者表，从节点中应该同步添加了。 双主环境以上搭建的是MySQL主备模式，双主模式其实一样，只不过两者之间互为主备。 同样的，把之前的备用节点当做主节点，之前的主节点当做备用节点，再来一遍。不要忘记修改my.cnf。 配置完成之后继续新建一个数据库或者表，测试是否同步。 Keepalived安装和配置CentOS上直接可以用yum安装Keepalived，省了不少事情，不过可能遇到错误 1Error: Package: 1:net-snmp-agent-libs-5.7.2-43.el7.x86_64 (base)2 Requires: libmysqlclient.so.18()(64bit)3Error: Package: 1:net-snmp-agent-libs-5.7.2-43.el7.x86_64 (base)4 Requires: libmysqlclient.so.18(libmysqlclient_18)(64bit) 安装MySQL安装包中的libs-compat即可 然后对keepalived进行配置，最简配置如下： 主节点： 1! Configuration File for keepalived23global_defs &#123;4 router_id master5&#125;67vrrp_instance VI_1 &#123;8 state MASTER9 interface eth110 virtual_router_id 5111 priority 10012 advert_int 113 unicast_src_ip 172.18.0.214 unicast_peer &#123;15 1172.18.0.316 &#125;17 authentication &#123;18 auth_type PASS19 auth_pass 111120 &#125;21 virtual_ipaddress &#123;22 172.18.0.1023 &#125;24&#125; 从节点： 1! Configuration File for keepalived23global_defs &#123;4 router_id master5&#125;67vrrp_instance VI_1 &#123;8 state MASTER9 interface eth110 virtual_router_id 5111 priority 9012 advert_int 113 unicast_src_ip 172.18.0.314 unicast_peer &#123;15 1172.18.0.216 &#125;17 authentication &#123;18 auth_type PASS19 auth_pass 111120 &#125;21 virtual_ipaddress &#123;22 172.18.0.1023 &#125;24&#125; 主要有几点重要的地方，一个是router_id不能一样，这个是区分每个实力上的标志。 然后state可以都为master，即按照权重来强占。 interface填写的是网卡名称，这个要结合自己实际的网卡名。 virtual_router_id在同一个网络环境下不可以重复，不然会出问题。 priority是权重，按照这个值来确定主节点。 一般采用单播的形式，方便配置而且不影响其他服务，所以两个配置文件中的unicast_src_ip和unicast_peer是相反的。 virtual_ipaddress填写需要使用的虚拟IP就可以了。 如果是在容器中创建虚拟IP，有可能遇到错误： 1IPVS: Can&#39;t initialize ipvs: Protocol not available 开启的必要条件有两个： 容器放开权限，即添加参数--privileged 宿主机同样需要开启ipvasdm 测试输入ip a应该就可以看到虚拟ip出现在网络信息中。 没有这个命令的话通过yum install iproute安装 附录MySQL状态监测脚本1#!/bin/bash2pgrep -x mysqld &amp;&gt; /dev/null3if [ $? -ne 0 ]4then5 echo \"not running\"6else7 echo \"running\"8fi Nginx状态监测脚本FastDFS检测脚本FTP检测脚本","categories":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/"},{"name":"MySQL","slug":"运维/MySQL","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/MySQL/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"Keepalived","slug":"Keepalived","permalink":"http://beritra.github.com/tags/Keepalived/"}]},{"title":"深入理解JVM虚拟机笔记","slug":"深入理解JVM虚拟机笔记","date":"2019-01-01T04:12:12.000Z","updated":"2019-12-15T13:22:04.703Z","comments":true,"path":"2019/01/01/深入理解JVM虚拟机笔记/","link":"","permalink":"http://beritra.github.com/2019/01/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AC%94%E8%AE%B0/","excerpt":"本文是《深入理解JVM虚拟机》的读书笔记和摘要","text":"本文是《深入理解JVM虚拟机》的读书笔记和摘要 内存管理运行时数据区域根据Java1.7版本的虚拟机规范，Java虚拟机包括以下几个运行时数据区。 一、程序计数器程序计数器(Program Counter Register)是一块较小的内存空间，可以看做当前线程所执行的字节码的行号指示器。分支、循环、跳转、异常处理、线程恢复等等都依赖于计数器完成。 每个线程拥有独立的计数器，互相不影响，独立存储。 执行Java方法的时候计数器记录的是虚拟机字节码指令的地址，如果执行的是Native方法，那么计数器则为空(Undefined)。该内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 二、虚拟机栈和程序计数器一样，Java虚拟机栈(Java Virtual Machine Stacks)也是线程私有的，它的生命周期和线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个帧栈(Stack Frame)用来存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成，都对应着一个栈帧在虚拟机中入栈到出栈的过程。 常规把Java内存区分为堆内存和栈内存的方法过于粗糙，实际上的区域划分更加复杂。 局部变量表存放了编译期可知的各种基本类型(boolean、byte、char、short、int、float、long、double)、对象引用(reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)、和returnAddress类型（指向一条字节码指令的地址）。 64位长度的long和double类型会占用两个局部变量空间（Slot）,其余的数据类型占用一个。进入一个方法的时候，局部表量表的打小就是确定的，运行期间不会在改变。 Java虚拟机规范中对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将会抛出StackOverFlowError异常。如果虚拟机栈可以动态扩展，扩展时无法申请足够内存，会抛出OutOfMemoryError异常。 总空间一定的情况下，局部变量表内容越多，栈帧越大，栈深度越小。进行大量递归的时候就有可能导致栈溢出。 三、本地方法栈本地方法栈(Native Method Stack)和虚拟机栈作用很相似，两者区别是后者为Java方法（字节码）服务，前者则为虚拟机使用到的Native方法服务。有的虚拟机就干脆合二为一（Sun HotSpot虚拟机），本地方法栈可能抛出的异常也是上面那两个。 四、Java堆对于多数应用来说，Java堆（Java Heap）是Java虚拟机管理的最大一块内存。Java堆被所有线程共享，在虚拟机启动的时候创建。此内存区域的唯一目的就是存放对象实例。Java虚拟机规范中规定所有的对象实例和数组都要在堆上分配，但是随着发展，现在变得并不这么绝对。 Java堆内存是垃圾收集管理器管理的主要区域，所以也成为GC堆。从内存回收的角度来看，现代收集器都采用分代收集算法，所以Java堆还可以细分为：新生代和老年代。再细致一点有Eden空间、From Survivor空间、To Survivor空间。 Java堆可以处在物理不连续的内存空间上，只要逻辑连续即可。 五、方法区方法区（Method Area）也是所有线程共享的内存区域，用于存储虚拟机加载的类信息、常量、静态变量、即时编译器编译的代码等数据。也称作Non-Heap非堆内存。 Java虚拟机规范堆方法区的限制非常宽松，可以选择不实现垃圾收集，但是这部分区域的回收确实是有必要的。 平时，说到永久带(PermGen space)的时候往往将其和方法区不加区别。这么理解在一定角度也说的过去。因为，《Java虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 同时，大多数用的JVM都是Sun公司的HotSpot。在HotSpot上把GC分代收集扩展至方法区，或者说使用永久代来实现方法区。 在JDK1.8及以后版本，永久带被移除，新出现的元空间（Metaspace）替代了它。元空间属于Native Memory Space 在1.8中，可以使用如下参数来调节方法区的大小 XX:MetaspaceSize 元空间初始大小 XX:MaxMetaspaceSize 元空间最大大小超过这个值将会抛出OutOfMemoryError异常:java.lang.OutOfMemoryError: Metadata space 六、运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容在类加载后进入方法区的运行时常量池中存放。 Java虚拟机堆Class文件的每一部分格式都有严格要求，要符合要求才能被虚拟机认、装载和执行。但是对于运行时常量池，Java虚拟机规范没有做任何细节要求。另外运行时常量池的一个重要特征就是具有动态性，运行期间可一个将新的常量放入池中 ，这种特性被开发人员利用得比较多的便是String类的intern()方法。 七、直接内存直接内存（Direct Memory）不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是也被频繁使用，也有可能导致OutOfMemoryError异常出现。 在JDK1.4中新加入的NIO类，引入了基于通道与缓冲区的I/O模式，可以使用Native函数库直接分配堆外内存，然后通过存储在Java堆中的DirectByteBuffer对象作为内存的引用进行操作。 这部分内存不受到Java堆大小的限制，但是仍然收到本机内存空间和处理器寻址空间的限制，也有可能出现OutOfMemoryError异常。 对象创建过程类加载虚拟机遇到一条new指令的时候，会首先检查指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否被加载、解析和初始化过。如果没有，就限制性类加载过程。这个部分后续讨论。 分配内存类加载检查完成之后，虚拟机会对新的对象分配内存。对象所需的内存大小在类加载完成之后就可以完全确定，为对象分配空间的任务等同于把一块确定大小的内存从堆内存中划分出来，这里有两种划分方式： 如果内存绝对规整，分配内存就是把指针向后移动与对象大小相等的距离。这种方法叫做指针碰撞（Bump the Pointer） 如果堆中的内存不是规整的，已经使用的和空闲的内存相互交错，分配的时候就需要找到一块足够大的空间划分给对象实例，并维护一个列表记录地址。 堆内存是否规整是由垃圾收集器是否带有压缩整理功能决定的。 除此之外，还需要考虑的是对象创建中线程安全的问题，假如两个线程同时移动内存指针，就有可能出现错误，解决这个问题也有两个方案： 使用CAS搭配失败重试的方式保证更新操作的原子性。 或者把内存的分配动作按照线程划分在不同的空间之中进行，即每个线程在堆中预先分配一小块内存，成为本地内存分配缓冲区（Thread Local Allocation Buffer，TLAB），虚拟机是否启用TLAB可以用参数-XX:+/-TLAB来设定。 初始化信息内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。这一操作保证了对象的实例字段在Java代码中可以不赋初值就直接使用。 然后，虚拟机需要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息。这些信息存放在对象的对象头（Object Header）中，根据虚拟机当前的运行状态不同，比如是否启用偏向锁等，对象头会有不同的设置方式。 从虚拟机的视角，一个新的对象已经产生，但是从Java程序的视角来看，对象创建才刚刚开始，init方法还没执行，所有字段还都为零。所以，执行new指令之后会接着执行init方法，把对象按照程序员意愿进行初始化。 对象的内存布局在HotSpot虚拟机中，对象可以分为三个区域：对象头（Header）、实例数据（Instance Date）和对齐填充（Padding）。 对象头HotSpot虚拟机的对象头包括两部分信息： 对象自身的运行时数据，如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据在32位和64位虚拟机上分别为32bit和64bit，官方称它为“Mark Word”。当对象需要存储的运营时数据很多时，它会根据对象的状态复用自己的存储空间。 类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 如果对象是一个Java数组，那在对象头中还必须有一块记录数组长度的数据，因为虚拟机可以通过普通Java对象的原数据确定Java对象的大小，但是从数组的原数据中却无法确定。 实例数据实例数据是对象真正存储的有效信息，也就是在程序代码中所定义的各种类型的字段信息，无论从父类继承下来的，还是在子类中定义的，都需要记录起来。这部分的存储顺序收到虚拟机分配策略参数和在源码中定义的顺序影响。 对齐填充这部分不是必然存在，没有特殊的含义，仅仅起着占位符的作用。HotSpot VM的自动内存管理系统要求对象其实地址必须是8字节的整数倍，因此当对象实例数据部分没有对齐的时候，就需要通过对齐填充来补全。 对象的访问定位对象建立之后，Java程序需要通过栈上的reference数据来操作堆上的具体内容。由于reference类型在Java虚拟机规范中之规定了一个指向对象的引用，并没有定义这个引用应该如何定位、访问堆中对象的具体位置，所以对象访问方式取决于虚拟机如何实现，当前主流有使用句柄和直接指针两种。 使用句柄访问的话，Java堆中会单独划分一部分区域作为句柄池，reference中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。 如果使用直接指针访问，那么Java对对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference中存储的直接就是对象地址。 这两种方式各有优势，使用句柄的好处是reference中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而reference本身不需要修改。 使用直接指针的最大好处是速度更快，节省了一次指针定位的开销。HotSpot使用的是第二种方式。 垃圾收集器和内存分配策略概述垃圾收集出现的时间远比Java要早，从出现起，垃圾收集就需要考虑三个问题： 哪些内存需要回收 什么时候回收 如何回收 在Java内存运行时区域里，程序计数器、虚拟机栈、本地方法栈随线程而生，随线程而灭；栈中的栈帧随着方法的进入和退出有条不紊的进行出栈和入栈操作。每个栈帧中分配的内存基本是在类结构确定下来的时候就是已知的。Java堆和方法区不一样，只有在运行时才知道创建那些对象，所以这部分内存的分配和回收都是动态的，垃圾收集器所关注的也是这部分内存。 哪些内存需要回收引用计数器法引用计数器法是这样的：给对象添加一个引用计数器，每当有一个地方引用它时，计数器+1，引用失效的时候，计数器-1，任何时刻计数器为0的对象就不可能再被使用。 主流的Java虚拟机中没有选用这个方法来管理内存的，最主要的原因是很难解决对象循环引用的问题。 可达性分析主流的商用程序语言的主流实现中，都是通过可达性分析（Reachability Analysis）来判定对象是否存活的。这个算法的基本思路是通过一系列的成为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称之为引用链（Reference Chain），当一个对象到达GC Roots没有任何引用链相连时，证明此对象不可用。 引用类型无论通过计数器，还是通过可达性分析，判断对象是否存活都与“引用”有关。在JDK1.2之后，Java对引用状态进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。 强引用就是指在代码中普遍存在的，类似于Object obj=new Object()这类引用，只要强引用还存在，垃圾收集器就永远不会回收被引用的对象。 软引用用来描述一些还有用但是不必须的对象。在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之内进行二次回收。如果这次回收之后还是没有足够内存，才会抛出内存溢出异常。 弱引用也是用来描述非必须对象达到，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾回收发生之前。当垃圾收集器工作的时候，无论当前内存是否充足，都会回收掉被弱引用关联的对象。 虚引用也称为幽灵引用或者幻影引用，是最弱的一种引用关系。一个对象是否有虚引用的的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用的唯一目的就是在这个对象被垃圾回收的时候收到一个系统通知。 生存还是死亡即使在可达性分析算法中不可达的对象，也不是必定被回收，他们暂时处于“缓刑”阶段。要真正宣告一个对象死亡，至少要经历两次标记过程：第一次在可达性分析中发现没有引用链，会被标记并且进行筛选，筛选条件条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法或者已经被虚拟机调用过，虚拟机将这两种情况视为“没有必要执行”。 如果有必要执行，对象会被放置在一个叫F-Queue的队列中，稍后由一个虚拟机建立的、低优先级的Finalizer线程去执行。这个“执行”是指虚拟机触发这个方法，但是不承诺等待执行完毕。finalize()方法是对象逃脱死亡的最后一次机会，如果对象要在其中拯救自己，那么与任何一个引用链上的对象建立关联即可。如果建立的关联，比如将自己赋值给了某个类变量，那么第二次标记的时候就会被移除“即将回收”的集合。 垃圾收集算法由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法有各不相同，所以只介绍几种算法的思想和发展过程。 标记-清除算法最基础的算法是“标记-清除”（Mark-Sweep）算法，如名字一样，算法分为“标记”和“清除”两个阶段：首先标记需要回收的对象，然后统一回收。他是最基础的收集算法，但是主要有两点不足： 效率问题，标记和清除效率都不高 空间问题，标记清除之后会有大量的不连续内存碎片，空间碎片太多导致后续分配大对象内存的时候，无法找到足够的连续内存而不得不触发另一次垃圾收集动作。 复制算法复制算法提高了效率，他可以将内存按照容量划分为大小相等的两块，每次使用其中一块。当一块的内存用完之后，把存活的对象移动到另一块，然后把已经使用的空间一次性清理掉。这个方式实现简单运行高效，但是代价是内存缩小为原来的一半。 现在的商业虚拟机都是采用这种收集算法来回收新生代，由于98%的对象都是“朝生夕死”的，所以不需要1:1的比例划分内存空间，而是划分一块较大的Eden和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。回收的时候，将Eden和Survivor中还存活的对象一次性复制到另一块Survivor中，然后清理掉Eden和使用的Survivor空间。HotSpot虚拟机中默认的Eden和Survivor空间比例划分是8:1。但是，98%的对象可回收只是一般场景下的数据，我们不能保证每次回收都是这样，所以当Survivor空间不够使用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 当一块Survivor空间中没有足够的空间存放上一次新生代收集下来的存活对象的时候，这些对象会直接通过分配担保机制进入老年代。 标记-整理算法复制收集算法在对象存活率较高的时候就要进行较多的复制操作，效率将会变低。更关键的是如果不想浪费50%的空间，就需要额外空间进行担保，所以老年代不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”算法，标记过程与“标记-清除”一样，但是后续不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当代商业虚拟机的垃圾收集都是采用分代收集（Generational Collection）算法，根据对象存活周期将不同内存划分为几块。一般是把Java堆分为新生代和老年代，这样可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集都有大量的对象死去，只有少量存活，那就选用复制算法。而老年代存活率高、没有额外空间进行担保，就必须使用“标记-清理”或者“标记-整理”算法来回收。 HotSpot算法实现枚举根节点可以作为GC Roots的节点主要在全局的引用（例如常量或者静态属性）与执行上下文（栈帧中的本地变量表）中，如果逐个检查里面的引用，会耗费很多时间。 另外，可达性分析对执行的敏感还体现在GC的停顿上，这项分析工作必须在能确保一致性的快照中进行，不可以出现分析过程中对象还在不断变化的情况。所以这导致GC进行时需要停顿所有的Java执行线程（Sun将这个事情称为“Stop The Word”） 由于目前主流的Java虚拟机使用的都是准确式GC，所以当执行系统停顿下来之后，不需要一个不漏的检查玩所有执行上下文和全局的引用位置，虚拟机有办法直接得知哪些地方存放着对象引用。在HotSpot的实现中，是通过使用一组称为OopMap的数据接口来达到这个目的。在类加载完成的时候，HotSpot就把对象内偏移量上是什么类型的数据计算出来，在JIT编译过程中，也会在特定的位置记录下帧和寄存器中哪些位置是引用的。 安全点在OopMap的帮助下，HotSpot可以快速的完成GC Root的枚举，但是随着引用变化，不可能每次指令都生成新的OopMap。实际上只有“特定的位置”才会暂停开始GC，这个位置叫做安全点（Safepoint）。Safepoint的选定既不能太少以至于GC等待时间过长，也不能太频繁以至于增大运行时负荷。 对于Safepoint另一个需要考虑的问题是如何在GC发生时让所有线程都跑在最近的安全点上再停下来。这里有两个方案： 抢先式中断（Preemptive Suspension）：不需要线程的执行代码主动配合，GC发生的时候先中断全部线程，如果发现有的线程中断地方不在安全点上，就恢复线程，让他跑到安全点上。现在几乎没有虚拟机实现抢先式中断来暂停线程。 主动式中断（Voluntary Suspension）：当GC需要中断线程的时候，不直接对线程操作，而是仅仅设置一个标志，各个线程主动轮询这个标志，发现标志为真的时候主动中断挂起。轮讯标志的地方和安全点是重合的。 安全区域程序执行的时候，通过Safepoint可以解决如何进入GC的问题，但是程序“不执行”的时候呐？比如线程处于Sleep状态或者Blocked状态，这时候线程无法相应JVM的中断请求，JVM也不太可能等待线程被重新分配CPU时间。对于这种情况，需要安全区域（Safe Region）来解决。 安全区域是指在一段代码中，引用关系不会发生变化，在这个区域中的任何位置进行GC都是安全的。当线程执行到Safe Region中的代码时，会标识自己已经进入了Safe Region，这样发起GC的时候就不用管进入Safe Region状态的线程了。 垃圾收集器如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 常见的垃圾收集器有以下几种： 他们分别适用于不同分代，连线说明可以配合使用。 jdk1.7 默认垃圾收集器Parallel Scavenge（新生代）+Serial Old（老年代） jdk1.8 默认垃圾收集器Parallel Scavenge（新生代）+Serial Old（老年代） jdk1.9 默认垃圾收集器G1 jdk10 默认垃圾收集器G1 Serial收集器Serial收集器是最基本、历史最悠久的收集器。它是一个单线程收集器，不仅仅是说只是用一个CPU进行垃圾收集工作，更重要的是进行垃圾收集的时候，必须暂停 其他所有工作线程直到收集结束。 Serial依然是Client模式下新生代默认的收集器，他也有着优于其他收集器的地方：简单而高效（单线程环境下）。 ParNew收集器ParNew其实就是Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The Wrokd、对象分配规则、回收策略等都与Serial收集器完全一样。 ParNew是许多运行在Server模式下的虚拟机中首选的新生代收集器，有一个重要的原因是除了Serial收集器外，它是唯一能与CMS收集器配合工作。ParNew在单CPU环境中不会比Serial有更好的效果，甚至由于线程切换的开销可能性能更低。 Parallel Scavenge收集器Parallel Scavenge收集器是一个新生代收集器，也是使用复制算法的收集器，又是并行的多线程收集器。 Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目的是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是CPU用于运行用户代码的时间和总的消耗时间的比值。虚拟机总共运行了100分钟，99分钟用来运行代码，1分钟垃圾收集，那么吞吐量就是99%。 Parallel Scavenge收集器提供了两个参数进行进准控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。 MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽可能的保证内存回收花费的时间不超过设定值。但是这个值不是越小越好，GC停顿时间缩短是以牺牲吞吐量和新生代空间换取的，停顿时间降下来很可能吞吐量也下降了。 GCTimeRatio参数的值应当是一个大于0且小于100的整数，也就是垃圾收集时间占总时间的比率，相当于吞吐量的倒数。 Serial Old收集器Serial Old是Serial收集器的老年代版本，他同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义也是给Client模式下的虚拟机使用。 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以最短回收停顿时间为目标的收集器。从名字就可以看出，CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几个收集器来说更复杂一点，整个过程分为4个步骤： 初始标记（initial mark） 并发标记（concurrent mark） 重新标记（remark） 并发清除（concurrent sweep） 其中，初始标记和重新标记仍然需要“Stop The World”。初始标记仅仅标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段是进行GC Roots Tracing的过程，而重新标记则是为了修正并发标记期间用户程序继续运作而导致的标记变动，这个阶段的停顿时间一般会比初始阶段稍长，但是远比并发标记阶段耗时短。 由于整个过程中耗时最长的并发标记和并发清除过程都可以和用户线程一起工作，所以总体来说CMS收集器的内存回收过程是和用户线程一起并发执行的。 但是，CMS同样有以下明显的缺点： CMS收集器对CPU资源非常敏感，默认启动了（CPU数量+3）/4个线程执行回收，也就是当CPU在4个以上并发回收时收集线程占用不少于25%的CPU资源，并且随着CPU数量增加而下降。CPU数量少的时候对用户的影响就很大了。 CMS无法处理浮动垃圾（Floating Garbage），可能出现Concurrent Mode Failure失败而导致另一个Full GC产生。即CMS在并发清除过程中产生的新的垃圾，只能在下一次GC时再处理。 最后一个缺点，由于CMS基于“标记-清除”算法实现，意味着收集结束可能会有大量的空间碎片产生。空间碎片过多会对大内存分配产生麻烦，导致分配内存的时候不得不提前触发Full GC。 G1收集器G1是jdk9版本之后JVM默认的垃圾收集器，它具有以下特点： 并行和并发：G1可以充分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短Stop The World停顿的时间。 分代收集：分代概念仍然在G1中保留，但是G1可以不需要与其他收集器配合独自管理整个GC堆。 空间整合：与CMS的“标记-清理”算法不同，G1从整体上看是基于“标记-整理”算法实现，但是从局部（两个Region）来看是基于“复制”算法实现的。无论如何，这两种算法一位置G1运作期间不会产生内存碎片，收集完成后可以提供规整可用的内存。 可预测的停顿：这是G1相对于CMS的另一大优势，G1除了追求低停顿之外，还能建立可预测的停顿时间模型，让使用者明确指定在一个长度为M毫秒的时间片断内，消耗在垃圾收集上的时间不超过N毫秒。 G1之前的收集器进行收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆内存的布局就与其他收集器有很大差别，他将整个Java堆划分为多个大小相等的独立区域（Region）虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离，都是一部分Region（不需要连续）的集合。 G1收集器之所以能建立可预测的时间模型，因为它有计划地避免在整个Java堆中进行全区域的垃圾收集。G1追踪各个Region里面垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的时间优先回收价值最大的Region。 在G1收集器中，Region之间的对象应用以及其他收集器中的新生代及老年代之间的对象引用，虚拟机都是使用Remembered Set来避免全堆扫描的。G1中每个Region都有一个与之对应的Remembered Set，虚拟机发现程序对Reference类型的数据进行写操作的时候，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之中，如果是，就通过CardTable把相关引用信息记录到被引用的对象所属的Region的Remembered Set中。内存回收的时候，在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会有遗漏。 除了维护Remembered Set的操作，G1收集器的运作大致可以分为以下几个步骤： 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Data Counting and Evacuation） 可以看出这几个步骤的运作过程和CMS有很多相似之处。初始标记阶段只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Star）的值，让下一阶段用户程序并发运行的时候，能在正确可用的Region中创建对象，这个阶段需要停顿线程，但是耗时很短。并发标记阶段是从GC Roots开始进行可达性分析，这个阶段耗时较长但是可以和用户程序并发执行。最终标记阶段则是修正并发标记阶段产生的标记变动，合并到Remembered Set中。这个阶段需要停顿线程，但是可以并行执行。最后筛选阶段首先对各个Region的回收价值和成本排序，根据用户指定的GC停顿时间来制定回收计划。 GC日志一般情况可以通过两种方式来获取GC日志，一种是使用命令动态查看，一种是在容器中设置相关参数打印GC日志。 可以通过jstat命令查看当前正在运行的Java进程的GC状态，命令如下： 1jstat -gc java进程号 毫秒单位的时间间隔 结果含义： S0C：年轻代中第一个survivor（幸存区）的容量 (字节) S1C：年轻代中第二个survivor（幸存区）的容量 (字节) S0U：年轻代中第一个survivor（幸存区）目前已使用空间 (字节) S1U：年轻代中第二个survivor（幸存区）目前已使用空间 (字节) EC：年轻代中Eden（伊甸园）的容量 (字节) EU：年轻代中Eden（伊甸园）目前已使用空间 (字节) OC：Old代的容量 (字节) OU：Old代目前已使用空间 (字节) PC：Perm(持久代)的容量 (字节) PU：Perm(持久代)目前已使用空间 (字节) YGC：从应用程序启动到采样时年轻代中gc次数 YGCT：从应用程序启动到采样时年轻代中gc所用时间(s) FGC：从应用程序启动到采样时old代(全gc)gc次数 FGCT：从应用程序启动到采样时old代(全gc)gc所用时间(s) GCT：从应用程序启动到采样时gc用的总时间(s) NGCMN：年轻代(young)中初始化(最小)的大小 (字节) NGCMX：年轻代(young)的最大容量 (字节) NGC：年轻代(young)中当前的容量 (字节) OGCMN：old代中初始化(最小)的大小 (字节) OGCMX：old代的最大容量 (字节) OGC：old代当前新生成的容量 (字节) PGCMN：perm代中初始化(最小)的大小 (字节) PGCMX：perm代的最大容量 (字节) PGC：perm代当前新生成的容量 (字节) S0：年轻代中第一个survivor（幸存区）已使用的占当前容量百分比 S1：年轻代中第二个survivor（幸存区）已使用的占当前容量百分比 E：年轻代中Eden（伊甸园）已使用的占当前容量百分比 O：old代已使用的占当前容量百分比 P：perm代已使用的占当前容量百分比 S0CMX：年轻代中第一个survivor（幸存区）的最大容量 (字节) S1CMX ：年轻代中第二个survivor（幸存区）的最大容量 (字节) ECMX：年轻代中Eden（伊甸园）的最大容量 (字节) DSS：当前需要survivor（幸存区）的容量 (字节)（Eden区已满） TT： 持有次数限制 MTT ： 最大持有次数限制 GC参数JVM的GC日志的主要参数包括如下几个： -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2017-09-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log 日志文件的输出路径 在生产环境中，根据需要配置相应的参数来监控JVM运行情况 内存分配和回收策略Java的内存管理可以归结为内存的自动分配和自动回收，上面讲了内存回收的策略，下面再说一下内存分配。 对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配，当Eden区没足够空间的时候，虚拟机将发起一次Mino GC。 虚拟机提供了-XX:+PrintGCDetail这个参数，告诉虚拟机在发生垃圾内存收集行为的时候打印内存回首日志，并且在进程退出的时候输出当前内存各个区域的分配情况。 大对象直接进入老年代所谓大的对象是指需要大量连续空间的Java对象，最典型的是很长的字符串和数组。 虚拟机提供了-XX:PretenureSizeThreshold参数，另大于这个设置值的对象直接在老年代分配。目的是避免Eden区及两个Survivor区之间大量的内存复制。 长期存活的对象进入老年代 为了分配哪些对象放在新生代，哪些放在老年代，虚拟机给每个对象定义了一个对象年龄（Age）计数器，如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor中每经过一次Minor GC，年龄就增加一岁，当年龄增加到一定成都（默认是15岁），就会被晋升到老年代，这个阈值可以通过参数-XX:MaxTenuringThreshold设置。 动态对象年龄判定为了适应不同程序的内存状况，不是只有对象年龄大于MaxTenuringThreshold才能晋升老年代。如果Survivor空间中相同年龄所有对象大小总和大于Survivor空间的一般，年龄大于或等于该年龄的对象就可以直接进入老年代。 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有的对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于晋升到老年代对象的平均大小。如果大于就尝试进行一次Minor GC，尽管这次GC是有风险的。如果小于，或者不允许冒险，就改为进行一次Full GC。 虚拟机性能监控和故障处理命令行工具 名称 主要作用 jps JVM Process Status Tool，显示制定系统内所有的HotSpot虚拟机进程 jstat JVM Statistics Monitoring Tool，用于收集HotSpot虚拟机各方面的运行数据 jinfo Configuration Info for Java，显示虚拟机配置信息 jmap Memory Map for Java，生成虚拟机的内存转储快照（heapdump文件） jhat JVM Heap Dump Browser，用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果 jstack Stack Track for Java，显示虚拟机的线程快照 jps：虚拟机进程状况工具jps除了名字像ps命令之外，功能也很类似：可以列出正在运行的虚拟机进程，并且显示虚拟机执行主类（Main Class，Main()函数所在的类）名称以及这些进程的本地虚拟机唯一ID（Local Virtual Machine Identifier，LVMID）。虽然功能简单，但是它是最常用的工具，因为其他JDK工具需要输入它查询出来的LVMID来确定监控的是哪个虚拟机进程。在本地虚拟机进程来说，LVMID和ps命令或者Windows中任务管理器中查到的进程号是一致的。但是如果启动了多个虚拟机进程，就要依赖jps命令根据主类来区分了。 选项 作用 -q 只输出LVMID，省略主类的名称 -m 输出虚拟机级进程启动的时候传给主类main()函数的参数 -l 输出主类的全名，如果进程执行的是Jar包，输出Jar包路径 -v 输出虚拟机进程启动时JVM参数 jstat：虚拟机统计信息监视工具jstat（JVM Statistics Monitoring Tool）适用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或远程虚拟机中的类装载、内存、垃圾收集、JIT编译等运行数据。 jstat命令格式为： 1jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] 对于命令格式中的VMID与LVMID需要说明的是，如果是本地虚拟机进程，VMID与LVMID是一致的，如果远程的虚拟机进程，VMID格式应当是： 1[protocol]:[&#x2F;&#x2F;]lvmid[@hostname[:port]&#x2F;servername] 参数interval和count代表查询间隔和次数，如果省略这两个参数，说明只查询一次。 选项option代表着用户希望查询的虚拟机信息，主要分为3类：类装载、垃圾收集、运行期编译状况。 选项 作用 -class 监视类装载、卸载数量、总空间以及类装载所耗费的时间 -gc 监视Java堆状况，包括Eden区、两个survivor区、老年代、永久带等的容量、已用空间、GC时间合计等信息 -gccapacity 监视内容与-gc基本相同，但是输出主要关注Java堆各个区域使用的最大、最小空间 -gcutil 监视内容与-gc基本相同，但是输出主要关注已使用空间占总空间的百分比 -gccause 与gcutil功能一样，但是会额外输出导致上一次GC产生的原因 -gcnew 监视新生代GC状况 -gcnewcapacity 监视内容与-gcnew基本相同，输出主要关注使用到的最大、最小空间 -gcold 监视老年代GC状况 -gcoldcapacity 监视内容与-gcold基本相同，输出主要关注使用到的最大、最小空间 -gcpermcapaciry 输出到永久带使用到的最大、最小空间。注意，由于永久带在JDK1.8之后已经被元空间替代，所以1.8之后都没有了这个选项 -compiler 输出JIT编译器编译过的方法、耗时等信息 -printcompilation 输出已经被JIT编译的方法 jinfo：Java配置信息工具jinfo（Configuration Info for Java）的作用是实时地查看和调整虚拟机各项参数。 用法如下： 1Usage:2 jinfo [option] &lt;pid&gt;3 (to connect to running process)4 jinfo [option] &lt;executable &lt;core&gt;5 (to connect to a core file)6 jinfo [option] [server_id@]&lt;remote server IP or hostname&gt;7 (to connect to remote debug server)89where &lt;option&gt; is one of:10 -flag &lt;name&gt; to print the value of the named VM flag11 -flag [+|-]&lt;name&gt; to enable or disable the named VM flag12 -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value13 -flags to print VM flags14 -sysprops to print Java system properties15 &lt;no option&gt; to print both of the above16 -h | -help to print this help message 比如使用-flag查看JVM参数 jinfo -flag MaxMetaspaceSize 18348，得到结果-XX:MaxMetaspaceSize=536870912，即MaxMetaspaceSize为512Mjinfo -flag ThreadStackSize 18348，得到结果-XX:ThreadStackSize=256，即Xss为256K jinfo也可以调整JVM参数： 如果是布尔类型的JVM参数: jinfo -flag [+|-] PID，enable or disable the named VM flag 如果是数字/字符串类型的JVM参数 jinfo -flag = PID，to set the named VM flag to the given value 那么怎么知道有哪些JVM参数可以动态修改呐？可以用下面这个命令： 1Linux环境：java -XX:+PrintFlagsInitial | grep manageable2Window环境：java -XX:+PrintFlagsInitial | findstr manageable jmap：Java内存映像工具jmap（Memory Map for Java）命令用于生成堆转储快照（一般称为heapdump或者dump文件）。如果不适用jmap命令，想要获取Java堆转储快照，还有一些比较暴力的手段：比如使用-XX:+HeapDumpOnOutOfMemoryError参数，可以让虚拟机在OOM异常出现之后自动生成dump文件，通过-XX:+HeapDumpOnCtrlBreak参数则可以使用[ctrl]+[Break]键让虚拟机生成dump文件，又或者在Linux系统下通过Kill -3命令发送进程退出信号“吓唬”一下虚拟机，也能拿到dump文件。 除此之外，jmap还可以查询finalize执行队列，Java堆和永久带的详细信息，如空间使用率、当前使用的哪种收集器等。 jmap命令格式： jmap [option] vmid option选项的合法值与具体含义如下： 选项 作用 -dump 生成Java堆转储快照。格式为：-dump:[live, ]format=b，files=&lt;filename&gt;，其中live子参数说明是否只dump出存活的对象 -finalizerinfo 显示在F-Queue中等待FInalizer线程执行finalizer方法的对象。只在Linux/Solaris平台下有效 -heap 显示Java堆详细信息，如使用哪种回收器、参数配置、分代状况等。只在Linux/Solaris平台下有效。 -histo 显示堆中对象统计信息，包括类、实例数量、合计容量 -permstat 以ClassLoader为统计口径显示永久带内存状态。只在Linux/Solaris平台下有效 -F 当虚拟机进程没有对-dump选项响应时，可以使用这个选项强制生成dump快照。只在Linux/Solaris平台下有效 -clstats 打印类加载器的统计信息 -J 传递参数给jmap的JVM jhat：虚拟机堆转储快照分析工具Sun JDK提供jhat（JVM Heap Analysis Tool）命令与jmap搭配使用，来分析jmap生成的堆转储快照。jhatm内置了一个微型的HTTP/HTML服务器，生成dump文件的分析成果后，可以在浏览器中查看。 执行命令jhatp 文件名，屏幕显示Server is ready的提示后，打开浏览器localhost的7000端口就可以看到分析结果。 jstack：Java堆栈跟踪工具jstack（Stack Track for Java）命令用于生成虚拟机当前时刻的线程快照（一般称为threaddump或者javacore文件）。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成快照的主要目的是定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部资源导致的长时间等待等。 jstack命令的格式： jstack [option] vmid 选项 作用 -F 当正常输出的请求不被相应时，强制输出线程堆栈 -l 除堆栈外，显示关于锁的附加信息 -m 如果调用本地方法的话，可以显示C/C++的堆栈 java.lang.Thread类的getAllStackTraces()方法可以获取所有线程的StackTraceElement对象。使用这个方法可以用简单的几行代码完成jstack的大部分功能。 Jconsole：Java监视与管理控制台JConsole（Java Monitoring and Management Console）是一种基于JMX的可视化管理工具。 通过JDK/bin目录下的jsonsole应用就可以启动JConsole，启动后将自动搜索本机运行的所有虚拟机进程。双击选择一个进城之后即可开始监控。 监控页面一目了然，不需要做太多说明。JConsole可以监控包括堆内存、线程、类、CPU等各个使用情况， VisualVM：多合一故障处理工具VisualVM（All-in-One Java Troubleshooting Tool）是目前为止随JDK发布的功能最强大的故障处理工具，并且在可预见的未来都是官方主力发展的虚拟机故障处理工具。 VisualVM可以做到： 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的CPU、GC、堆、方法去以及线程的信息（jps、jstack）。 dump以及分析堆转储快照（jamp、jhat）。 方法记得性能运行性能分析，找出被调用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程dump、内存dump等信息建立一个快照，可以将快照发送给开发者处进行Bug反馈。 其他Plugins Intellij IDEA已经集成了VisualVM，详细的使用方法先留个坑，后续再填。 类文件结构Class文件的结构Class文件是一组以8位字节为基础的二进制流，各个数据项目严格按照顺序紧凑的排列在Class文件之中，中间没有添加任何分隔符，这使得整个Class文件中存储的内容几乎全部是程序运行的必要数据，没有空隙存在。当遇到占用8字节以上空间的数据项时，会按照高位在前的方式分割成若干8位字节进行存储。 根据Java虚拟机规范的规定，Class文件格式 采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构只有两种数据类型：无符号数和表。 类加载机制参考目录： jinfo命令详解","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://beritra.github.com/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://beritra.github.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}]},{"title":"RESTful API","slug":"RESTful API","date":"2018-04-17T13:57:19.000Z","updated":"2019-12-15T14:39:58.908Z","comments":true,"path":"2018/04/17/RESTful API/","link":"","permalink":"http://beritra.github.com/2018/04/17/RESTful%20API/","excerpt":"RESTful API到底怎么玩？记录一下。","text":"RESTful API到底怎么玩？记录一下。 是啥按照是啥、有啥用、怎么用的分析步骤，先解决RESTful是什么的问题。先上高大上的定义，wiki这么说： 表现层状态转换（英语：Representational State Transfer，缩写：REST） ##有啥用 我的理解RESTful就是一种设计风格，通俗点说就是我们设计API的一种指导思想。主要有几个特点： 统一 无状态 可缓存 分层 其他特征没觉得有什么用，暂且不说。个人觉得无状态是RESTful最大的特点，和http协议的设计思路类似。另外RESTful应该着重强调对资源的控制，即每一个URI代表一种资源，客户端通过HTTP动词，对服务端的资源进行操作。 但是回到有啥用这个问题上，个人觉得风格统一，容易理解，方便使用是比较大的优点，其他的倒是没有了。 怎么用RESTful的核心思想是用五个HTTP动词对资源进行操作： GET(获取) 从服务器获取一个资源或者资源列表 POST(创建) 在服务器上创建一个新资源 PUT(更新) 以整体方式更新服务器上的一个资源 PATCH(更新) 更新服务器上的某个资源的部分属性 DELETE(删除) 删除服务器上的资源 另外有两个不常用的动词： HEAD 获取资源元数据 获取信息。这个cors解决跨域的时候有可能用到。 以下是一篇文章中的栗子： GET /zoos:获取所有动物园信息 POST /zoos: 创造一个新的动物园 GET /zoos/ZID: 获取整个动物园对象 PUT /zoos/ZID: 更新整个动物园对象 PATCH /zoos/ZID: 更新动物园对象中的某些属性 DELETE /zoos/ZID: 删除动物园 GET /zoos/ZID/animals: 获取ZID这个动物园下的所有动物 可以看出这里的栗子URL采用了path info的模式，所有查询参数都在URL路径中。但是RESTful风格一般没有对这个做要求，放在后面的参数中也可以。 ##认证怎么搞 常用的接口认证有如下几种方式： Token(JSON Web Token)JWT 是JSON风格轻量级的授权和身份认证规范，可实现无状态、分布式的Web应用授权，JWT主要由以下三部分构成，由.进行连接 Header Payload Signature 所以完整的JWT格式应该是类似xxxxx.yyyyy.zzzzz这样。 Headerheader主要声明token类型和使用的加密算法，比如： 1&#123;2 \"alg\": \"HS256\",3 \"typ\": \"JWT\"4&#125; 然后将头部进行Base64加密，得到第一部分。 Payloadpayload包含你进行认证需要的数据，比如： 1&#123;2 \"sub\": \"1234567890\",3 \"name\": \"John Doe\",4 \"admin\": true5&#125; 然后将有效Payload用Base64进行编码，以形成JWT的第二部分。 Signature要创建签名部分，必须要有已经编码的header，编过码的Payload，一个密匙（secret）和加密算法。 例如，如果想使用HMAC SHA256算法，签名将按以下方式创建： 1HMACSHA256(base64UrlEncode(header) + \".\" +base64UrlEncode(payload),secret) 更详细的文档可以参见官方网站：https://jwt.io/ 跨域既然前后端分离了，就不可避免的要遇到跨域问题。跨域问题单独拎出来总结吧。 //TODO 参考资料： 架构风格与基于网络应用软件的架构设计 RESTful API 设计指南 wiki REST 理解RESTful架构 好RESTful API的设计原则 RESTful API风格基于Token的鉴权机制分析(与JWT结合)","categories":[{"name":"最佳实践","slug":"最佳实践","permalink":"http://beritra.github.com/categories/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"}],"tags":[{"name":"RESTful","slug":"RESTful","permalink":"http://beritra.github.com/tags/RESTful/"},{"name":"最佳实践","slug":"最佳实践","permalink":"http://beritra.github.com/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"}]},{"title":"机器学习（一）线性回归","slug":"机器学习（一）线性回归","date":"2018-04-17T13:57:19.000Z","updated":"2019-12-15T14:09:09.225Z","comments":true,"path":"2018/04/17/机器学习（一）线性回归/","link":"","permalink":"http://beritra.github.com/2018/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","excerpt":"注意到大多数机器学习的课程都是从线性回归开始讲的，而且这部分直觉上的最容易理解，所以我也从线性回归开始学习。","text":"注意到大多数机器学习的课程都是从线性回归开始讲的，而且这部分直觉上的最容易理解，所以我也从线性回归开始学习。 概述面对素不相识的概念，我们先来个素质三连：What、Why、How。 什么是线性回归？线性回归是机器学习中最简单、基础的模型，维基百科线性回归条目中如此介绍： 在统计学中，线性回归（Linear regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。 这种函数是一个或多个称为回归系数的模型参数的线性组合。 只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归 举一个简单的栗子： 有如下一组数据，我们想根据这些数据研究他们之间的规律。 用脚趾头都能看出来，这横轴和纵轴数据是线性相关的，但是你不能用脚趾头证明它，所以需要有一种模型描述他们之间的规律，要尽量保证误差最小，从而能通过已有的数据预测未知的数据。 为什么要用线性回归？这个问题太显而易见反而不容易回答，现实中很多数据之间的关系是线性的，只有一个因变量一个自变量就可以表述它们之间的关系，直觉上也知道用简单线性回归再合适不过了。当然，多个参数自变量也可以是线性回归，不过叫多元线性回归。但是需要注意，线性回归对异常值非常敏感，所以要不要用，怎么用还得仔细考量。 ###怎么回归？ 突然感觉在概述中问这三个问题好蠢，都说清楚的话这篇文章就讲完了…所以下面就详细说怎么回归，先从最简单的一元线性回归，又叫简单线性回归开始。 ##一元线性回归 小学大概就学过，在直角坐标系中描述一条直线的方程是y=kx+b，这里也是一样的，我们的目的是寻找一个合适的方程，能最好的描述x和y的关系，这个方程在大多数课程里叫假设函数 hypothesis function，所以一元线性回归方程就是： 两个Theta怎么选，我们的目的应该是尽量拟合所有的点，所以需要一个指标来描述我们选的值合不合适，我们把它称作损失函数cost function 或者 loss function，这两个名称的含义似乎有些微不同，但是我没搞很清楚，先留个疑问。损失函数使用每个点上的对h(x)预期值与实际值的偏差先平方再取平均来描述，如下图所示。 它这里前面参数多了个二分之一，只是为了后续方便计算。我们的目标就是计算大量的cost function，然后找到其中的最小值。 参考： 机器学习(一) 简单的背景介绍、线性回归、梯度下降","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://beritra.github.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"机器学习","slug":"机器学习","permalink":"http://beritra.github.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Rancher CI/CD Pipeline 初步学习","slug":"Rancher-CI-CD-Pipeline-初步学习","date":"2018-02-06T04:00:50.000Z","updated":"2019-12-15T14:40:04.588Z","comments":true,"path":"2018/02/06/Rancher-CI-CD-Pipeline-初步学习/","link":"","permalink":"http://beritra.github.com/2018/02/06/Rancher-CI-CD-Pipeline-%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/","excerpt":"最近的工作任务是研究Docker、Rancher上CI/CD 流程，记录一些学习过程。","text":"最近的工作任务是研究Docker、Rancher上CI/CD 流程，记录一些学习过程。 什么是CI/CDCI/CD的意思就是持续集成(Continuous integration)和持续\b交付(continuous delivery)或持续部署(continuous deployment)。这三个步骤组成了现在软件开发的基本流程。 初次接触到这些概念基本是懵的，尤其是小公司或者传统软件行业，对软件迭代速度没那么高的要求。\b什么叫集成？怎么算持续？集成的意思就是说快速的把新开发的特性合并到主干上，可以理解为git上把某一个feature分支合并到master上，不过这一过程还包括了程序的构建和测试。持续集成一般是指\b每天进行很多次集成，它的目的主要是快速的更新产品，既能方便调整方向，又能及时发现bug，从而实现了“小步快跑\b”的策略。 而持续交付是指频繁的将产品交到用户或者质量评审团队手里，以供评审，通过之后就可以进入到生产。持续部署是指自动化部署到生产环境。通过实施CI/CD，可以在一天进行很多次的产品迭代，从而提升竞争力。 CI/CD的一般流程也就是软件开发的生命周期，只不过完全将其自动化。从git进行代码提交开始，webhook检测到提交事件之后，开始\b进行单元测试，测试通过之后，将代码合并到主分支，然后进行构建。构建完成在进行集成测试、系统测试，测试无误就可以交付了。然后进行部署，其中可能会有灰度发布，或者用户体验不佳然后进行版本回滚的操作。 基于Docker的CI/CD有什么优势由于自身理解不深，搜集了一些博文的观点： 一个完整的流程入是这样的，用户（也就是开发人员）将包含Dockerfile的源码从本地push到Git服务器上，然后触发Jenkins进行构建源码，源码构建完成后紧接着进行Docker image的构建，一切构建完成之后，顺带将构建成功的image上传到企业内部的镜像仓库，到此刻为止，其实一个基本的CI（持续集成）已经算是结束，剩下的部分就是持续部署或者进行持续的交付开发产物了。在以前传统的软件发布模式中，持续集成的产物是编译打包好的代码，如果想要发布程序，发布系统需要在持续集成的制品库中去获得对应的代码，然后根据一系列的环境检查来准备应用的运行时环境，而在此过程中往往会涉及到比较多的基本组件依赖，所以在整体的发布周期内来看，还是有一些问题的。在Docker或者容器时代，我们将容器的镜像构建部分融入到持续集成（CI）环节，最终持续集成的产出物是一些已经处理好依赖关系，基本不需要人工进行二次干预的Docker image，而在CD环节，发布系统只需要设置和管理很少的信息就能够很快将image运行起来，快速地将业务发布出去。在上面整个环节中，其实无非就是增加了Docker的那一层处理，但其实在整个软件开发的生命周期中，它是产生了极大的影响的。首先，部署系统不需要为统一的部署框架去做更多逻辑抽象，业务研发在开发代码的过程中选择自己依赖的base image即可，最终运行起来的业务也就是你当时提供的base image的模样；其次，由于base image已经处理好了相关的依赖，所以当发布系统拿到业务的image的时候，发布操作将会变得异常迅速，这对于互联网时代可谓是非常重要的；最后一点，也是我感受最深的，就是研发构建好的image可以在任何的Docker环境中run起来，研发人员不需要再关系环境一致性的问题，他们在自己本地的测试环境能够运行起来的应用，那么到生成环境也一定可以。——基于Docker的CI/CD流水线实践 一个想象的Java CI/CD 流程说了一堆理论的东西，回归到实际环境，由于我们公司主要开发语言是Java，我就先想象了一下对于Java语言，应该是个怎么样的CI/CD流程。首先，研发人员开发完成某一阶段之后，提交代码到主分支，然后触发webhook，开启pipeline。首先，应该在一个带有JDK的容器中下载代码，然后用maven下载依赖、编译、打包，然后运行测试，测试通过之后把class文件、jar包和配置文件转移到一个只含有jre的容器，进行镜像的构建步骤，然后推送到镜像仓库，最后将镜像发布，生\b成新的容器实例。 重点关注问题\b关于CI/CD Pipeline，主要需要从以下几个方面着手考虑。 崇伟提到重点关注的几个问题 git分支管理大致怎样？ 构建镜像的版本号如何管理？ 测试环节，如何整合自动化测试？ 推送镜像的目标harbor，是否区分开发环境/测试环境/正式环境？ 发布流程是否可以控制，比如整合到运维的发布流管理系统？ 灰度发布，调研时也需要关注 分支管理Rancher Pipeline的触发是用git某一个分支的webhook。每一个stage可以设置触发条件，包括commit\b ID、分支名、仓库url\b这三种进行区分。 这里需要考虑的主要问题是要如何指导研发人员的的git工作流，怎么设置分支，目标是尽量少的对现有方案进行改动。 这里可以参照两篇文章关于两种CI/CD策略与git分支模型的思考、A successful Git branch model 版本号Pipeline构建过程中可以使用以下变量： NAME DESC CICD_GIT_COMMIT git commit sha CICD_GIT_BRANCH git branch CICD_GIT_URL git repository url CICD_PIPELINE_ID pipeline id CICD_PIPELINE_NAME pipeline name CICD_TRIGGER_TYPE trigger type CICD_NODE_NAME jenkins node name CICD_ACTIVITY_ID pipeline history record id CICD_ACTIVITY_SEQUENCE run number of pipeline history record 都可以作为版本号，其中视频教程中推荐使用CICD_ACTIVITY_SEQUENCE，这基本可以当做一个Pileline的自增序号。 \b这里主要考虑得是在发布流中对版本号的控制。 测试环节整合自动化测试在一个步骤中选择类型为task之后，都可以执行shell，在这里可以调用自动化测试的命令。 \bharbor是否区分环境\b可以区分，不过通过仓库、镜像名、tag都可以\b作为区分，是不是还有必要分开多个环境？ 发布流程发布流程每一步都可以设置权限控制，相关人员确认后再进行下一步。 关于是否可以整合Jenkins插件，视频中说现在不支持，建议不要混搭。 灰度发布灰度发布的关键在于流量控制和版本控制，要求能快速切换版本，控制每个版本的流量。Rancher Pipeline中现有的相关操作是upgrade service可以设置步长，即选择保留的版本数量，同时可以选择先上线新版本再关闭旧版本，或是先关闭再上线，这个操作的时间间隔也可以设置。在Rancher操作界面中也可以随时会滚版本。关于流量控制可以使用HAproxy等负载均衡来做。 现有的工具询问多个同事得知，各个项目组还没有统一的流程，旭哥的eSim团队可能是相对比较健全的，之前使用过GitLab和Jenkins，现在基于方便考虑，切换到了同一家出品的Bitbucket+Bamboo。以ESim为例，当前的发布流程是这样： 提交代码到各自feature分支 每日把各自feature分支合并到develop分支（触发自动化build），同时执行单元测试用例。有冲突解决冲突，直至冲突解决完成 develop拉release分支提测 测试通过在bamboo上将release发布 Rancher Pipeline Demo 环境搭建启动环境Rancher Pipeline的搭建非常简单，首先需要满足Rancher server版本在v1.6.13以上，这里我们选择了v1.6.14。然后在商店里搜索pipeline就可以看到，不需要额外配置，启动服务就可以了。 启动完成之后，Rancher UI界面顶部菜单会出现一个新的按钮”流水线”，点击即可进入\bPipeline界面。 仓库认证和授权进入Pipeline界面之后，点击添加流水线，它会让你先完成仓库授权和添加代码仓库： 当前支持的代码仓库只有两个，GitHub和GitLab，我这里选择了自己部署的GitLab。首先打开Gitlab，登陆，点击右上角自己的头像，在下拉菜单中选择setting： 然后在新的页面里选择application，name里面取个名字，Redirect URI是Pipeline提供给你的，复制过来就行，然后选一个权限，我选择的api，最后保存： 其中，Redirect URI 在这里： \b都完成之后，GitLab会生成一个ID和密钥，将这两个值复制到Pipeline中： 然后别忘了选中”使用私有GitLab部署”，然后填上ip，就可以了，最后点击gitlab验证。 页面会跳到GitLab，点击认证，这一步就完了。 \b流水线配置回到流水线界面，点击添加流水线，会让你选择git用户、仓库和分支，按照各自情况选好需要跟踪的分支，点击添加，第一个阶段就完成了。 然后点击添加第二个阶段，这里可以选择并行或者串行，这里的并行或者串行是指本构建阶段(stage)下的每个步骤(step)执行逻辑。同时可以看到，这里有两个可选选项，分别提供条件筛选和审核。如果不满足条件，步骤不执行。或者执行到这一步的，等待审核人审核，审核通过之前不会执行。 添加完阶段之后添加一个步骤，Rancher教程以go语言为例，选择步骤类型为task，然后镜像为golang:1.8，输入需要执行的shell，这一步的作用就是把代码复制到相应的容器中，然后进行编译，使用容器作为编译环境保证了环境的一致性。这里的那些参数只是为了演示的时候加快变异速度，其实最重要的就是一步go build 进行到这里，无误的话源代码已经编译完成，应该执行单元测试。所以下一个阶段基本相同，再添加一个阶段，添加一个步骤，步骤类型task，命令稍有不同。 单元测试完成，开始构建镜像。添加一个阶段，添加一个步骤，这里步骤类型选择build，这里的build是指build image。\bDockerfile可以在这里填写，或者直接从代码中拉取，这里选择的是从代码中拉，由于只是demo，Dockerfile非常简单: 1FROM alpine2EXPOSE 80803COPY .&#x2F;bin&#x2F;outyet &#x2F;usr&#x2F;bin&#x2F;outyet4ENTRYPOINT &#x2F;usr&#x2F;bin&#x2F;outyet 同时可以看到镜像标签那里使用了上面说到的变量CICD_ACTIVITY_SEQUENCE，可以看做是一个自增序列。这一步顺利完成之后，就构建好了一个名为outyet的镜像，版本号是一个数字，然后被推送到2.8.0.3:8001/pipeline仓库。 进行到这里，自动集成的步骤就完成了。然后应该把镜像推送到Rancher中，产生容器实例，进行自动发布。新建一个阶段，添加一个步骤，这里步骤类型选择为upgradeStack\b。编写docker-compose配置文件，Pipeline会自动从仓库中拉取镜像，然后产生容器实例，更新之前发布的版本。 实际应用下的步骤\b上面基于GoLang语言的Demo是一个最小化的步骤示例，其实对于go语言来讲，编译是很简单的一个步骤，因为没有其他语言复杂的包依赖，构建产物也是直接的可执行文件，最后直接执行即可。但是对于公司广泛应用的\bJava来说，构建步骤需要考虑Jar包依赖的情况，而一般以来的Jar包都是通过maven管理，git上面是不会记录的，所以拉下来的源代码没办法直接进行编译。 \b第一反应是想到了两种方式： \b研发使用统一的基础镜像，在本地进行编译和打包，我们后续只对镜像进行管理。Intellij IDEA提供了插件倒是方便了直接生成镜像。 提供maven环境，统一在服务器上进行编译和打包，研发还是只提供源码就好。 第一种想法很快就被舍弃，因为研发小伙伴未必都对docker了解，水平参差不齐的话很难对镜像进行要求和管理，学习成本略高。\bmaven环境的话只需要把第二个阶段里golang镜像换成一个包含jdk、maven的镜像即可。 所以，结合现有的git flow，建议一个完整的开发流程如下： 研发人员在各自负责的模块新开feature分支，强制要求每日合并到develop分支，强制要求编写单元测试用例。 在完成阶段性开发之后，develop分支合并到release分支或者master分支，触发webhook，开启Pipelin。 \bRancher Pipeline 将代码下载到一个包含JDK和maven的镜像中，进行编译和打包。 \b执行单元测试。 测试无误之后，以一个只含有JRE的基础镜像中构建一个新的应用镜像，推送到Harbor。 测试环境从Harbor拉取镜像，产生容器示例，供QA进行完整的测试。 测试无误后发布到正式环境，这一步应当选择QA人员为审核人，审核通过的话自动发布。 其他\b了解相关信息的过程中，也听到了一些其他的方案，比如Gogs搭配jenkins，或者GitOps和Kubernetes。但是前者集成度不佳，后者依赖于我们尚未使用的K8s，所以暂不做考虑。Rancher分享中提到的Drone似乎也是一个不错的选择，可以直接对接企业微信，推送状态，精力原因还没做详细了解，后续再进行对比。 参考：Rancher Pipeline Referince Guide","categories":[{"name":"容器","slug":"容器","permalink":"http://beritra.github.com/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"容器云","slug":"容器云","permalink":"http://beritra.github.com/tags/%E5%AE%B9%E5%99%A8%E4%BA%91/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://beritra.github.com/tags/CI-CD/"},{"name":"Rancher","slug":"Rancher","permalink":"http://beritra.github.com/tags/Rancher/"},{"name":"Docker","slug":"Docker","permalink":"http://beritra.github.com/tags/Docker/"}]},{"title":"ELK应用栈的搭建和测试","slug":"ELK应用栈的搭建和测试","date":"2018-01-26T10:17:10.000Z","updated":"2019-12-15T14:37:12.660Z","comments":true,"path":"2018/01/26/ELK应用栈的搭建和测试/","link":"","permalink":"http://beritra.github.com/2018/01/26/ELK%E5%BA%94%E7%94%A8%E6%A0%88%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8C%E6%B5%8B%E8%AF%95/","excerpt":"本篇记录搭建、测试ELK应用栈的过程。","text":"本篇记录搭建、测试ELK应用栈的过程。 Logstash 安装和使用安装运行Logstash依赖于Java，所以需要预先安装jre，建议1.8版本及以上。然后在官网可以直接下载二进制压缩文件，解压即可。 创建最简单的示例Logstash关到有两个最基本元素，输入和输出，和一个可选的元素，拦截器。输入插件从一个输入源中消费数据，\b拦截器插件按照你指定的方式修改数据，然后输出插件将数据写入到一个目的地。 想要验证Logstash的安装是否成功，可以运行最基本的Logstash管道： 1cd logstash2bin&#x2F;logstash -e &#39;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&#39; -e标志可以直接通过命令行指定配置，从而让你快速的测试配置而不用多次修改一个配置文件。这个示例中的管道从标准输入stdin中获取输入，然后用一个结构化的格式把数据移动到标准输出stdout。 在启动Logstash完成之后，看到Pipline started提示后，可以在命令行中输入hello world，你会看到： 1hello world22018-01-25T09:26:53.134Z caih-OptiPlex-7050 hello world Logstash在信息中添加了\b时间戳和主机名。 使用Logstash解析日志前例中，我们创建了一个最基本的Logstash管道用来测试Logstash是否启动成功。实际应用中，Logstash管道会复杂很多，它会有多个不同类型的输入，有拦截器，还有输出插件。 直接使用Logstash跟踪日志文件Logstash也可以不借助Filebeat，直接跟踪日志文件。 这里我们写一个简单的shell脚本，名为log.sh，每一秒钟往日志文件里输出一个时间戳，然后跟踪这个日志文件。 1# &#x2F;bin&#x2F;bash2while :3do4 current&#x3D;&#96;date &quot;+%Y-%m-%d %H:%M:%S&quot;&#96;5 echo $current &gt;&gt; log.log6 sleep 1s7done 使用命令nohup bash log.sh &amp;让这个脚本在后台执行。然后tail -f log.log\b就可以看到日志文件在不断增加。 回到logstash的目录下bin文件夹，创建一个简单的配置文件，名为logstash.conf: 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log&quot;]4 &#125;5&#125;6output &#123;7 stdout&#123;&#125;8&#125; 这个配置非常简单，就是在追踪刚才写的日志作为输入，然后在命令行输出。执行命令./logstash -f logstash.conf，稍等之后就可以看到日志中打印的时间戳出现啦，同时每一条日志前面会有logstash添加的时间戳和主机名。这时候遇到了一个问题，就是logstash运行之前的那些历史日志，会有乱序。我怀疑是多个线程同时收集日志导致的顺序问题。这时候修改config文件夹里面的logstash.yml，将pipieline.worker改为1，这个情况有了好转，但是仍然偶尔可见顺序问题，尚有待解决。 可以注意到，每次重启logstash之后，日志都是从上次的结束为止开始输出的，那么如何每次都从日志文件的开头开始输出呐？可以修改配置如下： 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log&quot;]4 start_position &#x3D;&gt; &quot;beginning&quot;5 sincedb_path &#x3D;&gt; &quot;&#x2F;dev&#x2F;null&quot;6 &#125;7&#125;8output &#123;9 stdout&#123;&#125;10&#125; Logstash跟踪日志的时候，标记追踪位置的数据存储在sincedb中，这里sincedb_path就是指定sincedb文件的位置，然后把它配置为linux中自带的空位置，就实现了每次从头开始的目的。注意，start_position只是说每次从sincedb中标记的起点开始，不是从文件头开始，所以只添加这一项是不行的。 然后同样的，可以把文件位置改为系统日志： 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;var&#x2F;log&#x2F;*.log&quot;]4 type &#x3D;&gt; &quot;system&quot;5 start_position &#x3D;&gt; &quot;beginning&quot;6 sincedb_path &#x3D;&gt; &quot;&#x2F;dev&#x2F;null&quot;7 &#125;8&#125;9output &#123;10 stdout&#123;&#125;11&#125; 可以看到系统日志的情况。 借助Filebeat采集日志这部分我们创建一个Logstash管道，使用Filebeat将Apache的日志作为输入，解析这些日志，然后从中创建特定的、命名规范的字段，然后把数据输入到Elasticsearch集群中。这里使用配置文件来定义管道，而不是使用命令行参数。 关于FilebeatLogstash可以Filebeat配合使用来收集日志，那么Filebeat是个什么东西呐？ Filebeat是一个本地文件的日志信息收集器。安装后可以作为你服务端的代理，Filebeat监控日志目录或者指定的日志文件，不断输出文件然后将他们发送到Elasticsearch和Logstash进行索引。 Filebeat的工作模式是这样的：你启动Filebeat之后，它会开启一个或者多个勘探者（prospector）来跟踪本地路径下你指定的日志文件。对于每一个勘探者定位到的文件，Filebeat会启动一个收割者（harvester）。每个收割者读取单个日志文件，发送到libbeat，libbeat会聚合所有时间，然后发送到你配置好的输出中。 Filebeat的安装很简单，在Ubuntu上只需要执行： 1curl -L -O https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;downloads&#x2F;beats&#x2F;filebeat&#x2F;filebeat-6.1.2-amd64.deb2sudo dpkg -i filebeat-6.1.2-amd64.deb 安装完成之后，可以在/etc/filebeat/filebeat.yml文件中查看和更改配置。在Docker环境中，这个文件的位置是在/usr/share/filebeat/filebeat.yml。 这里以一个简单的配置为例： 1filebeat.prospectors:2- type: log3 enabled: true4 paths:5 - &#x2F;var&#x2F;log&#x2F;*.log6 #- c:\\programdata\\elasticsearch\\logs\\* 更详细的文档格式可以参看官方文档，这里只配置一个简单的栗子。 配置Filebeat来发送日志行到Logstash在创建Logstash管道之前，需要配置Filebeat用来发送日志到Logstash。Filebeat客户端是一个轻量级，资源友好型工具，用来从服务端的文件中收集日志，然后发送这些日志到Logstash实例中处理。Filebeat的设计理念是高可用和低延时。Filebeat有一个\b宿主机上轻量的资源占用，而且Beats input插件将Logstash实例上的资源需求做到了最小化。 \b创建Filebeat的配置文件名为filebeat.yml，内容如下： 1filebeat.prospectors:2- type: log3 paths:4 - &#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log5output.logstash:6 hosts: [&quot;127.0.0.1:5044&quot;] 其中的paths就是需要跟踪的日志地址。 然后修改Logstash的配置文件: 1input &#123;2 beats &#123;3 port &#x3D;&gt; &quot;5044&quot;4 &#125;5&#125;6output &#123;7 stdout &#123; codec &#x3D;&gt; rubydebug &#125;8&#125; 然后分别启动两者就可以了，启动Filebeat：filebeat -e -c /home/caih/docker/filebeat/filebeat.yml -d &quot;publish&quot;；启动Logstash：./logstash -f logstash.conf，随后就可以看到日志源源不断地在命令行出现啦！ Elasticsearch 安装和使用安装运行Elasticsearch的安装非常简单，需要依赖只有Java 1.8之后的版本。然后在官网下载文件，解压文件即可。 进入解压后的文件夹，bin目录下有名为Elasticsearch的可执行文件，执行./elasticsearch就可以启动了。 ./elasticsearch --help查看命令帮助，可以使用./elasticsearch -d在后台执行，不在当前命令行输出日志。 配置进入config文件夹可以看到配置文件，其中elasticsearch的配置是在elasticsearch.yml中，另外的jvm.options可以修改Java虚拟机的相关配置，log4j2.properties可以修改el本身的日志配置。 在elasticsearch.yml中，修改netword.host的值为0.0.0.0，然后就可以其他主机访问了。其他常用的配置还包括集群名称，节点名称，日志地址，端口号等等，更多的信息不再赘述。 在浏览器输入ip:9200，可以看到返回: 1&#123;2 &quot;name&quot; : &quot;t_ksdEI&quot;,3 &quot;cluster_name&quot; : &quot;elasticsearch&quot;,4 &quot;cluster_uuid&quot; : &quot;LdO3LVVyRPGwTARorxrNBA&quot;,5 &quot;version&quot; : &#123;6 &quot;number&quot; : &quot;5.5.3&quot;,7 &quot;build_hash&quot; : &quot;9305a5e&quot;,8 &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;,9 &quot;build_snapshot&quot; : false,10 &quot;lucene_version&quot; : &quot;6.6.0&quot;11 &#125;,12 &quot;tagline&quot; : &quot;You Know, for Search&quot;13&#125; 这样表示Elasticsearch启动成功。 添加一条文档在命令行输入： 1curl -XPUT &#39;localhost:9200&#x2F;customer&#x2F;doc&#x2F;1?pretty&amp;pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39;2&gt; &#123;3&gt; &quot;name&quot;: &quot;John Doe&quot;4&gt; &#125;5&gt; &#39; 然后就添加了一条最简单的文档，稍后可以在\bKibana中看到。 Kibana 安装和使用\bKibana的安装同样简单，同样依赖于Java，官网下载tar包解压，然后进入bin目录，使用命令./kibana --help可以查看启动参数。 输入./kibana serve即可启动，这里需要注意与Elasticsearch版本是否匹配。我这里用的都是最新版6.1.2。\bKiana默认寻找得是本地9200端口的Elasticsearch，如果要指定别的地方的实例，可以在/config/kiban.yml中修改。还有一点，\bKibana默认host是本地，所以要修改为0.0.0.0然后才能在其他机器上访问。 输入ip:5601就可以看到Kibana的ui界面了，刚开始会让你输入一个index pattern，可以先输入*匹配所有索引，然后再Discover中就可以看到Elasticsearch中的数据了。 从日志文件到Kibana日志到Logstash已经联通，Elasticsearch和\bKibana也连上了，现在只需要把Logstash的output设置为Elasticsearch。 修改logstash.conf如下： 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log&quot;]4 &#125;5&#125;6output &#123;7 elasticsearch &#123; hosts &#x3D;&gt; [&quot;localhost:9200&quot;] &#125;8&#125; 然后Logstash默认的索引是以logstash-开头，所以把kibana中的index pattern修改为logstash-*就可以过滤结果了。这时候在discover部分就可以看到每秒钟一条的时间戳。 Kibana默认的配置中，这里的信息是不自动刷新的，所以想要修改刷新频率，在Management -&gt; advance中，把timepicker:refreshIntervalDefaults这一项修改为{ &quot;display&quot;: &quot;5 seconds&quot;, &quot;pause&quot;: false, &quot;value&quot;: 5000 }，然后就可以看到日志五秒刷新一次，不断更新。 以上就是ELK栈收集、处理、展示日志最简单的demo。","categories":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"容器云","slug":"容器云","permalink":"http://beritra.github.com/tags/%E5%AE%B9%E5%99%A8%E4%BA%91/"},{"name":"日志","slug":"日志","permalink":"http://beritra.github.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"ELK","slug":"ELK","permalink":"http://beritra.github.com/tags/ELK/"},{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"dockerfile指南及最佳实践","slug":"dockerfile指南及最佳实践","date":"2018-01-11T01:45:34.000Z","updated":"2019-12-15T14:34:22.075Z","comments":true,"path":"2018/01/11/dockerfile指南及最佳实践/","link":"","permalink":"http://beritra.github.com/2018/01/11/dockerfile%E6%8C%87%E5%8D%97%E5%8F%8A%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"学习Docker的时候遇到了不少问题，也有同事询问的时候不能回答上来，所以系统的记录一些Docker原理方面的学习过程。先从dockerfile入手，本篇主要是官网文档的翻译。","text":"学习Docker的时候遇到了不少问题，也有同事询问的时候不能回答上来，所以系统的记录一些Docker原理方面的学习过程。先从dockerfile入手，本篇主要是官网文档的翻译。 Docker的内核基础提到Docker，基本都知道其本质是宿主机上面的一个进程，通过namespace实现了资源隔离。通过cgroups实现了资源限制，通过写时复制（copy-on-write）实现了高效的文件操作。从Linux内核3.8版本开始，提供了namespace功能，主要分为以下六项隔离： UTS：主机名与域名 IPC：信号量、消息队列和共享内存 PID：进程编号 Network：网络设备、网络栈、端口等 Mount：挂载点（文件系统） User：用户和用户组 Docker自底向上的结构构建一个Docker应用可以分为以下三层： Stack Service Container 使用Dockerfile定义一个容器Dockerfile定义了你的容器内的环境发生了什么。在这个环境里，资源的获取比如网络接口或者磁盘驱动都是虚拟化的，并且与你系统的其他部分是隔离开的，所以你必须将端口映射到外面，而且你必须制定那些文件需要“复制”到这个环境以内。 Docker可以通过Dockerfile的命令构建一个镜像，使用Docker build命令可以创建一个连续的命令行指令进行自动构建。 Dockerfile指南用法docker build命令从Dockerfile和上下文中构建镜像。构建的上下文是在特定位置的文件的集合，比如PATH和URL，PATH是你本地文件系统的目录，URL是git仓库地址。 上下文是递归处理的，所以PATH包括了子目录，URL也包括了仓库和它的子模块。比如把整个当前路径都作为上下文： 1$ docker build .2Sending build context to Docker daemon 6.51 MB3··· 为了在构建上下文的时候使用一个文件，Dockerfile使用一个命令去指定某个文件，比如COPY命令。为了增加构建过程的性能，可以通过添加.dockerignore来排除某些文件。 通常情况下，Dockerfile就叫Dockerfile，你可以使用-f命令指定使用某个Dockerfile,用法如下： 1docker build -f &#x2F;path&#x2F;to&#x2F;a&#x2F;Dockerfile . 如果构建成功，你也可以指定一个仓库和标签来说明在哪里存储你的镜像： 1docker build -t shykes&#x2F;myapp . 为了在构建完成之后，给镜像标注多个仓库，可以添加多个-t: 1docker build -t shykes&#x2F;myapp:1.0.2 -t shykes&#x2F;myapp:latest . 在Docker守护进程执行Dockerfile中的命令之前，它会执行一个初步的验证，如果语法不正确会返回错误： 1$ docker build -t test&#x2F;myapp .2Sending build context to Docker daemon 2.048 kB3Error response from daemon: Unknown instruction: RUNCMD Docker守护进程会依次逐条执行Dockerfile中的命令，在返回最终镜像的ID之前，如果需要，每一条命令的结果都会提交成为一个新的镜像。Docker守护进程会自动清理你发送的上下文。 需要注意，每一条命令都是独立运行的，而且会导致新的镜像被创建，所以RUN cd /tmp不会在对下一条命令产生任何影响。 如果可能，Docker会重新利用中间镜像（缓存），从而让docker build过程显著加快。你可以从控制台输出中看到Using cache的提示。（想要了解更多信息，参照Dockerfile最佳实践中的构建缓存部分） 1$ docker build -t svendowideit&#x2F;ambassador .2Sending build context to Docker daemon 15.36 kB3Step 1&#x2F;4 : FROM alpine:3.24 ---&gt; 31f630c650715Step 2&#x2F;4 : MAINTAINER SvenDowideit@home.org.au6 ---&gt; Using cache7 ---&gt; 2a1c91448f5f8Step 3&#x2F;4 : RUN apk update &amp;&amp; apk add socat &amp;&amp; rm -r &#x2F;var&#x2F;cache&#x2F;9 ---&gt; Using cache10 ---&gt; 21ed6e7fbb7311Step 4&#x2F;4 : CMD env | grep _TCP&#x3D; | (sed &#39;s&#x2F;.*_PORT_\\([0-9]*\\)_TCP&#x3D;tcp:\\&#x2F;\\&#x2F;\\(.*\\):\\(.*\\)&#x2F;socat -t 100000000 TCP4-LISTEN:\\1,fork,reuseaddr TCP4:\\2:\\3 \\&amp;&#x2F;&#39; &amp;&amp; echo wait) | sh12 ---&gt; Using cache13 ---&gt; 7ea8aef582cc14Successfully built 7ea8aef582cc 构建缓存只有在那些拥有本地父母链（local parent chain）的镜像中被使用。这意味着那些镜像是被前一阶段的构建产物所创造，或者被docker load装载的整个镜像链条所创造。如果你想指定某一个镜像使用构建缓存，你可以用--cache-from选项来指定。 完成你的构建过程之后，你就可以准备浏览将存储仓库推入注册 格式Dockerfile的格式是这样的： 1# Comment2INSTRUCTION arguments 指令是大小写不敏感的，但是惯例是把指令都写成大写，方便与参数区分开。 Docker会按照顺序执行Dockerfile中的指令，一个Dockerfile必须从FROM命令开始，FROM命令指定了一个你要构建的基础镜像。 Docker会把以#为起始的一行视为注释，除非这一行是一个有效的解析指令(parse directives)。一个在其他位置出现的#会被当做是参数的一部分，所以允许这样的语句： 1# Comment2RUN echo &#39;we are running some # of cool things&#39; 解析指令 Parse directives解析指令是可选的，而且会影响Dockerfile中后续指令行的处理方式。解析指令不会增加新的构建层数，也不会被认为是一个构建步骤。它的写法是类似于特殊的注释：# directive=value一个解析指令只被使用一次。 每当一个注释、空行或者构建指令被处理之后，Docker不会再去寻找解析指令。取而代之的是它会把解析指令格式的命令行视为一个注释，而且不会尝试去验证这是否是一个解析指令。因此，所有的解析指令都应该在Dockerfile的最上方。 解析指令大小写不敏感，按照惯例我们写成小写格式，而且后面添加一个空行。解析指令不支持行延长符号，所以如下是无效的： 1# direc \\2tive&#x3D;value 重复出现两次也是无效的: 1# directive&#x3D;value12# directive&#x3D;value234FROM ImageName 出现在构建指令之后的解析指令会被当做普通注释： 1FROM ImageName2# directive&#x3D;value 出现在普通注释之后的解析指令也会被当做普通注释： 1# About my dockerfile2# directive&#x3D;value3FROM ImageName 非法的解析指令会被当做普通注释，此外，紧随其后的合法解析指令也会被当做注释，因为出现在一条注释之后。 1# unknowndirective&#x3D;value2# knowndirective&#x3D;value 不换行空格允许出现在解析指令之中。因此，下列行被认为已知： 1#directive&#x3D;value2# directive &#x3D;value3# directive&#x3D; value4# directive &#x3D; value5# dIrEcTiVe&#x3D;value 支持以下解析指令：escape escape1# escape&#x3D;\\ (backslash) 或 1# escape&#x3D;&#96; (backtick) escape指令指定Dockerfile中的转义字符，如果没有指定，默认的转义字符是\\. 转义字符不仅作用在行中的转义字符，也作用在换行符。这允许一个Dockerfile指令跨越多行。需要注意，无论escape解释语句是否出现在Dockerfile中，转义不会在RUN命令中生效，除非在行尾。 将转义字符设置为`在Windows中尤其有用，因为\\是地址分隔符，`与Windows PowerShell相一致。 考虑到下面的例子会在Windows中以一个不明显的方式失败。在第二行结束位置的第二个\\会被当做换行符，而不是第一个\\转义的目标。同样，在第三行末尾位置的\\，假设它实际上是作用为一个指令，他被当做一个行延长符。这个Dockerfile的结果是第二行和第三行被当做一个单独的指令。 1FROM microsoft&#x2F;nanoserver2COPY testfile.txt c:\\\\3RUN dir c:\\ 结果： 1PS C:\\John&gt; docker build -t cmd .2Sending build context to Docker daemon 3.072 kB3Step 1&#x2F;2 : FROM microsoft&#x2F;nanoserver4 ---&gt; 22738ff49c6d5Step 2&#x2F;2 : COPY testfile.txt c:\\RUN dir c:6GetFileAttributesEx c:RUN: The system cannot find the file specified.7PS C:\\John&gt; 一个解决方案是使用/作为COPY指令和dir指令的目标。然而最好的情况下，这个语法在Windows上也并不自然，令人困惑，在比较坏的情况下，在Windows上使用/作为地址分隔符有可能出现错误。 通过添加escape解析指令，下面的Dockerfile成功的使用原生的系统地址分割语法如期执行： 1# escape&#x3D;&#96;23FROM microsoft&#x2F;nanoserver4COPY testfile.txt c:\\5RUN dir c:\\ 结果如下： 1PS C:\\John&gt; docker build -t succeeds --no-cache&#x3D;true .2Sending build context to Docker daemon 3.072 kB3Step 1&#x2F;3 : FROM microsoft&#x2F;nanoserver4 ---&gt; 22738ff49c6d5Step 2&#x2F;3 : COPY testfile.txt c:\\6 ---&gt; 96655de338de7Removing intermediate container 4db9acbb16828Step 3&#x2F;3 : RUN dir c:\\9 ---&gt; Running in a2c157f842f510 Volume in drive C has no label.11 Volume Serial Number is 7E6D-E0F71213 Directory of c:\\141510&#x2F;05&#x2F;2016 05:04 PM 1,894 License.txt1610&#x2F;05&#x2F;2016 02:22 PM &lt;DIR&gt; Program Files1710&#x2F;05&#x2F;2016 02:14 PM &lt;DIR&gt; Program Files (x86)1810&#x2F;28&#x2F;2016 11:18 AM 62 testfile.txt1910&#x2F;28&#x2F;2016 11:20 AM &lt;DIR&gt; Users2010&#x2F;28&#x2F;2016 11:20 AM &lt;DIR&gt; Windows21 2 File(s) 1,956 bytes22 4 Dir(s) 21,259,096,064 bytes free23 ---&gt; 01c7f3bef04f24Removing intermediate container a2c157f842f525Successfully built 01c7f3bef04f26PS C:\\John&gt; 环境替换环境变量（通过ENV命令声明的）也可以在用在某一指令中，像一个变量一样被Dockerfile解释。转义也被用作将类变量语法逐字逐句的包含到声明中。（Environment variables (declared with the ENV statement) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally.） 环境变量在Dockerfile中会表示为$variable_name或者${variable_name}。这两种表述方式等价，其中大括号方式通常被用作表述没有空格的变量名字，像${foo}_bar。 ${variable_name}这种语法同样支持几种标准的bash编辑方式如下： ${variable:-word}表示如果变量是一个集合，那么结果就是集合的值，否则结果是word。 ${variable:+word}表示如果变量是一个集合，那么结果是word，否则是空字符串。 在所有情况下，word可以使任何字符串，包括额外的环境变量。 可以通过在变量前添加\\进行转义：\\$foo或者\\${foo}。比如下面的例子，将会对$foo和${foo}逐字严格各自转换。 1FROM busybox2ENV foo &#x2F;bar3WORKDIR $&#123;foo&#125; # WORKDIR &#x2F;bar4ADD . $foo # ADD . &#x2F;bar5COPY \\$foo &#x2F;quux # COPY $foo &#x2F;quux 环境变量在所有下列指令中被支持： ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUMN WORKDIR 同样的： ONBUILD（与其他上述指令一同使用的时候） 在整个指令中，环境变量替代物会为每一个变量使用同一个值。如下所示： 1ENV abc&#x3D;hello2ENV abc&#x3D;bye def&#x3D;$abc3ENV ghi&#x3D;$abc 结果是def的值为hello，而不是bye。然而，ghi的值是bye因为这不是将abc设置为bye的那条语句。 .dockerignore file在docker命令行把上下文发送给docker守护进程之前，它会在上下文路径的根目录下搜索文件名为.dockerignore的文件。如果文件存在，命令行就会把符合的文件从上下文中排除出去。 .dockerignore文件的示例如下： 1# comment2*&#x2F;temp*3*&#x2F;*&#x2F;temp*4temp? 这个文件会产生如下构建行为：命令|行为—|—#comment|忽略*/temp*|排除根路径下所有直接子目录内文件名或者目录名以temp为开头的文件或文件夹。比如：/somedir/temporary.txt就被排除了,或者路径/somedir/temp。*/*/temp*|排除所有根路径下二级子目录中以temp为起始文件名的文件或文件夹。比如：/somedir/subdir/temporary.txt。temp?|排除根目录下文件名或目录名是temp后面加一个字符。比如：/tempa和tempb。 具体用法和.gitignore类似，不再赘述。 FROM1FROM &lt;image&gt; [AS &lt;name&gt;] 或者 1FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;] 或者 1FROM &lt;image&gt;[@&lt;digest&gt;] [AS &lt;name&gt;] FROM指令初始化了一个新的构建阶段，而且为后续的指令准备了基础镜像。所以，一个有效的Dockerfile应该以FROM指令起始。镜像可以是任何有效的镜像，尤其简单的是可以从公共仓库下载镜像作为开始。 ARG是唯一可以出现在FROM之前的指令。 在一个Dockerfile中，FROM可以出现多次，创建多个镜像，或者把一个构建阶段作为另一个的依赖。只需要注意，在一条新的FROM指令提交之前把上一个的镜像ID记录下来。每一个FROM指令会清除之前指令创造的所有状态。 可选的，使用AS name语句可以赋予新的构建阶段可以一个名称。这个名称还可以在随后的FROM和COPY --from=&lt;name|index&gt;中使用以指定某个镜像。 tag或者digest选项是可选的。如果你都省略了，系统会缺省设置latest标签。如果找不到任何tag的值，构建器会返回一个错误。 理解ARG和FROM是如何相互需作用的FROM指令支持那些由ARG声明的，出现在自身之前的变量。 1ARG CODE_VERSION&#x3D;latest2FROM base:$&#123;CODE_VERSION&#125;3CMD &#x2F;code&#x2F;run-app45FROM extras:$&#123;CODE_VERSION&#125;6CMD &#x2F;code&#x2F;run-extras 一个FROM指令之前的ARG声明是独立于构建阶段之外的，所以不能在FROM之后的任何指令中使用。想要使用FROM指令之前的ARG指令的默认属性，你需要在构建阶段之内再使用一次不带赋值的ARG指令： 1ARG VERSION&#x3D;latest2FROM busybox:$VERSION3ARG VERSION4RUN echo $VERSION &gt; image_version RUNRUN命令有两种格式; RUN &lt;command&gt;（shell格式，命令在shell中执行，默认是Linux中的/bin/sh -c后者Windows中的cmd /s /c） RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]（执行格式） RUN指令会在当前镜像之上的新分层中执行任何命令，然后提交结果。产生的被提交镜像会在Dockerfile中的后续步骤中使用。 分层的RUN指令和不断产生的提交符合Docker的核心思想：提交应当是简易的，容器可以从镜像的历史中的任何一个时间点创建，这点很像代码控制。 执行格式避免了shell字符歧义，而且你可以不指定使用特定的shell可执行文件（bash or sh or ?）的情况下使用RUN指令。 你可以在shell格式中使用SEHLL命令指定使用特定的shell。 在shell格式中你可以使用\\（反斜杠）来在多行中延续一条RUN指令，比如： 1RUN &#x2F;bin&#x2F;bash -c &#39;source $HOME&#x2F;.bashrc; \\2echo $HOME&#39; 等价于： 1RUN &#x2F;bin&#x2F;bash -c &#39;source $HOME&#x2F;.bashrc; echo $HOME&#39; 注意： 要使用不同的shell，而不是’/bin/sh’，请使用在所需shell中传递的exec形式。例如：RUN [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;echo hello&quot;] 注意： exec形式作为JSON数组解析，这意味着您必须在单词之外使用双引号（”）而不是单引号（’）。 注意： 与shell格式不同，exec格式不调用命令shell。这意味着正常的shell处理不会发生。例如，RUN [“echo”,”$HOME”]不会在$HOME上进行可变替换。如果你想要shell处理，那么使用shell形式或直接执行一个shell，例如：RUN [“sh”,”-c”,”echo $HOME”]。当使用exec形式并直接执行shell时，正如shell形式的情况，它是做环境变量扩展的shell，而不是docker。 注意： 在JSON形式中，有必要转义反斜杠。这在Windows上特别相关，其中反斜杠是路径分隔符。因为不是有效的JSON，并且以意外的方式失败，以下行将被视为shell形式：RUN [&quot;c:\\windows\\system32\\tasklist.exe&quot;]此示例的正确语法为：RUN [&quot;c:\\\\windows\\\\system32\\\\tasklist.exe&quot;] RUN指令的缓存不会在下一次构建期间自动失效。一条指令的缓存类似RUN apt-get dist-upgrade -y会在下一个次构建的时候重用。RUN指令的缓存可以使用--no-cache标志取消，比如docker build --no-cache。 在Dockerfile最佳实践中看到更多的信息。 ADD指令也会使RUN指令的缓存失效，详情见下。 CMDCMD指令有三种格式： CMD [“executable”,”param1”,”param2”]（执行格式，这也是首选的格式） CMD [“param1”,”param2”]（作为ENTRYPOINT的默认参数） CMD command param1 param2（shell格式） Dockerfile中只能有一条CMD命令，如果有多条，那么只有最后一条生效。 CMD指令的主要用意是提供一个执行容器的默认值。这些默认值可以包括一个可执行文件，或者他们可以省略可执行文件。在这种情况下你必须指定一个ENTRTPOINT指令。 注意： 如果使用CMD为ENTRYPOINT指令提供默认参数，CMD和ENTRYPOINT指令都应以JSON数组格式指定。 注意： exec形式作为JSON数组解析，这意味着您必须在单词之外使用双引号（”）而不是单引号（’）。 注意： 与shell表单不同，exec表单不调用命令shell。这意味着正常的shell处理不会发生。例如，CMD [&quot;echo&quot;，&quot;$HOME&quot;]不会在$HOME上进行可变替换。如果你想要shell处理，那么使用shell形式或直接执行一个shell，例如：CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;echo $HOME&quot;]。当使用exec形式并直接执行shell时，正如shell形式的情况，它是做环境变量扩展的shell，而不是docker。 当你使用shell或者exec格式的时候，CMD指令会在容器启动的时候执行你输入的命令。 如果你使用CMD的shell格式，那么命令会以/bin/sh -c的形式执行： 1FROM ubuntu2CMD echo &quot;This is a test.&quot; | wc - 如果你想执行你的命令但是不指定某个shell，你必须用JSON形式表述你的命令，而且给出一个可执行文件的完整地址。这种数组形式是CMD的首选格式。任何附加的参数必须单独表述为数组中的字符串： 1FROM ubuntu2CMD [&quot;&#x2F;usr&#x2F;bin&#x2F;wc&quot;,&quot;--help&quot;] 如果你想要你的容器每次都运行相同的可执行文件，那你应该考虑将ENTRYPOINT指令和CMD一起使用。 如果用户指定了docker run的参数，那么这些参数就会覆盖掉CMD中指定的命令。 注意： 不用疑惑RUN和CMD的区别。RUN其实是运行了一条指令然后提交结果；CMD不在构建阶段执行任何指令，但是为镜像准备好指令（意思是说在docker run的时候执行）。 LABEL1LABEL &lt;key&gt;&#x3D;&lt;value&gt; &lt;key&gt;&#x3D;&lt;value&gt; &lt;key&gt;&#x3D;&lt;value&gt; ... LABEL指令添加镜像的元数据。一个LABEL就是一组键值对。想要在LABEL的值中添加空格，要像命令行中的转义那样使用引号和反斜杠，例子如下： 1LABEL &quot;com.example.vendor&quot;&#x3D;&quot;ACME Incorporated&quot;2LABEL com.example.label-with-value&#x3D;&quot;foo&quot;3LABEL version&#x3D;&quot;1.0&quot;4LABEL description&#x3D;&quot;This text illustrates \\5that label-values can span multiple lines.&quot; 一个镜像可以有多个标签，你可以在一行内指定多个标签。在Docker 1.10之前，这么做会减小最终镜像的大小，但是现在不再如此。你仍然可以选择在一条指令内指定多个标签，如下两个例子所示： 1LABEL multi.label1&#x3D;&quot;value1&quot; multi.label2&#x3D;&quot;value2&quot; other&#x3D;&quot;value3&quot; 1LABEL multi.label1&#x3D;&quot;value1&quot; \\2 multi.label2&#x3D;&quot;value2&quot; \\3 other&#x3D;&quot;value3&quot; 基础镜像或者双亲镜像（FROM中指定的镜像）中包含的标签会被你的镜像继承。如果一个标签已经存在但是有不同的值，会以最近设置的值为准，之前的会被覆盖。想要查看一个镜像的所有标签，可以使用docker inspect命令。 1&quot;Labels&quot;: &#123;2 &quot;com.example.vendor&quot;: &quot;ACME Incorporated&quot;3 &quot;com.example.label-with-value&quot;: &quot;foo&quot;,4 &quot;version&quot;: &quot;1.0&quot;,5 &quot;description&quot;: &quot;This text illustrates that label-values can span multiple lines.&quot;,6 &quot;multi.label1&quot;: &quot;value1&quot;,7 &quot;multi.label2&quot;: &quot;value2&quot;,8 &quot;other&quot;: &quot;value3&quot;9&#125;, MAINTAINER(弃用)标注作者名，已经弃用，推荐使用LABEL代替。 EXPOSE1EXPOSE &lt;port&gt; [&lt;port&gt;&#x2F;&lt;protocol&gt;...] EXPOSE指令提示Docker，容器会在运行时监听特定的网络端口。你可以指定端口是监听TCP还是UDP，而且如果没有指定，那么默认为TCP。 EXPOSE指令实际上不发布端口。它的作用是作为一个镜像构建者和容器使用者之间的文档，告知哪一个端口应当被发布。实际上想要在运行容器的时候发布端口，应当在docker run命令中使用-p参数以发布或者映射一个或者多个端口，或者用-P参数发布所有暴露的端口并且映射它们到高优先级（high-order）的端口上。 想要建立宿主机的端口重定向，你需要看文档中的using -P flag。docker netword命令支持在免于暴露特定端口的情况下建立容器之间的网络通信，因为容器可以通过任意的端口连接到网络。 ENV1ENV &lt;key&gt; &lt;value&gt;2ENV &lt;key&gt;&#x3D;&lt;value&gt; ... ENV指令把环境变量的key设为value。这个值会在所有Dockerfile的后续命令中存在，而且支持上面提到的环境变量替换。 ENV指令有两种格式，第一种格式，ENV &lt;key&gt; &lt;value&gt;，会把一个单一的键设为某一值，在空格后面出现的整个字符串会被认为是值，包括空格和引号等字符。 第二种格式，ENV &lt;key&gt;=&lt;value&gt; ...，允许一次性设置多个值。需要注意第二种方式的语法中使用等号，而第一种没有使用。就像命令行转义，引号和反斜杠可以用来包含值中的空格。如下： 1ENV myName&#x3D;&quot;John Doe&quot; myDog&#x3D;Rex\\ The\\ Dog \\2 myCat&#x3D;fluffy 或 1ENV myName John Doe2ENV myDog Rex The Dog3ENV myCat fluffy 两者作用相同，但是推荐使用第一种，因为只会产生一层缓存层。 ENV命令设置的环境变量会一直存在，包括容器从最终镜像运行后。你可以使用docker inspect查看它们的状态，也可以使用docker run --env &lt;key&gt;=&lt;value&gt;改变它们的状态。 注意： 环境变量的持续存留有可能产生不可预期的副作用。比如设置ENV DEBIAN_FRONTEND noninteractive有可能让基于Debian镜像的apt-get用户产生困惑。想要为单条命令设置一个值，使用RUN &lt;key&gt;=&lt;value&gt; &lt;command&gt;。 ADDADD有两种格式： ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt; ADD [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;](包含空格的路径需要使用这种格式) 注意： --chown特性只支持构建Linux容器的Dockerfile，而且不会在Windows容器中生效。由于用户和用户组概念不能再Linux和Windows之间转化，所以使用/etc/passwd和/etc/group把用户和用户组转化为ID这种特性只在基于Linux的容器下可行。 ADD指令从&lt;src&gt;复制新的文件、路径或者远程文件URL到镜像的文件系统中的&lt;dest&gt;位置。 可以指定多个&lt;src&gt;资源，但是如果他们是文件或者目录的话，他们的路径应该是相对于构建上下文的。 每一个&lt;src&gt;可以包括通配符，适配规则将按照Go语言的filepath.Match规则。比如： 1ADD hom* &#x2F;mydir&#x2F; # 添加所有文件名以 &quot;hom&quot; 开头的文件2ADD hom?.txt &#x2F;mydir&#x2F; # ? 可以替代一个单个字符，比如， &quot;home.txt&quot; &lt;dest&gt;是一个绝对路径，或者相对于WORKDIR的路径，在其中，资源将被复制到目标容器。 1ADD test relativeDir&#x2F; # adds &quot;test&quot; to &#96;WORKDIR&#96;&#x2F;relativeDir&#x2F;2ADD test &#x2F;absoluteDir&#x2F; # adds &quot;test&quot; to &#x2F;absoluteDir&#x2F; 当添加一个包含特殊符号(比如[和])的文件或者目录，你需要对这些字符进行符合Golang规则的转义，防止它们被视为一个统配规则。比如，想要添加一个文件名为arr[0].txt的文件，你需要： 1ADD arr[[]0].txt &#x2F;mydir&#x2F; # 复制一个名为 &quot;arr[0].txt&quot; 的文件到 &#x2F;mydir&#x2F; 一个新的文件或者目录被创造的时候，它的UID和GID将为0，除非用选项--chown指定了用户和用户组或者UID/GID组合请求指定的内容添加所有权。--chown选线的格式允许用户名或者用户组名以字符串或者整形形式的UID或GID组合。提供一个不带用户组的用户名或者不带GID的UID的话，会使用同样的UID作为GID。如果提供了一个用户名或者用户组名，容器的根目录文件系统/etc/passwd和/etc/group文件会被使用，来执行从名称到UID或GID的转换。下列栗子展示了合法的--chown选项的定义： 1ADD --chown&#x3D;55:mygroup files* &#x2F;somedir&#x2F;2ADD --chown&#x3D;bin files* &#x2F;somedir&#x2F;3ADD --chown&#x3D;1 files* &#x2F;somedir&#x2F;4ADD --chown&#x3D;10:11 files* &#x2F;somedir&#x2F; 如果容器的根目录文件系统不包含/etc/passwd或者/etc/group文件并且用户名和用户组名都没有使用--chown选项，那么进行ADD操作的时候构建将会失败。使用数字的IDs不需要查找，而且对根文件系统的内容没有要求。 在&lt;src&gt;是远程文件URL的情况下，目标将具有600的权限。如果正在检索的远程文件具有HTTP Last-Modified标头，则来自该标头的时间戳将用于设置目的地上的mtime文件。然而，像在ADD期间处理的任何其它文件一样，在决定文件是否被更改或者缓存是否被更新的时候，mtime不会被考虑进去。 注意： 如果通过传递一个Dockerfile通过STDIN（docker build - &lt;somefile）构建，没有构建上下文，所以Dockerfile只能包含一个基于URL的ADD指令。您还可以通过STDIN传递压缩归档文件：（docker build - &lt;archive.tar.gz），归档根目录下的Dockerfile和归档的其余部分将在构建的上下文中使用。 注意： 如果您的URL文件使用身份验证保护，则您需要使用RUN wget，RUN curl或从容器内使用其他工具，因为ADD指令不支持身份验证。 注意： 如果&lt;src&gt;的内容已更改，第一个遇到的ADD指令将使来自Dockerfile的所有后续指令的高速缓存无效。这包括使用于RUN指令的高速缓存无效。有关详细信息，请参阅Dockerfile最佳实践指南。 ADD遵循以下规则： &lt;src&gt;路径必须在构建的上下文中;你不能ADD ../something /something，因为docker构建的第一步是发送上下文目录（和子目录）到docker守护进程。如果&lt;src&gt;是URL并且&lt;dest&gt;不以尾部斜杠结尾，则从URL下载文件并将其复制到&lt;dest&gt;。 如果&lt;src&gt;是URL并且&lt;dest&gt;以尾部斜杠结尾，则从URL中推断文件名，并将文件下载到&lt;dest&gt;/&lt;filename&gt;。例如，ADD http://example.com/foobar /会创建文件/foobar。网址必须有一个非平凡的路径，以便在这种情况下可以发现一个适当的文件名（http://example.com不会工作）。 如果&lt;src&gt;是目录，则复制目录的整个内容，包括文件系统元数据。 注意： 目录本身不被复制，只是其内容。 如果&lt;src&gt;是识别的压缩格式（identity，gzip，bzip2或xz）的本地tar存档，则将其解包为目录。来自远程URL的资源不会解压缩。当目录被复制或解压缩时，它具有与tar -x相同的行为：结果是以下的联合： 无论目的地路径上存在什么，而且 原目标树的内容，冲突以逐个文件为基础解析为“2.”。 注意： 文件是否被识别为识别的压缩格式，仅基于文件的内容，而不是文件的名称。例如，如果一个空文件以.tar.gz结尾，则不会被识别为压缩文件，并且不会生成任何解压缩错误消息，而是将该文件简单地复制到目的地。 如果&lt;src&gt;是任何其他类型的文件，它会与其元数据一起单独复制。在这种情况下，如果&lt;dest&gt;以尾部斜杠/结尾，它将被认为是一个目录，并且&lt;src&gt;的内容将被写在&lt;dest&gt;/base(&lt;src&gt;)。 如果直接或由于使用通配符指定了多个&lt;src&gt;资源，则&lt;dest&gt;必须是目录，并且必须以斜杠/结尾。 如果&lt;dest&gt;不以尾部斜杠结尾，它将被视为常规文件，&lt;src&gt;的内容将写在&lt;dest&gt;。 如果&lt;dest&gt;不存在，则会与其路径中的所有缺少的目录一起创建。 COPYCOPY有两种格式： COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;](带空格的路径需要使用这种格式) 注意： --chown特性只支持构建Linux容器的Dockerfile，而且不会在Windows容器中生效。由于用户和用户组概念不能再Linux和Windows之间转化，所以使用/etc/passwd和/etc/group把用户和用户组转化为ID这种特性只在基于Linux的容器下可行。 COPY指令从&lt;src&gt;复制新的文件、路径或者远程文件URL到容器的文件系统中的&lt;dest&gt;位置。 可以指定多个&lt;src&gt;资源，但是如果他们是文件或者目录的话，他们的路径应该是相对于构建上下文的。 每一个&lt;src&gt;可以包括通配符，适配规则将按照Go语言的filepath.Match规则。比如： 1COPY hom* &#x2F;mydir&#x2F; # 添加所有文件名以 &quot;hom&quot; 开头的文件2COPY hom?.txt &#x2F;mydir&#x2F; # ? 可以替代一个单个字符，比如， &quot;home.txt&quot; &lt;dest&gt;是一个绝对路径，或者相对于WORKDIR的路径，在其中，资源将被复制到目标容器。 1COPY test relativeDir&#x2F; # adds &quot;test&quot; to &#96;WORKDIR&#96;&#x2F;relativeDir&#x2F;2COPY test &#x2F;absoluteDir&#x2F; # adds &quot;test&quot; to &#x2F;absoluteDir&#x2F; 当添加一个包含特殊符号(比如[和])的文件或者目录，你需要对这些字符进行符合Golang规则的转义，防止它们被视为一个统配规则。比如，想要添加一个文件名为arr[0].txt的文件，你需要： 1COPY arr[[]0].txt &#x2F;mydir&#x2F; # 复制一个名为 &quot;arr[0].txt&quot; 的文件到 &#x2F;mydir&#x2F; 一个新的文件或者目录被创造的时候，它的UID和GID将为0，除非用选项--chown指定了用户和用户组或者UID/GID组合请求指定的内容添加所有权。--chown选线的格式允许用户名或者用户组名以字符串或者整形形式的UID或GID组合。提供一个不带用户组的用户名或者不带GID的UID的话，会使用同样的UID作为GID。如果提供了一个用户名或者用户组名，容器的根目录文件系统/etc/passwd和/etc/group文件会被使用，来执行从名称到UID或GID的转换。下列栗子展示了合法的--chown选项的定义： 1COPY --chown&#x3D;55:mygroup files* &#x2F;somedir&#x2F;2COPY --chown&#x3D;bin files* &#x2F;somedir&#x2F;3COPY --chown&#x3D;1 files* &#x2F;somedir&#x2F;4COPY --chown&#x3D;10:11 files* &#x2F;somedir&#x2F; 如果容器的根目录文件系统不包含/etc/passwd或者/etc/group文件并且用户名和用户组名都没有使用--chown选项，那么进行COPY操作的时候构建将会失败。使用数字的IDs不需要查找，而且对根文件系统的内容没有要求。 注意： 如果你构建的时候使用STDIN(docker build - &lt; somefile)，那么就没有构建上下文，所以COPY不能使用。 COPY可以选择添加--from=&lt;name|index&gt;选项，用来设置源目标位置为前一个构建阶段（用FROM .. AS &lt;name&gt;创建）从而用来代替用户发送的构建上下文。这个选项会接受一个数字索引，这个索引是从FROM指令开始所有之前的构建阶段分配的。 COPY遵循以下规则： &lt;src&gt;路径必须在构建的上下文中;你不能COPY ../something /something，因为docker构建的第一步是发送上下文目录（和子目录）到docker守护进程。 如果&lt;src&gt;是目录，则复制目录的整个内容，包括文件系统元数据。 注意： 目录本身不被复制，只是其内容。 如果&lt;src&gt;是任何其他类型的文件，它会与其元数据一起单独复制。在这种情况下，如果&lt;dest&gt;以尾部斜杠/结尾，它将被认为是一个目录，并且&lt;src&gt;的内容将被写在&lt;dest&gt;/base(&lt;src&gt;)。 如果直接或由于使用通配符指定了多个&lt;src&gt;资源，则&lt;dest&gt;必须是目录，并且必须以斜杠/结尾。 如果&lt;dest&gt;不以尾部斜杠结尾，它将被视为常规文件，&lt;src&gt;的内容将写在&lt;dest&gt;。 如果&lt;dest&gt;不存在，则会与其路径中的所有缺少的目录一起创建。 ENTRYPOPTINTENTRYPOINT有两种形式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;](exec形式，推荐) ENTRYPOINT command param1 param2(shell形式) 一个ENTRYPOINT允许你配置一个将作为可执行文件的容器。 比如，下例将会启动一个默认内容的nginx，监听80端口： 1docker run -i -t --rm -p 80:80 nginx docker run &lt;image&gt;的命令行参数将会在添加在exec格式的ENTRYPOINT的所有参数后面，而且会覆盖所有CMD指令所指定的参数。这允许参数通过，进入到入口点。比如docker run &lt;image&gt; -d会传递-d参数到入口点。你可以通过使用docker run --entrypoint重写ENTRYPOINT指令。 shell格式阻止使用任何CMD或者run命令行参数，但是有缺点就是你的ENTRYPOINT将会从作为/bin/bash -c的子命令开始，这就不会传递信号了。这意味着可执行文件不会成为容器的PID 1，而且不会接受Unix信号，所以你的可执行文件不会收到docker stop &lt;container&gt;的SIGTERM。 只有Dockerfile中最后的ENTRYPOINT指令会生效。 Exec格式的ENTRYPOINT示例你可以使用exec形式的ENTRYPOINT用于设置相当稳定的默认命令行和参数，然后使用任意形式的CMD指令设置额外的、可能被修改的默认命令。 1FROM ubuntu2ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;]3CMD [&quot;-c&quot;] 当你运行一个容器，你可以看到top是唯一的进程： 1$ docker run -it --rm --name test top -H2top - 08:25:00 up 7:27, 0 users, load average: 0.00, 0.01, 0.053Threads: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie4%Cpu(s): 0.1 us, 0.1 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st5KiB Mem: 2056668 total, 1616832 used, 439836 free, 99352 buffers6KiB Swap: 1441840 total, 0 used, 1441840 free. 1324440 cached Mem78 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND9 1 root 20 0 19744 2336 2080 R 0.0 0.1 0:00.04 top 想要检查更多的结果，可以使用docker exec: 1$ docker exec -it test ps aux2USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND3root 1 2.6 0.1 19752 2352 ? Ss+ 08:24 0:00 top -b -H4root 7 0.0 0.1 15572 2164 ? R+ 08:25 0:00 ps aux 而且你可以优雅的使用docker stop test请求关闭top。 下例Dockerfile展示了使用ENTRYPOINT在前台运行Apache（作为PID 1）： 1FROM debian:stable2RUN apt-get update &amp;&amp; apt-get install -y --force-yes apache23EXPOSE 80 4434VOLUME [&quot;&#x2F;var&#x2F;www&quot;, &quot;&#x2F;var&#x2F;log&#x2F;apache2&quot;, &quot;&#x2F;etc&#x2F;apache2&quot;]5ENTRYPOINT [&quot;&#x2F;usr&#x2F;sbin&#x2F;apache2ctl&quot;, &quot;-D&quot;, &quot;FOREGROUND&quot;] 如果你需要写一个开始脚本作为单独可执行文件，你可以保证最终的可执行文件通过exec和gosu命令接受到Unix信号： 1#!&#x2F;usr&#x2F;bin&#x2F;env bash2set -e34if [ &quot;$1&quot; &#x3D; &#39;postgres&#39; ]; then5 chown -R postgres &quot;$PGDATA&quot;67 if [ -z &quot;$(ls -A &quot;$PGDATA&quot;)&quot; ]; then8 gosu postgres initdb9 fi1011 exec gosu postgres &quot;$@&quot;12fi1314exec &quot;$@&quot; 最终，如果你需要在关闭的时候做一些额外的清理工作（或是与其他容器进行通讯），或者联合多个可执行文件，你可能需要确认ENTRYPOINT脚本能够接受到Unix信号并且传递他们，然后做更多工作： 1#!&#x2F;bin&#x2F;sh2# Note: I&#39;ve written this using sh so it works in the busybox container too34# USE the trap if you need to also do manual cleanup after the service is stopped,5# or need to start multiple services in the one container6trap &quot;echo TRAPed signal&quot; HUP INT QUIT TERM78# start service in background here9&#x2F;usr&#x2F;sbin&#x2F;apachectl start1011echo &quot;[hit enter key to exit] or run &#39;docker stop &lt;container&gt;&#39;&quot;12read1314# stop service and clean up here15echo &quot;stopping apache&quot;16&#x2F;usr&#x2F;sbin&#x2F;apachectl stop1718echo &quot;exited $0&quot; 如果你用docker run -it --rm -p 80:80 --name test apache命令运行这个镜像，你可以通过docker exec检查容器的进程，或者用docker top，然后请求脚本来停止Apache： 1$ docker exec -it test ps aux2USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND3root 1 0.1 0.0 4448 692 ? Ss+ 00:42 0:00 &#x2F;bin&#x2F;sh &#x2F;run.sh 123 cmd cmd24root 19 0.0 0.2 71304 4440 ? Ss 00:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start5www-data 20 0.2 0.2 360468 6004 ? Sl 00:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start6www-data 21 0.2 0.2 360468 6000 ? Sl 00:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start7root 81 0.0 0.1 15572 2140 ? R+ 00:44 0:00 ps aux8$ docker top test9PID USER COMMAND1010035 root &#123;run.sh&#125; &#x2F;bin&#x2F;sh &#x2F;run.sh 123 cmd cmd21110054 root &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start1210055 33 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start1310056 33 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start14$ &#x2F;usr&#x2F;bin&#x2F;time docker stop test15test16real 0m 0.27s17user 0m 0.03s18sys 0m 0.03s 注意： 你可以使用--entrypoint来重写ENTRYPOINT，但是这只会设置二进制到exec(不会使用sh -c)。 注意： exec格式是解析成JSON数组的格式，这意味着你必须使用双引号(“)包围文字，而不是用单引号(‘)。 注意： 与shell格式不同，exec格式不会调用shell命令，这意味着shell处理不会进行。比如，ENTRYPOINT [&quot;echo&quot;,&quot;$HOME&quot;]不会对$HOME进行变量替换。如果你想要使用shell处理，那么就用shell格式或者直接执行shell，比如：ENTRYPOINT [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]。当使用exec形式并直接执行shell时，正如shell形式的情况，它是做环境变量扩展的shell，而不是docker。 Shell形式ENTRYPOINT的示例您可以为ENTRYPOINT指定一个纯字符串，它将在/bin/sh -c中执行。这中形式将使用shell处理来替换shell环境变量，并且将忽略任何CMD或docker run命令行参数。要确保docker stop将正确地发出任何长时间运行的ENTRYPOINT可执行文件，您需要记住用exec启动它： 1FROM ubuntu2ENTRYPOINT exec top -b 当你运行这个景象，你会看到单独一个PID 1进程： 1$ docker run -it --rm --name test top2Mem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cached3CPU: 5% usr 0% sys 0% nic 94% idle 0% io 0% irq 0% sirq4Load average: 0.08 0.03 0.05 2&#x2F;98 65 PID PPID USER STAT VSZ %VSZ %CPU COMMAND6 1 0 root R 3164 0% 0% top -b 这些会在docker stop的时候干净利落的退出： 1$ &#x2F;usr&#x2F;bin&#x2F;time docker stop test2test3real 0m 0.20s4user 0m 0.02s5sys 0m 0.04s 如果你忘记在ENTRYPOINT的开头加上exec： 1FROM ubuntu2ENTRYPOINT top -b3CMD --ignored-param1 你可以运行它（为了下一步的运行，需要给它个名字）: 1$ docker run -it --name test top --ignored-param22Mem: 1704184K used, 352484K free, 0K shrd, 0K buff, 140621524238337K cached3CPU: 9% usr 2% sys 0% nic 88% idle 0% io 0% irq 0% sirq4Load average: 0.01 0.02 0.05 2&#x2F;101 75 PID PPID USER STAT VSZ %VSZ %CPU COMMAND6 1 0 root S 3168 0% 0% &#x2F;bin&#x2F;sh -c top -b cmd cmd27 7 1 root R 3164 0% 0% top -b 你可以看到top的输出信息中指定的ENTRYPOINT不是PID 1。 如果你运行了docker stop test，容器不会退出的很干净，stop命令会在超时之后强制发送一个SIGKILL： 1$ docker exec -it test ps aux2PID USER COMMAND3 1 root &#x2F;bin&#x2F;sh -c top -b cmd cmd24 7 root top -b5 8 root ps aux6$ &#x2F;usr&#x2F;bin&#x2F;time docker stop test7test8real 0m 10.19s9user 0m 0.04s10sys 0m 0.03s 理解CMD和ENTRYPOINT是如何交互的CMD和ENTRYPOINT指令都定义了容器运行的时候命令是如何执行的，下面有几条规则描述了他们的关系： Dockerfile应当是定至少一条CMD或者ENTRYPOINT命令。 使用容器作为一个可执行文件的时候，ENTRYPOINT应当被定义。 CMD应该作为一种为ENTRYPOINT命令定义默认参数的方法，或者作为容器中执行临时命令的方式。 在使用交互式参数运行容器的时候，CMD将会被重写。 下表展示了不同的ENTRYPOINT/CMD组合是如何执行的：||No ENTRYPOINT|ENTRYPOINT exec_entry p1_entry|ENTRYPOINT [“exec_entry”, “p1_entry”]-|-|-|-No CMD|error,not allowed|/bin/sh -c exec_entry p1_entry|exec_entry p1_entryCMD [“exec_cmd”, “p1_cmd”]|exec_cmd p1_cmd|/bin/sh -c exec_entry p1_entry|exec_entry p1_entry exec_cmd p1_cmdCMD [“p1_cmd”, “p2_cmd”]|p1_cmd p2_cmd|/bin/sh -c exec_entry p1_entry|exec_entry p1_entry p1_cmd p2_cmdCMD exec_cmd p1_cmd|/bin/sh -c exec_cmd p1_cmd|/bin/sh -c exec_entry p1_entry|exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME1VOLUME [&quot;&#x2F;data&quot;] VOLUME指令创建了一个挂载点，并且指定了一个名字，然后标记这个挂载点来承载来自原生宿主机或者其他容器的额外被挂载数据卷。它的值可以使一个JSON数组，VOLUME [&quot;/var/log/&quot;]，或者一个普通的有多个参数的字符串，比如VOLUME /var/log或VOLUME /var/log /var/db。想看更多的信息/举例和通过Docker客户端进行挂载的指令，可以参考Share Directories via Volumes文档。 docker run命令用基础映像中指定位置存在的任何数据初始化新创建的卷。比如，思考下例Dockerfile片段： 1FROM ubuntu2RUN mkdir &#x2F;myvol3RUN echo &quot;hello world&quot; &gt; &#x2F;myvol&#x2F;greeting4VOLUME &#x2F;myvol 这个Dockerfile产生了一个镜像使docker run创建了一个新的在/myvol的挂载点而且复制greeting文件到新创建的数据卷上。 注意指定数据卷记住下列关于Dockerfile中关于数据卷的内容： 基于Windows的容器的数据卷：当你使用基于Windows的容器的时候，容器的数据卷的目标地址必须是下列之一： 一个不存在的或者是空的目录 除了C:的驱动 改变Dockerfile内的数据卷：如果任何构建步骤在被声明之前修改了数据卷内的数据，那么这些改变将被抛弃。 JSON格式化：这个列表会被解析为一个JSON数组，所以你必须用双引号(“)包含字符，而不是用单引号(“)。 宿主目录是在docker运行时声明：宿主路径(挂载点)，源于它自身的特性，依赖于宿主。这是为了保护镜像的可移植性，因为一个指定的宿主路径不能保证在所有主机上可得。基于这个原因，你不能在Dockerfile中挂载主机目录。VOLUMN指令不支持制定一个host-dir参数。你必须在创建或者运行容器的时候指定一个挂载点。 USER1USER &lt;user&gt;[:&lt;group&gt;] or2USER &lt;UID&gt;[:&lt;GID&gt;] USER指令在使用CMD、RUN或者ENTRYPOINT命令启动一个镜像的时候，设置用户名(或者UID)并且可选择指定用户组(或者GID)。 注意： 当一个用户没有一个首选用户组的时候，镜像（或者下一个指令）将会用root用户组来运行。 在Windows下，必须先创建用户，如果不是內建用户的情况。这可以通过作为Dockerfile的一部分调用的net user命令来完成。 1FROM microsoft&#x2F;windowsservercore2# Create Windows user in the container3RUN net user &#x2F;add patrick4# Set it for subsequent commands5USER patrick WORKDIR1WORKDIR &#x2F;path&#x2F;to&#x2F;workdir WORKDIR指令为Dockerfile中后续的RUN、CMD、ENTRYPOINT、COPY和ADD指令设置工作路径。如果WORKDIR不存在，那么即便没使用Dockerfile中的任何指令，也会被创建。 WORKDIR指令会在Dockerfile中被多次使用。如果提供了一个相关的路径，那么它就会与前面的WORKDIR指令的路径相关联。比如： 1WORKDIR &#x2F;a2WORKDIR b3WORKDIR c4RUN pwd 最终pwd命令的输出结果会是/a/b/c。 WORKDIR指令可以处理之前使用ENV命令设置的环境变量。你可以只使用Dockerfile中明确设置的环境变量。比如： 1ENV DIRPATH &#x2F;path2WORKDIR $DIRPATH&#x2F;$DIRNAME3RUN pwd 最终pwd命令的输出结果会是/path/$DIRNAME ARG1ARG &lt;name&gt;[&#x3D;&lt;default value&gt;] ARG指令定义了一个变量，用户可以使用--build-arg &lt;varname&gt; = &lt;value&gt;标志在构建器中通过docker build命令将其传递给构建器。如果用户指定了一个在Dockerfile中没有定义的构建参数，那么构建器会输出一个警告。 1[Warning] One or more build-args [foo] were not consumed. 一个Dockerfile可以包含一个或者多个ARG指令，比如，下面是一个合法的Dockerfile: 1FROM busybox2ARG user13ARG buildno4... 注意： 不推荐使用构建阶段的变量传递密钥比如github的keys，用户证书等，构建阶段变量对任何用户都是可见的，只要使用docker history命令。 默认值一个ARG指令可以选择包含一个默认值： 1FROM busybox2ARG user1&#x3D;someuser3ARG buildno&#x3D;14... 如果一个ARG指令有一个默认值，而且没有其他值在构建阶段传递，那么构建器就会使用默认值。 作用范围一个ARG变量的定义从它被定义的哪一行开始生效，而不是从命令行或其他地方使用参数的时候开始。比如，考虑下面这个Dockerfile: 11 FROM busybox22 USER $&#123;user:-some_user&#125;33 ARG user44 USER $user5... 一个用户用下面的命令构建了这个文件： 1$ docker build --build-arg user&#x3D;what_user . 在这种情况下，RUN指令使用v1.0.0而不是用户传递的ARG设置：v2.0.1这种行为类似于一个shell脚本，其中一个本地作用域变量覆盖作为参数传递的变量或从环境继承的变量，定义点。 使用上面的示例，但使用不同的ENV规范，您可以在ARG和ENV指令之间创建更有用的交互： 11 FROM ubuntu22 ARG CONT_IMG_VER33 ENV CONT_IMG_VER $&#123;CONT_IMG_VER:-v1.0.0&#125;44 RUN echo $CONT_IMG_VER 与ARG指令不通，ENV的值会在构建镜像的阶段一直存在。考虑一个docker不使用--build-arg参数的构建： 1$ docker build . 使用这个Dockerfile为例，CONT_IMG_VER仍然在镜像中持续存在，但是它的值会是v1.0.0，如同第三行中ENV指令设定的那样。 这个例子中的变量扩展技术允许你从命令行中传递一个参数，并通过利用ENV指令将其保存在最终镜像中。变量扩展只支持有限的Dockerfile指令。 预定义的ARGsDocker 有一组预定义的ARG变量，您可以在Dockerfile中使用没有相应的ARG指令的变量。 HTTP_PROXY http_proxy HTTPS_PROXY https_proxy FTP_PROXY ftp_proxy NO_PROXY no_proxy 想要使用它们，只需要使用命令行传递它们： 1--build-arg &lt;varname&gt;&#x3D;&lt;value&gt; 默认的，这些预定义变量会被docker history的输出结果排除。将它们排除在外可以减少在HTTP_PROXY变量中意外泄漏敏感身份验证信息的风险。 举个栗子，考虑使用下面的语句构建Dockerfile： 1--build-arg HTTP_PROXY&#x3D;http:&#x2F;&#x2F;user:pass@proxy.lon.example.com 1FROM ubuntu2RUN echo &quot;Hello World&quot; 在这种情况下，HTTP_PROXY变量的值在docker history中不可见，而且没有被缓存。如果你本地作了修改，而且你的代理服务器设置成了http://user:pass@proxy.sfo.example.com，后续的构建不会导致缓存未命中。 如果你需要重写这种状况，你可能需要在Dockerfile中添加一个ARG声明，如下： 1FROM ubuntu2ARG HTTP_PROXY3RUN echo &quot;Hello World&quot; 当构建这个Dockerfile的时候，HTTP_PROXY被保存在了docker history中，而且改变了它的值使构建缓存失效。 对构建缓存的影响作为ENV变量，ARG变量不会保留在构建的镜像中。但是，ARG变量确实会以类似的方式影响构建缓存。如果Dockerfile定义了一个ARG变量，其值与以前的版本不同，那么在第一次使用时会发生“缓存未命中”，而不是其定义。尤其是，ARG指令之后的所有RUN指令隐式使用ARG变量（作为环境变量），因此可能导致缓存未命中。除非在Dockerfile中存在匹配的ARG语句，否则所有预定义的ARG变量都可以免于缓存。 举个栗子，考虑下面两个Dockerfile： 11 FROM ubuntu22 ARG CONT_IMG_VER33 RUN echo $CONT_IMG_VER 11 FROM ubuntu22 ARG CONT_IMG_VER33 RUN echo hello 如果在命令行中指定--build-arg CONT_IMG_VER = &lt;value&gt;，则在这两种情况下，第2行的规范不会导致缓存未命中;第3行确实导致缓存未命中.ARG CONT_IMG_VER导致RUN行被识别为与运行CONT_IMG_VER = &lt;value&gt;echo hello相同，因此如果&lt;value&gt;发生更改，则会导致缓存未命中。 考虑同一个命令行下的另一个例子： 11 FROM ubuntu22 ARG CONT_IMG_VER33 ENV CONT_IMG_VER $CONT_IMG_VER44 RUN echo $CONT_IMG_VER 在这个例子中，高速缓存未命中发生在第3行。发生未命中是因为ENV中的变量值引用了ARG变量，并且该变量通过命令行进行了更改。在这个例子中，ENV命令会使镜像包含该值。 如果ENV指令覆盖同名的ARG指令，就像这个Dockerfile： 11 FROM ubuntu22 ARG CONT_IMG_VER33 ENV CONT_IMG_VER hello44 RUN echo $CONT_IMG_VER 第3行不会导致缓存未命中，因为CONT_IMG_VER的值是常量（hello）。因此，RUN（第4行）上使用的环境变量和值在构建之间不会改变。 ONBUILD1ONBUILD [INSTRUCTION] ONBUILD指令为镜像添加一个触发器指令，稍后将该镜像用作另一个构建的基础。触发器将在下游构建的上下文中执行，就像它已经在下游Dockerfile中的FROM指令之后立即插入一样。 任何构建指令都可以注册为触发器。 如果您正在构建将用作构建其他映像的基础的映像，那么这非常有用，例如可以使用用户特定配置自定义的应用程序构建环境或守护程序。 例如，如果您的映像是可重用的Python应用程序构建器，则需要将应用程序源代码添加到特定目录中，并且可能需要在此之后调用构建脚本。你现在不能只调用ADD和RUN，因为你还没有访问应用程序的源代码，每个应用程序的版本都不一样。您可以简单地向应用程序开发人员提供样板化的Dockerfile以复制粘贴到他们的应用程序中，但效率低下，容易出错，难以更新，因为它与特定于应用程序的代码混合在一起。 解决方案是使用ONBUILD注册先行指令，以便在以后的构建阶段运行。 这是它如何工作的： 当遇到ONBUILD指令时，构建器会为正在构建的映像的元数据添加一个触发器。该指令不会影响当前的构建。 在构建结束时，所有触发器的列表都存储在图像清单中的OnBuild键下。可以使用docker inspect命令检查它们。 稍后，可以使用FROM指令将镜像用作新构建的基础。作为处理FROM指令的一部分，下游构建器会查找ONBUILD触发器，并按照它们所注册的相同顺序执行它们。如果任何触发器失败，则FROM指令将被中止，从而导致构建失败。如果所有触发器都成功，则FROM指令完成，构建将像往常一样继续。 触发器在执行后从最终图像中清除。换句话说，他们不是由“大孩子”构建的遗传。 例如，你可能会添加这样的东西： 1[...]2ONBUILD ADD . &#x2F;app&#x2F;src3ONBUILD RUN &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python-build --dir &#x2F;app&#x2F;src4[...] 注意： 不允许使用ONBUILD ONBUILD链接ONBUILD。 注意： ONBUILD指令可能不会触发FROM或者MAINTAINER指令。 STOPSIGNAL1STOPSIGNAL signal STOPSIGNAL指令设置将被发送到容器的系统调用信号以退出。该信号可以是一个有效的无符号数字，与内核的syscall表中的一个位置（例如9）匹配，或者是一个SIGNAME格式的信号名称，例如SIGKILL。 HEALTHCHECKHEALTHCHECK指令有两种形式： HEALTHCHECK [选项] CMD命令（通过在容器中运行一个命令来检查容器的健康状况） HEALTHCHECK NONE（禁用从基础映像继承的任何健康检查） HEALTHCHECK指令告诉Docker如何测试一个容器来检查它是否还在工作。这可以检测到一些情况，例如一个陷入无限循环的web服务器，即使服务器进程仍在运行，也无法处理新的连接。 当容器指定了健康状况检查时，除了正常状态之外，还有一个健康状态。这个状态是最初开始的。每当健康检查通过，它变得健康（无论以前在哪个状态）。经过一定次数的连续失败后，变得不健康。 可以在CMD之前出现的选项是： --interval = DURATION（默认：30秒） --timeout = DURATION（默认：30s） --start-period = DURATION（默认值：0s） --retries = N（默认值：3） 运行状况检查将首先在容器启动后的间隔秒内运行，然后在每次前一次检查完成后再次间隔几秒。 如果单次运行检查花费的时间超过了超过秒数，则认为检查失败。 多次连续的健康检查失败的容器被认为是不健康的。 启动周期为需要时间启动的容器提供初始化时间。在此期间的探测失败不会计入最大重试次数。但是，如果在启动期间运行状况检查成功，则认为容器已启动，并且所有连续的故障都将计入最大重试次数。 Dockerfile中只能有一个HEALTHCHECK指令。如果您列出多个，则只有最后一个HEALTHCHECK将生效。 CMD关键字之后的命令可以是shell命令（例如，HEALTHCHECK CMD / bin / check-running）或exec阵列（与其他Dockerfile命令一样;例如参见ENTRYPOINT以获得详细信息）。 该命令的退出状态表示容器的健康状态。可能的值是： 0：成功 - 容器很健康而且随时可以使用 1：不健康 - 容器运行状态不正常 2：保留 - 不要使用这个退出状态码 例如，要每隔五分钟检查一次，网络服务器是否能够在三秒内为网站的主页提供服务： 1HEALTHCHECK --interval &#x3D; 5m --timeout &#x3D; 3s \\2 CMD curl -f http：&#x2F;&#x2F; localhost &#x2F; ||exit 1 为了帮助调试失败的探测器，命令在stdout或stderr上写入的任何输出文本（UTF-8编码）都将存储在健康状态中，并且可以通过docker检查进行查询。这样的输出应该保持简短（目前只有前4096个字节被存储）。 当容器的运行状况发生变化时，会以新状态生成一个health_status事件。 在Docker 1.12中添加了HEALTHCHECK功能。 SHELL1SHELL [&quot;executable&quot;, &quot;parameters&quot;] SHELL指令允许覆盖用于shell命令形式的默认shell。 Linux上的默认shell是[&quot;/bin/sh&quot;，&quot;-c&quot;]，在Windows上是[&quot;cmd&quot;，&quot;/S&quot;，&quot;/C&quot;]。 SHELL指令必须以JSON格式写入Dockerfile中。 SHELL指令在Windows中有两个常用和完全不同的本机shell特别有用：cmd和powershell，以及可用的备用shell，包括sh。 SHELL指令可以出现多次。每个SHELL指令将覆盖所有先前的SHELL指令，并影响所有后续指令。例如： 1FROM microsoft&#x2F;windowsservercore23# Executed as cmd &#x2F;S &#x2F;C echo default4RUN echo default56# Executed as cmd &#x2F;S &#x2F;C powershell -command Write-Host default7RUN powershell -command Write-Host default89# Executed as powershell -command Write-Host hello10SHELL [&quot;powershell&quot;, &quot;-command&quot;]11RUN Write-Host hello1213# Executed as cmd &#x2F;S &#x2F;C echo hello14SHELL [&quot;cmd&quot;, &quot;&#x2F;S&quot;&quot;, &quot;&#x2F;C&quot;]15RUN echo hello 如果在Dockerfile中使用SHELL指令，则会影响以下指令：RUN，CMD和ENTRYPOINT。 以下示例是在Windows上可以通过使用SHELL指令简化的常见模式： 1...2RUN powershell -command Execute-MyCmdlet -param1 &quot;c:\\foo.txt&quot;3... docker调用的命令是： 1cmd &#x2F;S &#x2F;C powershell -command Execute-MyCmdlet -param1 &quot;c:\\foo.txt&quot; 这是低效率的，原因有两个。首先，调用一个不必要的cmd.exe命令处理器（又名shell）。其次，shell格式中的每条RUN指令都需要在命令前加上一个额外的powershell命令。 为了提高效率，可以采用两种机制之一。一种是使用RUN命令的JSON格式，例如： 1...2RUN [&quot;powershell&quot;，&quot;-command&quot;，&quot;Execute-MyCmdlet&quot;，&quot;-param1 \\&quot;c:\\\\foo.txt\\&quot;&quot;]3... 虽然JSON格式是明确的，并且不使用不必要的cmd.exe，但是通过双引号和转义确实需要更多的冗长。另一种机制是使用SHELL指令和shell形式，为Windows用户提供更自然的语法，特别是与escape转义指令结合使用时： 1# escape&#x3D;&#96;23FROM microsoft&#x2F;nanoserver4SHELL [&quot;powershell&quot;,&quot;-command&quot;]5RUN New-Item -ItemType Directory C:\\Example6ADD Execute-MyCmdlet.ps1 c:\\example\\7RUN c:\\example\\Execute-MyCmdlet -sample &#39;hello world&#39; 结果是： 1PS E:\\docker\\build\\shell&gt; docker build -t shell .2Sending build context to Docker daemon 4.096 kB3Step 1&#x2F;5 : FROM microsoft&#x2F;nanoserver4 ---&gt; 22738ff49c6d5Step 2&#x2F;5 : SHELL powershell -command6 ---&gt; Running in 6fcdb6855ae27 ---&gt; 6331462d43008Removing intermediate container 6fcdb6855ae29Step 3&#x2F;5 : RUN New-Item -ItemType Directory C:\\Example10 ---&gt; Running in d0eef8386e97111213 Directory: C:\\141516Mode LastWriteTime Length Name17---- ------------- ------ ----18d----- 10&#x2F;28&#x2F;2016 11:26 AM Example192021 ---&gt; 3f2fbf1395d922Removing intermediate container d0eef8386e9723Step 4&#x2F;5 : ADD Execute-MyCmdlet.ps1 c:\\example\\24 ---&gt; a955b2621c3125Removing intermediate container b825593d39fc26Step 5&#x2F;5 : RUN c:\\example\\Execute-MyCmdlet &#39;hello world&#39;27 ---&gt; Running in be6d8e63fe7528hello world29 ---&gt; 8e559e9bf42430Removing intermediate container be6d8e63fe7531Successfully built 8e559e9bf42432PS E:\\docker\\build\\shell&gt; SHELL指令也可以用来修改shell的运行方式。例如，在Windows上使用SHELL cmd /S /C /V:ON|OFF，可以修改延迟的环境变量扩展语义。 SHELL指令也可以在Linux上使用，如果需要备用shell，如zsh，csh，tcsh等。 SHELL功能是在Docker 1.12中添加的。 Dockerfile 示例在你可以看到Dockerfile语法之前，如果你对更多的实例感兴趣，可以参考docker文档。 1# Nginx2#3# VERSION 0.0.145FROM ubuntu6LABEL Description&#x3D;&quot;This image is used to start the foobar executable&quot; Vendor&#x3D;&quot;ACME Products&quot; Version&#x3D;&quot;1.0&quot;7RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server 1# Firefox over VNC2#3# VERSION 0.345FROM ubuntu67# Install vnc, xvfb in order to create a &#39;fake&#39; display and firefox8RUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefox9RUN mkdir ~&#x2F;.vnc10# Setup a password11RUN x11vnc -storepasswd 1234 ~&#x2F;.vnc&#x2F;passwd12# Autostart firefox (might not be the best way, but it does the trick)13RUN bash -c &#39;echo &quot;firefox&quot; &gt;&gt; &#x2F;.bashrc&#39;1415EXPOSE 590016CMD [&quot;x11vnc&quot;, &quot;-forever&quot;, &quot;-usepw&quot;, &quot;-create&quot;]17# Multiple images example18#19# VERSION 0.12021FROM ubuntu22RUN echo foo &gt; bar23# Will output something like &#x3D;&#x3D;&#x3D;&gt; 907ad6c2736f2425FROM ubuntu26RUN echo moo &gt; oink27# Will output something like &#x3D;&#x3D;&#x3D;&gt; 695d7793cbe42829# You&#39;ll now have two images, 907ad6c2736f with &#x2F;bar, and 695d7793cbe4 with30# &#x2F;oink. Dockerfile 最佳实践Docker可以通过从Dockerfile中阅读 一般参考和建议容器应该是短暂的你Dockerfile中定义的镜像所产生的容器应该是尽可能短暂的。提到“短暂“，我们的意思是它可以停止和被销毁，而且可以用一个最小化的启动和配置流程来构建一个新的替代它。 使用.dockerignore文件当您发出docker build命令时，您所在的当前工作目录称为构建上下文，并且Dockerfile必须位于此构建上下文中的某个位置。默认情况下，它假定位于当前目录中，但是可以使用-f标志指定不同的位置。无论Dockerfile实际存在于哪里，当前目录中的所有文件和目录的递归内容都会作为构建上下文发送到Docker守护进程。无意中包含构建映像所不需要的文件会产生较大的构建上下文和较大的映像大小。这些反过来可以增加构建时间，拉和推图像的时间以及容器的运行时间大小。要查看构建上下文有多大，在构建Dockerfile时查找如下消息。 1Sending build context to Docker daemon 187.8MB 要排除与构建无关的文件，而不重构源代码库，请使用.dockerignore文件。该文件支持类似于.gitignore文件的排除模式。有关创建一个的信息，请参阅.dockerignore文件。除了使用.dockerignore文件之外，请查看以下关于多阶段构建的信息。 使用多阶段构建如果你使用Docker 17.05或者更高版本，你可以使用多阶段构建来彻底减少最终镜像的大小。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://beritra.github.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://beritra.github.com/tags/Docker/"},{"name":"Linux","slug":"Linux","permalink":"http://beritra.github.com/tags/Linux/"},{"name":"Dockerfile","slug":"Dockerfile","permalink":"http://beritra.github.com/tags/Dockerfile/"}]},{"title":"Java 多线程 常见面试问题","slug":"Java-多线程-常见面试问题","date":"2017-05-07T08:46:29.000Z","updated":"2019-12-15T14:27:41.761Z","comments":true,"path":"2017/05/07/Java-多线程-常见面试问题/","link":"","permalink":"http://beritra.github.com/2017/05/07/Java-%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/","excerpt":"整理下Java多线程相关知识点和面试可能会问到的问题","text":"整理下Java多线程相关知识点和面试可能会问到的问题 现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行？join方法 在java中wait和sleep方法的不同？最大的不同是wait会在等待时释放锁，而sleep会持有锁。wait通常会被用于线程之间的交互，sleep通常被用于暂停执行。 volatile能提供什么保证？volatile变量提供顺序保证和可见性保证，比如JVM或者JIT会为了更好的性能对语句重排顺序，使用volatile修饰的变量即使在没有同步块的情况下赋值也不会与其他语句重拍序。volatile提供happens-before保证，确保一个线程的修改能对其他线程可见。在某些情况下还能提供原子性，比如long和double类型的数据。 Java中能创建volatile数组吗?能，Java中能创建volatile数组，但是只是一个指向数组的引用，而不是整个数组，也就是说如果多个线程同时改变数组的元素，volatile标识符就不能起到保护作用。、 volatile标识符能使一个非原子操作变成一个原子操作吗？一个典型的例子就是一个long类型。Java中读取long类型变量不是原子的，需要分成两步，如果一个线程正在修改该long变量的值，另一个线程可能就只能看到该值的一半（前32位）。但是如果对一个volatile修饰的long或者double变量的读写是原子的。","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"},{"name":"面试","slug":"Java/面试","permalink":"http://beritra.github.com/categories/Java/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Java JVM 常见面试问题","slug":"Java-JVM-常见面试问题","date":"2017-05-07T08:46:28.000Z","updated":"2019-12-15T14:27:36.921Z","comments":true,"path":"2017/05/07/Java-JVM-常见面试问题/","link":"","permalink":"http://beritra.github.com/2017/05/07/Java-JVM-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/","excerpt":"整理下JVM相关知识点和面试可能会问到的问题","text":"整理下JVM相关知识点和面试可能会问到的问题 64位JVM中，int的长度是多少位？Java中，int类型变量的长度是一个固定值，与平台无关，都是32位。 ####","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"},{"name":"面试","slug":"Java/面试","permalink":"http://beritra.github.com/categories/Java/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://beritra.github.com/tags/JVM/"},{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Java API 常见面试问题","slug":"Java-API-常见面试问题","date":"2017-05-01T16:21:53.000Z","updated":"2019-12-15T14:06:27.987Z","comments":true,"path":"2017/05/02/Java-API-常见面试问题/","link":"","permalink":"http://beritra.github.com/2017/05/02/Java-API-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/","excerpt":"整理下面试的时候可能会问到的Java常用API问题","text":"整理下面试的时候可能会问到的Java常用API问题 数据类型怎么将byte转换为String？可以使用String接收byte[]参数的构造器来进行转换，需要注意的是使用正确的编码。 1new String(\"你好啊\".getBytes(), StandardCharsets.UTF_8); 我们能将int强制转换位byte类型的变量吗？如果该值大于byte类型的范围，将会出现什么现象？是的，可以强制转换,但是Java中int是32位的，而byte是8位的，所以，如果强制转化，int类型的高24位将会被丢弃，byte类型的范围是-127到128。 Java中的++操作符是线程安全的吗？不是，这个操作实际上涉及三个指令：读取变量值，增加，再存储会内存，这个过程有可能出现多个线程交叉。 3*0.1 == 0.3 将会返回什么？true 还是 false？false，因为有些浮点数不能精确度的表示出来。可以用BigDecimal转换后进行精确计算： 1BigDecimal bigDecimal = new BigDecimal(\"0.2\");2System.out.println(bigDecimal.multiply(new BigDecimal(\"6\")).doubleValue()); 这里注意要用BigDecimal的String构造器，如果是用double类型，仍然是不精确的。参考：Java中浮点型数据Float和Double进行精确计算的问题 可以用Switch中使用String吗？从Java7开始，我们可以在switch case中使用字符串，但这仅仅是一个语法糖。内部实现在switch中使用字符串的hash code。 String和StringBuilder/StringBufferString是只读字符串，它所引用的字符串内容是不能被改变的。而StringBuffer和StringBuilder表示的字符串对象是可以修改的。StringBuilder是JDK1.5引入的，区别在于它是在单线程环境下使用的，所有方面都没有被synchronized修饰，因此它的效率也比StringBuffer高。关于性能问题，有个知乎问题：JAVA 中的 StringBuilder 和 StringBuffer 适用的场景是什么？ 评论区提到，现在用+号拼接字符串已经没有了性能损失，除非在一个循环里面重复执行，因为这样会多次创建StringBuilder 有一个面试题问：有没有哪种情况用 + 做字符串连接比调用 StringBuffer / StringBuilder 对象的 append 方法性能更好？如果连接后得到的字符串在静态存储区中是早已存在的，那么用+做字符串连接是优于 StringBuffer / StringBuilder 的 append 方法的。 集合ArrayList和LinkedList区别是什么ArrayList底层使用数组，随机访问效率高，插入和删除元素的效率低，需要连续空间；LinkedList底层使用链表，随机访问元素效率低，插入和删除效率高，不需要连续空间。 LinkedHashMap 和 PriorityQueue 的区别是什么？PriorityQueue保证最高或者最低优先级的元素总在队列头部，但是LinkedHashMap维持的顺序是元素插入的顺序。当遍历一个PriorityQueue时，没有任何顺序保证，但是LinkedHashMap保证按照插入顺序。 Java中怎么打印数组？用Arrays.toString()和Arrays.deepToString()方法。 TreeMap是用什么实现的？红黑树。 ArrayList和HashMap的默认大小是多少？ArrayList的默认大小是是个元素，HashMap的默认大小是16个元素（必须是2的幂） Iterator和ListIterator的区别是什么？ Iterator可以用来遍历Set和List集合，但是ListIterator只能用来遍历List。 Iterator对集合只能是前向遍历，但是ListIterator可以前向也可以后向。 ListIterator实现了Iterator接口，并且包含其他功能。比如：增加元素，替换元素，获取前一个和后一个元素的索引等等。 快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？Iterator的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有集合都是快速失败的，而java.util.current包下面的所有类都是安全失败的。快速失败的迭代器会抛出CurrentModificationException异常，而安全失败的迭代器永远不会抛出这样的异常。 用什么方法来实现集合的排序？可以使用有序集合比如TreeSet或者TreeMap，你也可以使用Collections.sort()来排序","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"},{"name":"面试","slug":"Java/面试","permalink":"http://beritra.github.com/categories/Java/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"API","slug":"API","permalink":"http://beritra.github.com/tags/API/"}]},{"title":"JavaFX简单入门","slug":"JavaFX简单入门","date":"2017-04-13T11:58:39.000Z","updated":"2019-12-15T15:13:48.309Z","comments":true,"path":"2017/04/13/JavaFX简单入门/","link":"","permalink":"http://beritra.github.com/2017/04/13/JavaFX%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/","excerpt":"这几天鼓捣bilibili的直播，想做一个桌面弹窗，方便查看直播消息，开始用Python和PyQt5，后来觉得Python手实在太生，写出来的代码惨不忍睹，不如就用ava写了，找工作也用得上。Java写GUI一向饱受诟病，看了半天也没啥很好的方案，推荐最多的就是JavaFX了。中文资料比较缺乏，只能慢慢摸索了，做到哪里就跟着写到哪里吧，这篇就总结下JavaFx学到的东西。","text":"这几天鼓捣bilibili的直播，想做一个桌面弹窗，方便查看直播消息，开始用Python和PyQt5，后来觉得Python手实在太生，写出来的代码惨不忍睹，不如就用ava写了，找工作也用得上。Java写GUI一向饱受诟病，看了半天也没啥很好的方案，推荐最多的就是JavaFX了。中文资料比较缺乏，只能慢慢摸索了，做到哪里就跟着写到哪里吧，这篇就总结下JavaFx学到的东西。 概述JavaFX是一个强大的图形和多媒体处理工具包集合，和Java一样是跨平台的，在Java8里面已经内置。JavaFX8新的特性（不知道啥意思跟着抄下来装逼就对了）： JavaFX应用程序的默认主题是新设计的Modena主题。详见“关键特性”一节中的Modena主题部分。 已经加入了对HTML5的支持。详见“向JavaFX应用程序中添加HTML内容”相关章节。 新添加的SwingNode类改进了与Swing的互操作性。参考“在JavaFX应用程序中嵌入Swing内容”相关章节。 新的内置UI控件，DatePicker和TableView，已经可用。参考《使用JavaFX UI控件》一文来获得更多信息。 3D图形库被改进了，增加了一些新的API类。参考“关键特性”一节中的3D图形特性部分和“开始使用JavaFX 3D图形”章节来获得更多信息。 print包现在是可用的，并且提供了公开的JavaFX打印API 加入了富本文支持 对Hi-DPI显示的支持已经变得可用了 CSS样式类变成了公开API 引入了调度服务类 JavaFX架构先放一个现成的架构图： Scene 场景图Scene在顶层部分，是构建JavaFX应用的入口，这是一个层级结构的节点树，表示了所有用户界面的视觉元素，可以处理输入，可以被渲染。场景图中的元素被称为一个节点（Node），每个节点有一个ID，样式类和一个包围盒（bounding volime），除了根节点，其他都有一个父节点，零个或者多个子节点。除此之外还有如下特性： 效果（Effects），比如模糊和阴影 不透明度（Opacity） 变换（Transforms） 事件处理器（Event handlers，例如鼠标、键盘和输入法） 应用相关的状态（Application-specific state） 图形系统JavaFX图形系统（Graphics System）是架构图中蓝色部分，是在JavaFX场景图层之下的实现细节。当系统中的图形硬件无法支持硬件加速渲染时，它将提供软件渲染技术。 在JavaFX平台中实现了两套图形加速流水线： Prism用于处理渲染工作。它可以在硬件和软件渲染器之上工作，包括3D。它负责将JavaFX场景进行光栅化和渲染。下面的各种渲染方式都有可能被用到： 在Windows XP和Vista上的DirectX 9 在Windows 7上的DirectX 11 Mac、Linux、嵌入式设备上的OpenGL 当硬件加速技术不支持时使用软件渲染，如果可能将会优先使用硬件加速，但是如果硬件加速不可用则会使用软件渲染，软件渲染技术已经内置于JRE之中。这点在展示3D场景时尤其重要。当然，使用硬件加速时性能将会更好。 Quantum Toolkit 将Prism和Glass Windowing ToolKit绑在一起，使得它们可以被其上层的JavaFX层使用。它也负责管理与渲染有关的事件处理的线程规则。 用法Hello World最简单的一个例子： 1import javafx.application.Application;2import javafx.scene.Group;3import javafx.scene.Scene;4import javafx.scene.paint.Color;5import javafx.stage.Stage;6 7public class Main extends Application &#123;8 9 @Override10 public void start(Stage stage) &#123;11 Group root = new Group();12 Scene scene = new Scene(root, 500, 500, Color.BLACK);13 stage.setTitle(\"JavaFX Scene Graph Demo\");14 stage.setScene(scene);15 stage.show();16 &#125;17 18 public static void main(String[] args) &#123;19 launch(args);20 &#125;21&#125; 参考资料JavaFX中文资料","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"},{"name":"JavaFX","slug":"Java/JavaFX","permalink":"http://beritra.github.com/categories/Java/JavaFX/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"JavaFX","slug":"JavaFX","permalink":"http://beritra.github.com/tags/JavaFX/"},{"name":"GUI","slug":"GUI","permalink":"http://beritra.github.com/tags/GUI/"}]},{"title":"Spring配置文件","slug":"Spring配置文件","date":"2017-04-07T08:36:42.000Z","updated":"2019-12-15T14:39:56.164Z","comments":true,"path":"2017/04/07/Spring配置文件/","link":"","permalink":"http://beritra.github.com/2017/04/07/Spring%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"被Spring配置文件搞得不胜其烦，干脆记下来，也不用到处找教程了。","text":"被Spring配置文件搞得不胜其烦，干脆记下来，也不用到处找教程了。 配置内容web.xml1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;2&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"3 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"4 xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\"5 version=\"3.1\"&gt;6 &lt;servlet&gt;7 &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt;8 &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;9 &lt;init-param&gt;10 &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;11 &lt;param-value&gt;WEB-INF/springmvc-servlet.xml&lt;/param-value&gt;12 &lt;/init-param&gt;13 &lt;/servlet&gt;14 &lt;servlet-mapping&gt;15 &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt;16 &lt;url-pattern&gt;/&lt;/url-pattern&gt;17 &lt;/servlet-mapping&gt;18&lt;/web-app&gt; springspringmvc然后是springmvc的配置： 1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;2&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"3 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"4 xmlns:context=\"http://www.springframework.org/schema/context\"5 xmlns:mvc=\"http://www.springframework.org/schema/mvc\"6 xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"&gt;7 &lt;context:component-scan base-package=\"index\"/&gt;8 &lt;mvc:default-servlet-handler/&gt;9 &lt;mvc:annotation-driven/&gt;10 &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" id=\"internalResourceViewResolver\"&gt;11 &lt;property name=\"prefix\" value=\"/WEB-INF/pages\"/&gt;12 &lt;property name=\"suffix\" value=\".jsp\"/&gt;13 &lt;/bean&gt;14&lt;/beans&gt; 详细解释web.xmlweb.xml文件元素schemaSun公司定义的，在根元素中，都必须标明web.xml是用的拿个文件模式。其他元素放在内部比如： 1&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"2 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"3 xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\"4 version=\"3.1\"&gt;5&lt;/web-app&gt; icon、display、disciption、分别用来描述web应用的图标、名称和描述，基本没用过… 1&lt;icon&gt;2 &lt;small-icon&gt;/images/app_small.gif&lt;/small-icon&gt;3 &lt;large-icon&gt;/images/app_large.gif&lt;/large-icon&gt;4&lt;/icon&gt;5&lt;display-name&gt;Tomcat Example&lt;/display-name&gt;6&lt;disciption&gt;Tomcat Example servlets and JSP pages.&lt;/disciption&gt; context-param 上下文参数声明应用范围内的初始化参数。向ServletContext提供键值对，即应用程序上下文信息。listener，filter等在初始化的时候会用到这些上下文中的信息。在servlet了里面可以通过getServletContext().getInitParameter(&quot;cibtext/param&quot;)得到 1&lt;context-param&gt;2 &lt;param-name&gt;ContextParameter&lt;/para-name&gt;3 &lt;param-value&gt;test&lt;/param-value&gt;4 &lt;description&gt;It is a test parameter.&lt;/description&gt;5&lt;/context-param&gt; filter 过滤器将一个名字和一个实现javaxs.servlet.Filter接口的类相关联通过Filter技术，对web服务器管理的所有web资源：例如Jsp, Servlet, 静态图片文件或静态 html 文件等进行拦截，从而实现一些特殊的功能。例如实现URL级别的权限访问控制、过滤敏感词汇、压缩响应信息等一些高级功能。使用Filter 的完整流程：Filter 对用户请求进行预处理，接着将请求交给Servlet 进行处理并生成响应，最后Filter 再对服务器响应进行后处理。 功能 在HttpServletRequest 到达 Servlet 之前，拦截客户的 HttpServletRequest 。 根据需要检查 HttpServletRequest ，也可以修改HttpServletRequest 头和数据。 在HttpServletResponse 到达客户端之前，拦截HttpServletResponse 。 根据需要检查 HttpServletResponse ，也可以修改HttpServletResponse头和数据。 filter链在一个web应用中，可以开发编写多个Filter，这些Filter组合起来称之为一个Filter链。web服务器根据Filter在web.xml文件中的注册顺序，决定先调用哪个Filter，当第一个Filter的doFilter方法被调用时，web服务器会创建一个代表Filter链的FilterChain对象传递给该方法。在doFilter方法中，开发人员如果调用了FilterChain对象的doFilter方法，则web服务器会检查FilterChain对象中是否还有filter，如果有，则调用第2个filter，如果没有，则调用目标资源。 生命周期web服务器负责创建和销毁，应用程序启动的时候创建实例，并且调用init方法创建：public void init(FilterConfig filterConfig) throws ServletException;//初始化执行：public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException;//拦截请求销毁：public void destroy();//销毁 FilterConfig接口编写filter的时候可以通过filterConfig对象的方法获取filter初始化参数。 1String getFilterName();//得到filter的名称。2String getInitParameter(String name);//返回在部署描述中指定名称的初始化参数的值。如果不存在返回null.3Enumeration getInitParameterNames();//返回过滤器的所有初始化参数的名字的枚举集合。4public ServletContext getServletContext();//返回Servlet上下文对象的引用。 1&lt;filter&gt;2 &lt;filter-name&gt;setCharacterEncoding&lt;/filter-name&gt;3 &lt;filter-class&gt;com.myTest.setCharacterEncodingFilter&lt;/filter-class&gt;4 &lt;init-param&gt;&lt;!--通过FilterConfig类的getInitParameter(\"paramName\") --&gt;5 &lt;param-name&gt;encoding&lt;/param-name&gt;6 &lt;param-value&gt;UTF-8&lt;/param-value&gt;7 &lt;/init-param&gt;8&lt;/filter&gt;9&lt;filter-mapping&gt;10 &lt;filter-name&gt;setCharacterEncoding&lt;/filter-name&gt;11 &lt;url-pattern&gt;/*&lt;/url-pattern&gt;12 &lt;servlet-name&gt;指定过滤器所拦截的Servlet名称。&lt;/servlet-name&gt;13 &lt;dispatcher&gt;指定过滤器所拦截的资源被 Servlet 容器调用的方式，可以是REQUEST,INCLUDE,FORWARD和ERROR之一，默认REQUEST。用户可以设置多个dispatcher子元素用来指定 Filter 对资源的多种调用方式进行拦截。&lt;/dispatcher&gt;14&lt;/filter-mapping&gt; listener 监听器监听器Listener就是在application,session,request三个对象创建、销毁或者往其中添加修改删除属性时自动执行代码的功能组件。Listener是Servlet的监听器，可以监听客户端的请求，服务端的操作等。 1&lt;listener&gt;2 &lt;listener-class&gt;com.listener.class&lt;/listener-class&gt;3&lt;/listener&gt; ServletContext监听ServletContextListerner接口: 用于对Servlet整个上下文进行监听(创建、销毁) 1public void contextInitialized(ServletContextEvent sce);//上下文初始化2public void contextDestroyed(ServletContextEvent sce);//上下文销毁3public ServletContext getServletContext();//ServletContextEvent事件：取得一个ServletContext（application）对象 ServletContextAttributeListener接口：对Servlet上下文属性的监听(增删改属性) 1public void attributeAdded(ServletContextAttributeEvent scab);//增加属性2public void attributeRemoved(ServletContextAttributeEvent scab);//属性删除3public void attributeRepalced(ServletContextAttributeEvent scab);//属性替换（第二次设置同一属性）4//ServletContextAttributeEvent事件：能取得设置属性的名称与内容5public String getName();//得到属性名称6public Object getValue();//取得属性的值 Session监听Session属于http协议下的内容，接口位于javax.servlet.http.*包下。HttpSessionListener接口：对Session的整体状态的监听。 1public void sessionCreated(HttpSessionEvent se);//session创建2public void sessionDestroyed(HttpSessionEvent se);//session销毁3//HttpSessionEvent事件：4public HttpSession getSession();//取得当前操作的session HttpSessionAttributeListener接口：对session的属性监听。 1public void attributeAdded(HttpSessionBindingEvent se);//增加属性2public void attributeRemoved(HttpSessionBindingEvent se);//删除属性3public void attributeReplaced(HttpSessionBindingEvent se);//替换属性4//HttpSessionBindingEvent事件：5public String getName();//取得属性的名称6public Object getValue();//取得属性的值7public HttpSession getSession();//取得当前的session request监听ServletRequestListener： 用于Request请求进行监听(创建、销毁) 1public void requestInitialized(ServletRequestEvent sre);//request初始化2public void requestDestroyed(ServletRequestEvent sre);//request销毁3//ServletRequestEvent事件：4public ServletRequest getServletRequest();//取得一个ServletRequest对象5public ServletContext getServletContext();//取得一个ServletContext（application）对象 ServletRequestAttributeListener: 对Request属性的监听(增删改属性) 1public void attributeAdded(ServletRequestAttributeEvent srae);//增加属性2public void attributeRemoved(ServletRequestAttributeEvent srae);//属性删除3public void attributeReplaced(ServletRequestAttributeEvent srae);//属性替换（第二次设置同一属性）4//ServletRequestAttributeEvent事件：能取得设置属性的名称与内容5public String getName();//得到属性名称6public Object getValue();//取得属性的值 servletservlet标签只要有以下子元素： servlet-name指定servlet的名称 servlet-class制定servlet的类名称 jsp-file制定某个jsp网页的完整路径 init-param用来定义参数，可以有多个init-param。在servlet类中通过getInitParamenter(String name)方法初始化参数 load-on-startup 指定当Web应用启动的时候，装载Servlet的次序,当值为正数或零时：Servlet容器先加载数值小的servlet，再依次加载其他数值大的servlet。当值为负或未定义：Servlet容器将在Web客户首次访问这个servlet时加载它。 servlet-mapping 用来定义servlet所对应的URL包含两个子元素 servlet-name 指定servlet的名称 url-parttern 指定servlet所对应的URL1&lt;!-- 基本配置 --&gt;2&lt;servlet&gt;3 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;4 &lt;servlet-class&gt;SnoopServlet&lt;/servlet-class&gt;5&lt;/servlet&gt;6&lt;servlet-mapping&gt;7 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;8 &lt;url-pattern&gt;/snoop&lt;/url-pattern&gt;9&lt;/servlet-mapping&gt;10&lt;!-- 高级配置 --&gt;11&lt;servlet&gt;12 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;13 &lt;servlet-class&gt;SnoopServlet&lt;/servlet-class&gt;14 &lt;init-param&gt;15 &lt;param-name&gt;foo&lt;/param-name&gt;16 &lt;param-value&gt;bar&lt;/param-value&gt;17 &lt;/init-param&gt;18 &lt;run-as&gt;19 &lt;description&gt;Security role for anonymous access&lt;/description&gt;20 &lt;role-name&gt;tomcat&lt;/role-name&gt;21 &lt;/run-as&gt;22&lt;/servlet&gt;23&lt;servlet-mapping&gt;24 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;25 &lt;url-pattern&gt;/snoop&lt;/url-pattern&gt;26&lt;/servlet-mapping&gt; session-config 会话超时配置单位是分钟 1&lt;session-config&gt;2 &lt;session-timeout&gt;120&lt;/session-timeout&gt;3&lt;/session-config&gt; mime-mapping、welcome-file-list、error-page错误页面1&lt;mime-mapping&gt;2 &lt;extension&gt;htm&lt;/extension&gt;3 &lt;mime-type&gt;text/html&lt;/mime-type&gt;4&lt;/mime-mapping&gt;5&lt;welcome-file-list&gt;6 &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;7 &lt;welcome-file&gt;index.html&lt;/welcome-file&gt;8 &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt;9&lt;/welcome-file-list&gt;10&lt;error-page&gt;11 &lt;error-code&gt;404&lt;/error-code&gt;12 &lt;location&gt;/NotFound.jsp&lt;/location&gt;13&lt;/error-page&gt;14&lt;error-page&gt;15 &lt;exception-type&gt;java.lang.NullException&lt;/exception-type&gt;16 &lt;location&gt;/error.jsp&lt;/location&gt;17&lt;/error-page&gt; jsp-config 设置jsp1&lt;jsp-config&gt;2 &lt;taglib&gt;3 &lt;taglib-uri&gt;Taglib&lt;/taglib-uri&gt;&lt;!--jstl标签配置--&gt;4 &lt;taglib-location&gt;/WEB-INF/tlds/MyTaglib.tld&lt;/taglib-location&gt;5 &lt;/taglib&gt;6 &lt;jsp-property-group&gt;7 &lt;description&gt;Special property group for JSP Configuration JSP example.&lt;/description&gt;&lt;!--设定的说明--&gt;8 &lt;display-name&gt;JSPConfiguration&lt;/display-name&gt;9 &lt;url-pattern&gt;/jsp/* &lt;/url-pattern&gt;10 &lt;el-ignored&gt;true&lt;/el-ignored&gt;11 &lt;page-encoding&gt;GB2312&lt;/page-encoding&gt;12 &lt;scripting-invalid&gt;true&lt;/scripting-invalid&gt;13 &lt;include-prelude&gt;/include/prelude.jspf&lt;/include-prelude&gt;14 &lt;include-coda&gt;/include/coda.jspf&lt;/include-coda&gt;15 &lt;/jsp-property-group&gt;16&lt;/jsp-config&gt; 容器加载web.xml和启动过程当要启动某个j2ee项目的时候，服务器软件或者容器会第一步加载项目中的web.xml文件，通过其中的各种配置来启动项目，只有其中配置各项均无误的时候，项目才正确启动。web.xml有多响标签，在加载过程中顺序依次为：context-param &gt; listener &gt; filter &gt; servlet web.xml先读取context-param和listen这两个节点； 然后容器创建一个ServletContext，应用于整个项目； 将读取到的context-param转化为键值对并且存入servletContext 根据listener创建监听 容器读取，根据指定的类路径来实例化过滤器 项目初始化完成 发起第一次请求的时候，servlet节点被加载实例化。 参考：web.xml文件详解、Web启动过程及web.xml配置、web.xml官方文档、Filter过滤器、Listener监听器、JSP标准标签库","categories":[{"name":"Spring","slug":"Spring","permalink":"http://beritra.github.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://beritra.github.com/tags/Spring/"},{"name":"framework","slug":"framework","permalink":"http://beritra.github.com/tags/framework/"}]},{"title":"Java关键字","slug":"Java关键字","date":"2017-04-05T13:33:59.000Z","updated":"2019-12-15T14:27:30.617Z","comments":true,"path":"2017/04/05/Java关键字/","link":"","permalink":"http://beritra.github.com/2017/04/05/Java%E5%85%B3%E9%94%AE%E5%AD%97/","excerpt":"Java里面的关键字和相关的知识点","text":"Java里面的关键字和相关的知识点 final关键字final可以声明成员变、方法、类和本地变量。一旦将引用声明为final，这个引用就不能改变了。 final变量凡是对成员变量或者本地变量(在方法中的或者代码块中的变量称为本地变量)声明为final的都叫作final变量。final变量经常和static关键字一起使用，作为常量。 final方法声明为final 的方法不能被子类的方法重写。final方法比非final方法要快，因为编译的时候已经静态绑定，不需要运行时再动态绑定。 final类final修饰的类不能被继承。Java中有String，Integer和其他包装类都是final类。 final关键字的好处 提高性能，JVM和Java应用都会缓存final变量。 可以在安全的多线程环境下进行共享，不需要额外的同步开销。 使用final关键字，JVM会对方法、变量和类进行优化。 不可变类不可变类是指对象一旦创建就不能更改，要使用final关键字创建。 为什么字符串类需要是不可变的？ 实现字符串池，节省heap空间。如果是可变的那么String interning就不能实现了，因为这样的话如果变量改变了他的值，那么这个值的变量的值也会一起改变。 安全问题：比如数据库的用户名密码都是字符串形式传入数据库的连接，可变的话就有风险被篡改。 多线程安全，字符串可以被多个线程共享。 类加载器要用到字符串。比如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。 创建的时候hashcode就被缓存了，不需要重新计算。这使得字符串很适合作为Map中的键，字符串的处理速度要快过其他的键对象。 如何写一个不可变类其他要点 final成员变量必须在声明的时候初始化或者在构造器中初始化，否则编译报错。 匿名类所有变量必须是final的。 接口中所有变量本身是final的/ final和abstract是冲突的，不能同时使用。 集合对象声明为final，引用不能被更改，但是不影响增删改查内部元素。 (来源:深入理解Java中的final关键字、为什么String类是不可变的)","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"API","slug":"API","permalink":"http://beritra.github.com/tags/API/"}]},{"title":"遇到的小问题合集","slug":"遇到的小问题合集","date":"2017-03-17T04:12:19.000Z","updated":"2019-12-15T13:24:11.791Z","comments":true,"path":"2017/03/17/遇到的小问题合集/","link":"","permalink":"http://beritra.github.com/2017/03/17/%E9%81%87%E5%88%B0%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/","excerpt":"记录平时遇到的小问题和踩过的坑，当做备忘。","text":"记录平时遇到的小问题和踩过的坑，当做备忘。 框架Spring MVC表单传值中文乱码在web.xml中增加拦截器： 1&lt;filter&gt;2 &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;3 &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;4 &lt;init-param&gt;5 &lt;param-name&gt;encoding&lt;/param-name&gt;6 &lt;param-value&gt;UTF-8&lt;/param-value&gt;7 &lt;/init-param&gt;8 &lt;/filter&gt;9 &lt;filter-mapping&gt;10 &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;11 &lt;url-pattern&gt;/*&lt;/url-pattern&gt;12 &lt;/filter-mapping&gt; Spring MVC返回JSON数据只需要添加依赖： 1&lt;dependency&gt;2 &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;3 &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;4 &lt;version&gt;2.8.8&lt;/version&gt;5&lt;/dependency&gt; 其实试了好几个解析JSON数据的包，但是就这个行了，没搞明白咋回事。 Linux系统Docker免sudo使用docker： 1$sudo addgroup --system docker2$sudo adduser $USER docker3$newgrp docker 在容器中创建虚拟IP的时候，有可能遇到错误： 1IPVS: Can&#39;t initialize ipvs: Protocol not available 开启的必要条件有两个： 容器放开权限，即添加参数--privileged 宿主机同样需要开启ipvasdm 在容器中使用Systemctl命令经常会遇到提示： 1systemctl start fdfs_trckerd2Failed to get D-Bus connection: Operation not permitted 解决办法，用特权模式启动容器： 1docker run -d -name centos7 --privileged=true centos:7 /usr/sbin/init ssh设置免密码ssh登陆的时候，把公钥放到authorized_keys里面，重启ssh service都不生效，百度得知home目录，.ssh目录和authorized_keys文件需要分别为700,700,600权限，错一个都不行，更改权限后可以免密登陆了。 apt-get经常遇到apt-get遇到问题卡住之后，出错的地方常年在那摆着，无论干啥都提示一遍，神烦。 1cd &#x2F;var&#x2F;lib&#x2F;dpkg2sudo mv info info.bak3sudo mkdir info info删掉就搞定了，原理有待深入研究 数据库Linux下的MySQL安装顺序1rpm -ivh mysql-community-common-5.7.17-1.el7.x86_64.rpm 2rpm -ivh mysql-community-libs-5.7.17-1.el7.x86_64.rpm 3rpm -ivh mysql-community-client-5.7.17-1.el7.x86_64.rpm 4rpm -ivh mysql-community-server-5.7.17-1.el7.x86_64.rpm 5rpm -ivh mysql-community-devel-5.7.17-1.el7.x86_64.rpm 安装最后一个的时候有可能出现依赖openssl的问题，重新安装openssl无效，输入参数-e --nodeps可以解决，暂不清楚原理 MySQL 8修改用户密码命令1ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;; 如果提示ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 说明密码太简单，经过测试大小写英文字母+数字+标点符号可以满足要求。 IDEA和java中文字符问题运行Tomcat的时候，Messages Build出现了错误提示：然后中文因为字体的关系不能显示，这里由于实在是喜欢这个字体，不想换成很丑的中文，而且JVM的错误提示还是英文搜索起来比较方便，所以想换成中文。然后发现在终端输入Java命令得到的是中文：这两个应该是相关的。然后修改系统字体：发现可以显示成英文，但是目的是只是Java为英文，所以不想这样。尝试过的方法： 在Tomcat的VM option里面添加-Duser.language=en -Duser.country=US 在Setting-&gt;BUild,Execution,Deployment-&gt;Compiler中的两个地方添加-Duser.language=en -Duser.country=US 在IDEA的安装路径/bin/idea.vmoptions中添加-Duser.language=en -Duser.country=US 最后发现上面那个配置文件是32位程序的，64位程序不用这个….点击 Help-&gt;Edit Custom VM Option 会生成一个 64 位的配置文件，然后再添加-D 巴拉拉巴拉就行了….害得我V2EX号还被封了….. JavaFX窗口闪烁问题用JavaFX绘制窗口的时候，由于Scene填充成了比较深的颜色，所以每次窗口初始化和大小变化的时候，背景都会出现白色闪烁： 代码如下： 1public class Panel extends Application &#123;2 private Group root;3 private Scene scene;4 private static final Color BACKGROUND_COLOR = Color.rgb(0, 0, 0, 0.5);5 private Stage stage;67 @Override8 public void start(Stage primaryStage) throws Exception &#123;9 root = new Group();10 stage = primaryStage;11 primaryStage.initStyle(StageStyle.TRANSPARENT);12 scene = new Scene(root, 1000, 250, BACKGROUND_COLOR);13 Screen screen = Screen.getScreens().get(0);14 double windowHeight = screen.getBounds().getHeight();15 double windowWidth = screen.getBounds().getWidth();16 stage.setY(windowHeight + 1000);17 stage.setX(windowWidth - 1000);18 stage.setScene(scene);19 stage.show();20 Timer timer = new Timer();21 final int[] index = &#123;0&#125;;22 timer.schedule(new TimerTask() &#123;23 @Override24 public void run() &#123;25 System.out.println(\"111\");26 index[0]++;27 primaryStage.setY(primaryStage.getY() - 1);28 primaryStage.setHeight(primaryStage.getHeight() + 1);29 if (index[0] == 300) &#123;30 this.cancel();31 &#125;3233 &#125;34 &#125;, 1000, 1);35 &#125;36&#125; 浪费了大概一天的时间…查找各种文档，教学，试了好多方法还是不行。一开始以为我填充的区域不对，不是最底层的，后来以为是填充背景色时间不对，跟底层绘制时间差距太大导致出现闪烁，但是不能解决。最后还是在万能的Google上找到了一个向jdk开发人员反馈的帖子：White flashing when opening a stage with dark background原来是个Bug…窗口初始化的时候默认填充了白色。第一次碰到jdk级别的Bug还有点小激动。跟着帖子看到的修复信息：changeset 10131:bfe35c702696 jdk-9+145解决方法是取得了子节点的颜色再填充的，修复的版本是9…所以只好下了个预览版的jdk9最后要记住这个网站：https://bugs.openjdk.java.net，可以向jdk的开发组反馈遇到的问题～ 修改Maven中央仓库.m2文件夹下新建文件settings.xml 1&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"2 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"3 xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.04 https://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt;5&lt;mirrors&gt;6 &lt;mirror&gt;7 &lt;id&gt;alimaven&lt;/id&gt;8 &lt;name&gt;aliyun maven&lt;/name&gt;9 &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;10 &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;11 &lt;/mirror&gt;12&lt;/mirrors&gt;13&lt;/settings&gt; 其他设置参考官方文档 其他工具Hexo的git问题写完博客，上传到服务器的时候，出现了错误： 1FATAL Permission denied (publickey).2fatal: Could not read from remote repository.34Please make sure you have the correct access rights5and the repository exists.67Error: Permission denied (publickey).8fatal: Could not read from remote repository.910Please make sure you have the correct access rights11and the repository exists.1213 at ChildProcess.&lt;anonymous&gt; (&#x2F;home&#x2F;ray&#x2F;文档&#x2F;笔记&#x2F;Blogs&#x2F;node_modules&#x2F;hexo-util&#x2F;lib&#x2F;spawn.js文档&#x2F;笔记&#x2F;Blogs&#x2F;node_modules&#x2F;hexo-util&#x2F;lib&#x2F;spawn.js:37:17)14 at emitTwo (events.js:87:13)15 at ChildProcess.emit (events.js:172:7)16 at maybeClose (internal&#x2F;child_process.js:821:16)17 at Socket.&lt;anonymous&gt; (internal&#x2F;child_process.js:319:11)18 at emitOne (events.js:77:13)19 at Socket.emit (events.js:169:7)20 at Pipe._onclose (net.js:469:12) 试了试github能登上，也能push，把hexo的配置文件里面关于git的配置，ssh改成了https,就能成功上传代码了，原因不明。每次都得输入账号密码，这也不是办法。","categories":[{"name":"问题","slug":"问题","permalink":"http://beritra.github.com/categories/%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"问题","slug":"问题","permalink":"http://beritra.github.com/tags/%E9%97%AE%E9%A2%98/"}]},{"title":"字符串匹配算法","slug":"字符串匹配算法","date":"2017-03-07T05:45:22.000Z","updated":"2019-12-15T13:17:57.414Z","comments":true,"path":"2017/03/07/字符串匹配算法/","link":"","permalink":"http://beritra.github.com/2017/03/07/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/","excerpt":"刷题的时候遇到的几个与字符串匹配相关的算法。","text":"刷题的时候遇到的几个与字符串匹配相关的算法。 搜索子字符串KMP首先是比较出名的KMP算法 Sunday算法主要思想：匹配失败的时候关注的是参与匹配的最末位字符的下一个字符。这个字符如果没有出现在下面字符串里，就直接向右移动字符串长度+1位。如果出现，就移动字符串长度-出现位置位。比如，要在substring searching algorithm中搜索algorithm，先从头匹配：显然不符合，然后直接跳到最末位的下一个，空格没有出现在下面字符串里不符合，接着找下一个g出现了，在下标为2的位置，所以右移9-2=7位显然还是不行，然后再去找下一个，i出现了，在下标5的位置，所以右移4个经验证符合要求。java代码如下： 1public int searchStr(String patternString, String str) &#123;2 if (patternString.equals(\"\") || str.equals(\"\")||str.length()&gt;patternString.length())3 return -1;4 char[] pattern = patternString.toCharArray();5 char[] s = str.toCharArray();6 int strIndex;7 for (int i = 0; i &lt; pattern.length;) &#123;8 strIndex = 0;9 while (strIndex &lt; s.length) &#123;10 if(pattern[i + strIndex] != s[strIndex]) &#123;11 i+=strIndex;12 break;13 &#125;14 if(strIndex==s.length-1)&#123;15 return i;16 &#125;17 strIndex++;18 &#125;19 if (i + 1 &lt; pattern.length) &#123;20 int charIndex=findChar(s,pattern[i+s.length]);21 if(charIndex==-1)&#123;22 i+=s.length+1;23 &#125;else&#123;24 i+=s.length-charIndex;25 &#125;26 &#125; else &#123;27 return -1;28 &#125;29 &#125;30 return -1;31 &#125;32 private int findChar(char[] c,char search)&#123;33 for(int i=0;i&lt;c.length;i++)&#123;34 if(c[i]==search)35 return i;36 &#125;37 return -1;38 &#125; 最长回文子串暴力求解三层循环，外层遍历字符串，中层遍历子串，三层判定是否回文 中心扩散法先循环找到相邻且相等的，或者间隔一个且相等两个字符，然后向外扩展找到最长时间复杂度n² 1public String longestPalindrome(String s) &#123;2 // Write your code here3 if (s.equals(\"\") || s.length() == 1)4 return s;5 char[] result = &#123;&#125;;6 char[] charArray = s.toCharArray();7 for (int i = 0; i &lt; s.length(); i++) &#123;8 if (i + 1 &lt; s.length() &amp;&amp; charArray[i] == charArray[i + 1]) &#123;9 result = result.length &lt;= 2 ? Arrays.copyOfRange(charArray, i, i + 2) : result;10 for (int j = 0; j &lt;= s.length() / 2; j++) &#123;11 if (i - j+1 &gt; 0 &amp;&amp; i + j + 1 &lt; s.length() &amp;&amp; charArray[i - j] == charArray[i + j + 1]) &#123;12 result = result.length &lt; (2 * j + 2) ? Arrays.copyOfRange(charArray, i - j, i + j + 2) : result;13 &#125; else &#123;14 break;15 &#125;16 &#125;17 &#125;18 if (i + 2 &lt; s.length() &amp;&amp; charArray[i] == charArray[i + 2]) &#123;19 result = result.length &lt;= 3 ? Arrays.copyOfRange(charArray, i, i + 3) : result;20 for (int j = 0; j &lt;= s.length() / 2; j++) &#123;21 if (i - j+1 &gt; 0 &amp;&amp; i + j + 2 &lt; s.length() &amp;&amp; charArray[i - j] == charArray[i + j + 2]) &#123;22 result = result.length &lt; (2 * j + 3) ? Arrays.copyOfRange(charArray, i - j, i + j + 3) : result;23 &#125; else &#123;24 break;25 &#125;26 &#125;27 &#125;28 &#125;29 return new String(result);30 &#125; Manacher算法先对字符串预处理，每个字符中间添加特殊符号比如#然后引入数组P[i]代表以字符S[i]为中心的最长回文子串向左/右扩张的长度（包括S[i]）S[]ghaweuifhwe123454321fahwu-&gt;S[]#g#h#a#w#e#u#i#f#h#w#e#1#2#1#4#4#1#2#1#f#a#h#w#u#P[] 1212121212121212121212121412129212141212121212121可以看出P[i]-1就是原字符串中回文子串的长度，比如12144241中对应的9-1，121对应4-1，所以需要求P[]最大值","categories":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"字符串","slug":"字符串","permalink":"http://beritra.github.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}]},{"title":"LintCode记录","slug":"LintCode记录","date":"2017-03-01T15:45:17.000Z","updated":"2019-12-15T14:27:22.264Z","comments":true,"path":"2017/03/01/LintCode记录/","link":"","permalink":"http://beritra.github.com/2017/03/01/LintCode%E8%AE%B0%E5%BD%95/","excerpt":"记录刷的那些麻烦的题","text":"记录刷的那些麻烦的题 二叉树的所有路径：1/**2 * Definition of TreeNode:3 * public class TreeNode &#123;4 * public int val;5 * public TreeNode left, right;6 * public TreeNode(int val) &#123;7 * this.val = val;8 * this.left = this.right = null;9 * &#125;10 * &#125;11 */12//递归解法：13public List binaryTreePaths(TreeNode root) &#123;14 List&lt;String&gt; leftList=new ArrayList&lt;&gt;();15 if(root==null)16 return leftList;17 if(root.left!=null)&#123;18 for(String list:binaryTreePaths(root.left))19 leftList.add(root.val+\"-&gt;\"+list);20 &#125;21 if(root.right!=null)&#123;22 for(String list:binaryTreePaths(root.right))23 leftList.add(root.val+\"-&gt;\"+list);24 &#125;25 if(root.right==null&amp;&amp;root.left==null)&#123;26 leftList.add(String.valueOf(root.val));27 &#125;28 return leftList;29&#125;3031//非递归解法：32public class Solution &#123;33 public List binaryTreePaths(TreeNode root) &#123;34 List list=new ArrayList&lt;&gt;();35 if(root==null)36 return list;37 boolean down=true;38 Stack stack=new Stack&lt;&gt;();39 Stack result=new Stack&lt;&gt;();40 TreeNode test=root;41 while(true)&#123;42 if(test.left!=null&amp;&amp;test.right!=null)&#123;43 if(down)&#123;44 if(test==root)&#123;45 result.push(String.valueOf(test.val));46 result.push(String.valueOf(test.val));47 &#125;48 else&#123;49 String str=result.pop();50 result.push(str+\"-&gt;\"+String.valueOf(test.val));51 result.push(str+\"-&gt;\"+String.valueOf(test.val));52 &#125;53 stack.push(test);54 test=test.left;55 down=true;56 &#125;else&#123;57 result.push(String.valueOf(result.pop()));58 test=test.right;59 down=true;60 &#125;61 &#125;else if(test.left!=null)&#123;62 if(test==root)&#123;63 result.push(String.valueOf(test.val));64 &#125;else65 result.push(result.pop()+\"-&gt;\"+String.valueOf(test.val));66 test=test.left;67 &#125;else if(test.right!=null)&#123;68 if(test==root)&#123;69 result.push(String.valueOf(test.val));70 &#125;else71 result.push(result.pop()+\"-&gt;\"+String.valueOf(test.val));72 test=test.right;73 &#125;else&#123;74 if(test==root)&#123;75 list.add(String.valueOf(test.val));76 &#125;else&#123;77 list.add(result.pop()+\"-&gt;\"+String.valueOf(test.val));78 &#125;79 if(stack.empty())80 break;81 test=stack.pop();82 down=false;83 &#125;84 &#125;85 return list;86 &#125;87&#125; 动态规划：上楼问题。一次一个或者两个台阶，上N层楼。1// 递归：消耗太大，简直变态2public class Solution &#123;3 /**4 * @param n: An integer5 * @return: An integer6 */7 public int climbStairs(int n) &#123;8 if(n==0)&#123;9 return 0;10 &#125;else if(n==1)&#123;11 return 1;12 &#125;else if(n==2)&#123;13 return 2;14 &#125;else&#123;15 // write your code here16 return climbStairs(n-1)+climbStairs(n-2);17 &#125;18 &#125;19&#125;2021// 非递归解法：22public class Solution &#123;23 /**24 * @param n: An integer25 * @return: An integer26 */27 public int climbStairs(int n) &#123;28 if(n==0)&#123;29 return 1;30 &#125;else if(n==1)&#123;31 return 1;32 &#125;else if(n==2)&#123;return 2;&#125;else&#123;33 int last=1;34 int swap;35 int result=2;36 for(int i=0;i&lt;n-2;i++)&#123;37 swap=result;38 result+=last;39 last=swap;40 &#125;41 return result;42 &#125;43 &#125;44&#125; 非递归的二叉树后序遍历应该还有更简洁的方式 1/**2 * Definition of TreeNode:3 * public class TreeNode &#123;4 * public int val;5 * public TreeNode left, right;6 * public TreeNode(int val) &#123;7 * this.val = val;8 * this.left = this.right = null;9 * &#125;10 * &#125;11 */12public class Solution &#123;13 /**14 * @param root: The root of binary tree.15 * @return: Postorder in ArrayList which contains node values.16 */17 public ArrayList&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;18 ArrayList list=new ArrayList();19 TreeNode node=root;20 if(root==null)21 return list;22 Stack&lt;TreeNode&gt; stack=new Stack&lt;&gt;();23 boolean down=true;24 Stack&lt;TreeNode&gt; last=new Stack&lt;&gt;();25 last.push(new TreeNode(0));26 while(true)&#123;27 if(node.left!=null&amp;&amp;node.right!=null)&#123;28 if(down)&#123;29 stack.push(node);30 stack.push(node);31 node=node.left;32 down=true;33 &#125;else&#123;34 if(node==last.peek())&#123;35 list.add(node.val);36 if(stack.empty())37 break;38 node=stack.pop();39 last.pop();40 &#125;else&#123;41 last.push(node);42 node=node.right;43 down=true;44 &#125;45 &#125;46 &#125;47 else if(node.left!=null)&#123;48 if(down)&#123;49 stack.push(node);50 node=node.left;down=true;51 &#125;else&#123;52 list.add(node.val);53 if(stack.empty())54 break;55 node=stack.pop();56 &#125;57 &#125;else if(node.right!=null)&#123;58 if(down)&#123;59 stack.push(node);60 node=node.right;down=true;61 &#125;else&#123;62 list.add(node.val);63 if(stack.empty())64 break;65 node=stack.pop();66 &#125;67 &#125;else&#123;68 list.add(node.val);69 if(stack.empty())70 break;71 node=stack.pop();72 down=false;73 &#125;74 &#125;75 return list;76 &#125;77&#125; 二叉树的复制 非递归1public class Solution &#123;2 /**3 * @param root: The root of binary tree4 * @return root of new tree5 */6 public TreeNode cloneTree(TreeNode root) &#123;7 if(root==null)8 return null;9 TreeNode another=new TreeNode(root.val);10 TreeNode test=root;11 TreeNode test2=another;12 boolean down=true;13 Stack&lt;TreeNode&gt; stack1=new Stack&lt;&gt;();14 Stack&lt;TreeNode&gt; stack2=new Stack&lt;&gt;();15 while(true)&#123;16 if(test.left!=null&amp;&amp;test.right!=null)&#123;17 if(down)&#123;18 stack1.push(test);19 stack2.push(test2);20 test2.left=new TreeNode(test.left.val);21 test=test.left;22 test2=test2.left;23 continue;24 &#125;else&#123;25 test2.right=new TreeNode(test.right.val);26 test=test.right;27 test2=test2.right;28 down=true;29 continue;30 &#125;31 &#125;32 else if(test.left!=null)&#123;33 test2.left=new TreeNode(test.left.val);34 test=test.left;35 test2=test2.left;36 &#125;else if(test.right!=null)&#123;37 test2.right=new TreeNode(test.right.val);38 test=test.right;39 test2=test2.right;40 &#125;else&#123;41 if(stack1.empty())42 break;43 test=stack1.pop();44 test2=stack2.pop();45 down=false;46 &#125;47 &#125;48 return another;49 &#125;50&#125; 二分查找本来是很简单的问题，但是不知道position为啥要减一，回头再看. 1public int binarySearch(int[] nums, int target) &#123;2 if (nums == null)3 return -1;4 int position = nums.length;5 int[] temp = nums;67 while (true) &#123;8 int len = temp.length;9 if (len == 2) &#123;10 if (temp[0] == target)11 return position - 2;12 else if(temp[1] == target)13 return position-1;14 else15 return -1;16 &#125; else if (len == 1) &#123;17 if(temp[0]==target)18 return position-1;19 else20 return -1;21 &#125;22 len = len / 2;23 if (temp[len] &gt;= target) &#123;24 position = position-temp.length+len+1;25 temp = Arrays.copyOfRange(temp, 0, len + 1);26 &#125; else &#123;27 temp = Arrays.copyOfRange(temp, len + 1, temp.length);28 &#125;29 &#125; 二叉树的层次遍历一层一层的输出二叉树： 基本解法 1public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;2 // write your code here3 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result=new ArrayList&lt;&gt;();4 ArrayList&lt;Integer&gt; list=new ArrayList&lt;&gt;();5 if(root==null)6 return result;7 TreeNode currentNode=root;8 Queue queue=new LinkedList();9 int index=0;10 int index2=0;11 while(true)&#123;12 list.add(currentNode.val);13 if(currentNode.left!=null)&#123;14 queue.add(currentNode.left);15 index2++;16 &#125;17 if(currentNode.right!=null)&#123;18 queue.add(currentNode.right);19 index2++;20 &#125;21 if(index==0)&#123;22 index=index2;23 index2=0;24 result.add(new ArrayList(list));25 list.clear();26 if(queue.isEmpty())27 break;28 &#125;29 currentNode=(TreeNode)queue.remove();30 index--;31 &#125;32 return result;33 &#125; 二叉树的路径和我觉得我的智商有问题… 1public List&lt;List&lt;Integer&gt;&gt; binaryTreePathSum(TreeNode root, int target) &#123;2 TreeNode node = root;3 List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;();4 if (root == null)5 return list;6 if (target == node.val) &#123;7 if (node.left == null &amp;&amp; node.right == null) &#123;8 List l = new ArrayList();9 l.add(node.val);10 list.add(l);11 return list;12 &#125;13 &#125; else if (target &lt; node.val&amp;&amp;node.left == null &amp;&amp; node.right == null) &#123;14 return null;15 &#125;16 if (node.left != null) &#123;17 List l = binaryTreePathSum(node.left, target - node.val);18 if (l != null)19 for (int i = 0; i &lt; l.size(); i++) &#123;20 List ll = (List) l.get(i);21 List lll = new ArrayList();22 lll.add(node.val);23 lll.addAll(ll);24 list.add(lll);25 &#125;26 &#125;27 if (node.right != null) &#123;28 List l = binaryTreePathSum(node.right, target - node.val);29 if (l != null)30 for (int i = 0; i &lt; l.size(); i++) &#123;31 List ll = (List) l.get(i);32 List lll = new ArrayList();33 lll.add(node.val);34 lll.addAll(ll);35 list.add(lll);36 &#125;3738 &#125;39 return list;40 &#125; 几个出错了的点： 没考虑负数 new ArrayList(i)的参数i不是初始值，是initCapacity初始容量 二叉查找树的删除元素1public TreeNode removeNode(TreeNode root, int value) &#123;2 // write your code here3 if(root==null)4 return null;5 if(root.val==value)&#123;6 if(root.left!=null)&#123;7 TreeNode retuNode=root;8 root=root.left;9 TreeNode max=root;10 while(max.right!=null)&#123;11 max=max.right;12 &#125;13 max.right=retuNode.right;14 return root;15 &#125;else if(root.right!=null)&#123;16 root=root.right;17 return root;18 &#125;19 return null;20 &#125;else if(value&lt;root.val)&#123;21 root.left=removeNode(root.left,value);22 return root;23 &#125;else if(value&gt;root.val)&#123;24 root.right=removeNode(root.right,value);25 return root;26 &#125;27 return null;28 &#125; 最长无重复字符的子串这个简单，不过就是不知道使用了hashmap的话,时间复杂度应该怎么算 1public int lengthOfLongestSubstring(String s) &#123;2 // write your code here3 if (s.equals(\"\")) &#123;4 return 0;5 &#125;6 char[] c = s.toCharArray();7 int start = 0;8 int end = 0;9 int length = 1;10 HashSet set = new HashSet();11 while (end &lt; c.length) &#123;12 boolean added = set.add(c[end]);13 length = set.size() &gt; length ? set.size() : length;1415 if (!added) &#123;16 while (c[start] != c[end]) &#123;17 set.remove(c[start]);18 start++;19 &#125;20 start++;21 &#125;22 end++;23 &#125;24 return length;25 &#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"计算机基础","slug":"计算机基础","permalink":"http://beritra.github.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"数据结构","slug":"数据结构","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"奇奇怪怪的面试题记录","slug":"奇奇怪怪的面试题记录","date":"2017-02-26T17:58:51.000Z","updated":"2019-12-15T13:21:44.299Z","comments":true,"path":"2017/02/27/奇奇怪怪的面试题记录/","link":"","permalink":"http://beritra.github.com/2017/02/27/%E5%A5%87%E5%A5%87%E6%80%AA%E6%80%AA%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"好几次碰到已经刷过的奇怪的题，还是想不起来，所以还是好记性不如烂笔头，都记下来面试之前看一遍好了。","text":"好几次碰到已经刷过的奇怪的题，还是想不起来，所以还是好记性不如烂笔头，都记下来面试之前看一遍好了。 语言特性不借助中间变量，交换字符串要用到三次亦或： 1int a=5555;2int b=6666;3a^=b;4b^=a;5a^=b;6System.out.println(a+\" \"+b); Java不能字符串的亦或操作，用数字表示了，好像c可以？？ Java类型转换今天犯了个错，char转int直接强转，然后得到了Unicode。可以Integer.parseInt(&#39;x&#39;+&quot;&quot;)转成string再转换或者Character的静态方法:Character.getNumericValue((int)&#39;5&#39;) 今天发现个好玩的事情，测试一个回文字符串算法，重复十亿次看时间，发现二十个字符和十个字符好时差别不大，四十个和二十个却有十倍的差距，我猜应该是虚拟机对段字符串有优化，留个坑改天详细研究。 Java语言基础final关键字static关键字静态变量同一个类在内存中只有同一个拷贝，可以用类名直接访问，也可以通过对象访问（不推荐） 静态方法可以通过类名调用，任何势力都可以调用，所以静态方法中不能有this和super关键字。静态方法必须被实现，不能是abstract。 静态代码块JVM加载类的时候按照出现的先后顺序执行，每个代码块只被执行一次。 静态内部类 一个内部类如果想要有静态的成员变量或者成员方法，那他本身必须是static关键字修饰的。 静态内部类只能访问外部的静态方法和变量，非静态的不能访问，这是静态内部类最大的使用限制，普通的非静态内部类没有这个限制。静态导包导入静态方法，简化操作，比如：1import static java.lang.System.out2out.println(\"text\"); 算法","categories":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"用github.pages和Hexo搭建个人博客","slug":"Article","date":"2017-02-24T13:26:06.000Z","updated":"2019-12-15T14:28:38.521Z","comments":true,"path":"2017/02/24/Article/","link":"","permalink":"http://beritra.github.com/2017/02/24/Article/","excerpt":"最近深感进步速度太慢，希望能养成写博客的习惯，不断总结学习。之前都是想在Atom上本地写写，后来用手机或者在别的地方看起来不方便，于是决定自己用比较现成的github.pages+Hexo解决，顺便玩玩github。","text":"最近深感进步速度太慢，希望能养成写博客的习惯，不断总结学习。之前都是想在Atom上本地写写，后来用手机或者在别的地方看起来不方便，于是决定自己用比较现成的github.pages+Hexo解决，顺便玩玩github。 基本的搭建过程网上教程一大把，不在赘述，也没啥技术含量，主要记录个人遇到的几个坑 Repository name 一定要是 username.github.id 生成ssh-key的时候，因为之前gitlab上用了一套，然后开始以为不能用同一个，就找了条命令：ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot; -f ~/.ssh/github(后来发现默认的命令也会让你填写保存的文件名称)，然后发现没卵用…仍然是permission denied 然后看到人说是root用户的问题，看了下自己用的，果然是root用户，马上切回到普通用户，生成一遍，还是不行… 再回头翻了翻github的帮助，发现可以不用生成新的key： 1、Start the ssh-agent in the background.$ eval “$(ssh-agent -s)”$ Agent pid 595662、Add your SSH key to the ssh-agent. If you are using an existing SSH key rather than generating a new SSH key, you’ll need to replace id_rsa in the command with the name of your existing private key file.$ ssh-add ~/.ssh/id_rsa 终于搞定基本的连接，开始搭Hexo，deployer的时候,出现了错误:error deployer not found:git在v2ex上找到了解决办法：搭建 hexo，在执行 hexo deploy 后,出现 error deployer not found:github 的错误。 npm install hexo-deployer-git –save 改了之后执行，然后再部署试试 顺便安利下这个v2ex还是个挺不错的程序员社区 搞了大半夜折腾了个Next.mist主题…我发誓再也不做美化界面的事…","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://beritra.github.com/categories/Hexo/"},{"name":"Blog","slug":"Hexo/Blog","permalink":"http://beritra.github.com/categories/Hexo/Blog/"}],"tags":[{"name":"Github","slug":"Github","permalink":"http://beritra.github.com/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"http://beritra.github.com/tags/Blog/"},{"name":"Hexo","slug":"Hexo","permalink":"http://beritra.github.com/tags/Hexo/"}]}]}