{"meta":{"title":"Beritra","subtitle":"Blogs","description":"Beritra's Blogs","author":"Beritra","url":"http://Beritra.github.com","root":"/"},"pages":[{"title":"关于","date":"2017-02-24T16:28:26.000Z","updated":"2022-05-10T15:55:29.248Z","comments":true,"path":"about/index.html","permalink":"http://beritra.github.com/about/index.html","excerpt":"","text":"自我介绍我2015年毕业于山东大学，随后一直在北京工作和生活。 目前就职于北京的互联网公司，主要负责某中台的后端研发工作，主要使用语言是Java，另外会一点Python和JS。 关于博客写博客的目的是总结知识，建立自己的知识体系，也方便及时复习。另外也有一部分读书笔记和个人感想与大家分享。希望能借写博客督促自己不断学习进步。 博客内容主要关注我平时遇到的问题，工作中涉及到的框架、组件的使用方式和原理。包括Java常见的技术栈、Linux和一些架构知识。 联系方式Gmail：luckyrayyy@gmail.com"},{"title":"分类","date":"2017-02-24T16:28:56.000Z","updated":"2019-12-14T21:39:54.400Z","comments":false,"path":"tags/index.html","permalink":"http://beritra.github.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-12-14T21:37:39.000Z","updated":"2019-12-15T14:42:02.136Z","comments":true,"path":"categories/index.html","permalink":"http://beritra.github.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Vertx和响应式编程","slug":"Vertx和响应式编程","date":"2021-08-09T15:00:58.000Z","updated":"2021-08-11T14:17:06.131Z","comments":true,"path":"2021/08/09/Vertx和响应式编程/","link":"","permalink":"http://beritra.github.com/2021/08/09/Vertx%E5%92%8C%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BC%96%E7%A8%8B/","excerpt":"今天了解下听说了好久，但是没怎么用过的东西。","text":"今天了解下听说了好久，但是没怎么用过的东西。 响应式介绍可能有的同学对 Vertx 和响应式还不熟悉，按照惯例，先上定义。 响应式编程（Reactive Programming）： 在计算中，响应式编程或反应式编程（英语：Reactive programming）是一种面向数据流和变化传播的声明式编程范式。这意味着可以在编程语言中很方便地表达静态或动态的数据流，而相关的计算模型会自动将变化的值通过数据流进行传播。 好，估计还是不懂，Wiki 上的定义一向晦涩难懂。 ”响应式“这个概念其实是编程范式（Programming paradigm）里的“声明式编程”的子类，我们不妨对比一下，才能更好地理解什么才算是响应式编程。 过程化/命令式编程：命令式编程的主要思想是关注计算机执行的步骤， 即一步一步告诉计算机先做什么再做什么。 声明式编程：声明式编程是以数据结构的形式来表达程序执行的逻辑。 它的主要思想是告诉计算机应该做什么，但不指定具体要怎么做。 比如我们想在一个数字集合中筛选数字大于 6 的，对于命令式，可能是这么做： 1for(int num=0;i&lt;list.size();i++)2&#123;3 if (num &gt; 6)&#123;4 return num;5 &#125;6&#125; 而声明式编程中最典型的就是 SQL，我们想筛选同样的数据，对于 SQL 来说可能是下面这样： 1SELECT num FROM list where num &gt; 6 limit 1; 除了 SQL，网页编程中用到的 HTML 和 CSS 也都属于声明式编程。 命令式编程和声明式编程起源的不同决定这两大类范式代表着迥然不同的编程理念和风格：命令式编程是行动导向（ Action-Oriented ）的，因而算法是显性而目标是隐性的；声明式编程是目标驱动（ Goal-Driven ）的，因而目标是显性而算法是隐性的。 明确什么是“声明式”之后，我们在看什么叫”面向数据流(data stream)和变化传播(propagation of change)“。 与传统的处理方式相比，响应式能够基于数据流中的事件进行反应处理。举个例子：a+b=c的场景，在传统编程方式下如果 a、b 发生变化，那么我们需要重新计算 a+b 来得到 c 的新值。而反应式编程中，我们不需要重新计算，a、b 的变化事件会触发 c 的值自动更新。这种方式类似于我们在消息中间件中常见的发布/订阅模式或者观察者模式。由流发布事件，而我们的代码逻辑作为订阅方基于事件进行处理，并且是异步处理的。 观察者模式，或者可以叫发布-订阅模式，定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。 反应式编程中，最基本的处理单元是事件流(事件流是不可变的，对流进行操作只会返回新的流)中的事件。流中的事件包括正常事件(对象代表的数据、数据流结束标识)和异常事件(异常对象，例如 Exception)。同时，只有当订阅者第一次发布者，发布者发布的事件流才会被消费，后续的订阅者只能从订阅点开始消费，但是我们可以通过背压、流控等方式控制消费。 常用的反应式编程实现类库包括：Reactor、RxJava 2、Akka Streams、Vert.x 以及 Ratpack 等等。 Vert.x 一个基于 JVM、轻量级、高性能的应用平台。 介绍相信任何一个程序猿，对”高性能“的追求永远都不会停歇。Vertx 既然自称高性能，那么有多高的性能？参见下面这个 Benchmark。 Web Framework Benchmarks 这个测试非常全，基本覆盖了常见语言的常见框架，并且分为多种场景，每隔半年左右重新测试一遍更新数据。可以看到 Vertx 在 JVM 语言里面性能排名还是非常靠前的。但是前几名还是还是 C++、Rust 的天下。 作为对比，Spring 大概在这个位置： 当然，Vertx 不是一个大而全的框架，和 Spring 的定位不一样也不好直接对比。 Vertx 有如下几个特性: 轻量级，核心包大小只有 650kB。 速度快，基于全异步的 Netty，Netty 性能很强大家应该都了解，Vertx 基于 Netty 做了很多封装，保持高性能的同时增加了易用性。 同时支持多种编程语言，比如 Java，JavaScript，Groovy，Ruby，Ceylon，Scala 和 Kotlin。 极好的分布式开发支持。 模块化。 不讲这些比较虚的，我们直接来看下 Vertx 的理念和玩法。 官方文档的 Introduction 其实就有比较详细的介绍，我在根据我的理解再复述补充一遍： 最开始的多线程解决并发问题最经典的用法就是用多线程，多个线程可以存在于单个进程中，执行并发工作，并共享相同的内存空间。 大多数应用程序和服务开发框架都是基于这种多线程模式。表面上看，每个连接有一个线程的模式是很让人放心，因为开发人员可以依靠传统的命令式代码。但是实际上你还是要自己去规避在内存访问方面的风险。 多线程简单但是受限当工作负荷增长到超过中等工作负荷时会发生什么？(比如C10k问题) 答案很简单：你的操作系统内核开始受到影响，每次请求期间就有太多的上下文切换。 你的一些线程会被阻塞，因为它们在等待 I/O 操作的完成，一些线程会准备处理 I/O 结果，一些线程会在做 CPU 密集型任务。 现代内核有非常好的调度器，但你不能指望它们像处理 5000 个线程那样容易。另外，线程并不便宜：创建一个线程需要几毫秒，一个新的线程会吃掉大约 1MB 的内存。 使用异步来增加扩展性和效率当你使用异步 I/O 时，用更少的线程处理更多的并发连接是可能的。当一个 I/O 操作发生时，我们不是阻塞一个线程，而是转到另一个准备好的任务上，并在以后准备好时恢复最初的任务。 Vertx 使用事件循环来复用并发的工作负载。 在事件循环上运行的代码不应该执行阻塞的 I/O 或冗长的处理。但如果你有这样的代码，也不用担心。Vertx 有工人线程和 API，可以在事件循环上处理回事件。 自行挑选最好的异步编程模型想要玩好异步编程需要更多的努力。在 Vertx 的核心模块，支持回调和 promises/futures，后者是一种简单而优雅的异步操作链模型。 使用 RxJava 可以实现高级的反应式编程，如果你喜欢更接近于传统的命令式编程，Vertx 还可以提供一流的 Kotlin coroutines 支持。 Vertx 支持许多异步编程模型：方便为每个问题选择最有效的方法。 Vertx 组件上面只是对 Vertx 的设计和理念的简单介绍。想要了解 Vertx 我们可能还得熟悉他的几个基本概念和组。 Event LoopEvent Loop 顾名思义，就是事件循环的。在 Vertx 的生命周期内，会不断的轮询查询事件。 刚才也介绍了，传统的多线程编程模型，每个请求就 fork 一个新的线程对请求进行处理。这样的编程模型有实现起来比较简单，一个连接对应一个线程。但是如果有大量的请求需要处理，就需要 fork 出大量的线程进行处理，对于操作系统来说调度大量线程造成系统负载升高。 Event Loop 不断的轮训，获取事件然后安排上不同的 Handler 处理对应的 Event。 这里要注意的是为了保证程序的正常运行，event 必须是非阻塞的。否则就会造成 Event Loop 的阻塞，影响 Vertx 的表现。但是现实中的程序肯定不能保证都是非阻塞的，Vertx 也提供了相应的处理阻塞的方法的机制。 VerticleVerticle 是由 Vertx 部署和运行的代码块。默认情况一个 Vertx 实例维护了 N（默认情况下 N = CPU核数 x 2）个 Event Loop 线程。Verticle 实例可使用任意 Vertx 支持的编程语言编写，而且一个简单的应用程序也可以包含多种语言编写的 Verticle。 可以将 Verticle 想成 Actor Model 中的 Actor。一个应用程序通常是由在同一个 Vertx 实例中同时运行的许多 Verticle 实例组合而成。不同的 Verticle 实例通过向 Event Bus 收发送消息来相互通信。 不了解 Actor 模型的同学可以参考 Golang 社区的一句话：不要通过共享内存来通信，而应该通过通信来共享内存。Actor 模型也是类似的理念，互相之间通过传递消息来共享数据，并且数据处理是单线程的，就不涉及到锁和竞争。 Verticle 其实可以分为几种： Stardand Verticle 是最常用的一类 Verticle —— 它们永远运行在 Event Loop 线程上。 Worker Verticle 会运行在 Worker Pool 中的线程上。一个实例绝对不会被多个线程同时执行。 Stardand Verticle 在创建时被分配了到 Event Loop 线程上，并且启动方法是通过 Event Loop 调用的。当你从 Event Loop 中调用任何其他需要处理核心 API 的方法时，Vertx 会保证这些程序，在同一个 Event Loop 中执行。 这意味着 Vertx 保证 verticle 实例中的所有代码总是在同一个事件循环中执行（只要你不创建自己的线程并调用它！）。 所以我们可以将所有代码写成单线程，让 Vertx 来担心线程和扩展的问题。不用担心同步和易失性的问题了，而且也避免了许多其他的竞赛条件和死锁的情况，这些情况传统多线程应用开发时非常普遍。 Worker Verticle 和 Stardand Verticle 一样，但它是使用 Vertx Worker 线程池的线程来执行的，而不是使用事件循环。 Worker Verticle 是为调用阻塞代码而设计的，因为它们不会阻塞任何 Event Loop。 如果你不想使用工作线程来运行阻塞代码，你也可以在 Event Loop 中直接运行内联阻塞代码。 Vertx 不会由一个以上的线程同时执行 Worker Verticle 实例，但可以由不同的线程在不同时间执行。 Event bus上面说了 Vertical 互相之间需要通过消息来贡献数据，而 Event bus 就是承载消息的消息总线。可能类比后端常用的 MQ 就更加容易理解了。 Vertx 的使用原则使用 Vertx 的时候要遵循基本原则： Don’t call us, we’ll call you.Vertx 的 API 基本都是事件驱动的，这意味着当 Vertx 中发生你感兴趣的事情时，Vertx 会通过向你发送事件来调用你。 事件有可能是： 定时器启动了 一些数据已经到达套接字上 一些数据被从磁盘上读取 发生了一个异常 一个 HTTP 服务器收到了一个请求 等等，比如一个处理 http 请求事件的写法： 1server.requestHandler(request -&gt; &#123;2 // This handler will be called every time an HTTP request is received at the server3 request.response().end(\"hello world!\");4&#125;); 如果事件被触发，Vertx 会自动的进行异步调用。 不要阻塞！除了极少数的例外（即一些以 Sync 结尾的文件系统操作），Vertx 中的所有 API 都不会阻塞调用线程。 如果可以立即提供一个结果，它就会立即返回，否则你通常会提供一个 handler，在一段时间后接收事件。 因为 Vertx 的 API 都不阻塞线程，所以可以使用 Vertx 只用少量的线程来处理大量的并发性。 前面也说了，传统线程模型通过创建大量的线程来处理请求，而 Vertx 通过在 Event Loop 中调用你的处理程序来应对并发请求。这其实就是并发模型中的反应器模式（Reactor Pattern）——以高性能著称的 Node.js 也是这种模式。 在一个标准的反应器实现中，有一个单一的事件循环线程，它在一个循环中运行，当所有的事件到达时，将它们传递给所有的处理程序。 单个线程的问题是它在任何时候都只能在单个核心上运行，所以如果你想让你的单线程 Reactor 应用（例如你的 Node.js 应用）在多核心服务器上扩展，你必须启动和管理许多不同的进程。 Vertx 的工作方式略有不同。每个 Vertx 实例不是一个单一的事件循环，而是维护几个事件循环。默认情况下，我们根据机器上可用的核心数量来选择数量，也可以自行设置。 也就是说一个 Vertx 进程可以在服务器上扩展，他们把这种模式称为多反应器模式，以区别于单线程的反应器模式。 黄金法则——不要阻塞事件循环我们已经知道 Vertx 的 API 是非阻塞的，不会阻塞 Event Loop，但如果你在处理程序中阻塞 Event Loop，那就没有什么帮助了。 如果你这样做了，那么这个 Event Loop 在被阻塞的时候将不能做任何其他事情，应用程序就会完全停滞不前，所以不要这样做。 阻塞的例子包括： Thread.sleep() 在锁上等待 等待一个 mutex 或监视器（如同步部分）。 做一个长期的数据库操作并等待结果 做一个复杂的计算，需要一些重要的时间。 在一个循环中旋转 具体等待多长的时间算是阻塞？ 如果你有一个单一的事件循环，并且你想每秒处理 10000 个 http 请求，那么很明显，每个请求的处理时间不能超过 0.1 毫秒，所以你不能阻塞超过这个时间。 如果你的应用程序没有反应，这可能是你在某个地方阻塞了一个事件循环的迹象。为了帮助你诊断这类问题，如果Vert.x检测到一个事件循环在一段时间内没有返回，它会自动记录警告。如果你在日志中看到类似这样的警告，那么你应该进行调查。 具体用法Vertx 的原理搞清楚之后，使用就非常简单了。比如启动一个 http 服务器，只需要这么一点代码： 1vertx.createHttpServer().requestHandler(request -&gt; &#123;2 request.response().end(\"Hello world\");3&#125;).listen(8080); 还可以用他封装好的 http client 工具： 1WebClient client = WebClient.create(vertx);23// Send a GET request4client5 .get(8080, \"myserver.mycompany.com\", \"/some-uri\")6 .send()7 .onSuccess(response -&gt; System.out8 .println(\"Received response with status code\" + response.statusCode()))9 .onFailure(err -&gt;10 System.out.println(\"Something went wrong \" + err.getMessage())); 更多用法参考官方文档吧，写的比较详细就不赘述。 干掉回调地狱在使用 Vertx 的异步无阻塞 API 时，如果我们要保证一系列操作的执行顺序，通常不能像一般的框架那样简单的依次调用，而是依次把要调用的方法放在前一个方法的事件处理函数中，用回调函数用的比较多的同事一定遇到过这种情况： 1vertx.fileSystem().writeFile(filePath, buffer, write -&gt; &#123;2 if (write.succeeded()) &#123;3 vertx.createNetClient().connect(1234, \"localhost\", connect -&gt; &#123;4 if (connect.succeeded()) &#123;5 connect.result().sendFile(filePath, send -&gt; &#123;6 connect.result().close(); // 关闭不再使用的Socket7 if (send.succeeded()) &#123;8 vertx.fileSystem().copy(filePath, backupPath, copy -&gt; &#123;9 if (copy.succeeded()) &#123;10 vertx.fileSystem().delete(filePath, delete -&gt; &#123;11 if (delete.succeeded()) &#123;12 logger.info(\"Hello, callback hell.\");13 &#125; else &#123;14 logger.error(delete.cause().getMessage());15 &#125;16 &#125;);17 &#125; else &#123;18 logger.error(copy.cause().getMessage());19 &#125;20 &#125;);21 &#125; else &#123;22 logger.error(send.cause().getMessage());23 &#125;24 &#125;);25 &#125; else &#123;26 logger.error(connect.cause().getMessage());27 &#125;28 &#125;);29 &#125; else &#123;30 logger.error(write.cause().getMessage());31 &#125;32&#125;); 这段代码先是把一段内容写到一个新文件里，然后建立一个 TCP 连接把文件发过去，再把这个文件拷贝到另一个目录作为备份，最后把原文件删掉。回调函数一层层的嵌套，形成了这样的代码结构，这就是回调地狱。 嵌套个两三层，其实还可以接受，如果业务流程比较长，这样的代码就很难看了。Vertx 提供了四种方法解决这个问题： Future Vert.x Rx Vert.x Async Kotlin coroutine 1public interface Future&lt;T&gt; extends AsyncResult&lt;T&gt;, Handler&lt;AsyncResult&lt;T&gt;&gt; &#123;2 // 用结果 result 将这个未完成的 AsyncResult&lt;T&gt; 设为成功3 void complete(T result);4 // 用异常 cause 将这个未完成的 AsyncResult&lt;T&gt; 设为失败5 void fail(Throwable cause);6 // 设置该 Future 对象被完成时应该调用的处理函数7 Future&lt;T&gt; setHandler(Handler&lt;AsyncResult&lt;T&gt;&gt; handler);8 // 其他方法9&#125; 所以我们用 Future 改写上面的代码的话： 1Future&lt;Void&gt; futureWrite = Future.future();2Future&lt;NetSocket&gt; futureConnect = Future.future();3Future&lt;Void&gt; futureSend = Future.future();4Future&lt;Void&gt; futureCopy = Future.future();5Future&lt;Void&gt; futureDelete = Future.future();67vertx.fileSystem().writeFile(filePath, buffer, futureWrite);89futureWrite.setHandler(ar -&gt; &#123;10 if (ar.succeeded()) &#123;11 vertx.createNetClient().connect(1234, \"localhost\", futureConnect);12 &#125; else &#123;13 logger.error(ar.cause().getMessage());14 &#125;15&#125;);1617futureConnect.setHandler(ar -&gt; &#123;18 if (ar.succeeded()) &#123;19 ar.result().sendFile(filePath, futureSend);20 &#125; else &#123;21 logger.error(ar.cause().getMessage());22 &#125;23&#125;);24// ...... 看起来效果还不错，整齐多了，代码也不会随着业务流程长度而无限制缩进了。不过这样还是存在两个问题： 颠倒两个代码块的顺序，该程序仍然是可以运行的，这样一来没有顺序上的约束，很容易产生混乱的代码。 异常处理存在大量重复代码。 好在 Future 还提供了一个用于链式调用的方法 compose，我们使用 Future 的 compose 方法再次重构这部分代码： 1Future&lt;Void&gt; futureWrite = Future.future();2Future&lt;NetSocket&gt; futureConnect = Future.future();3Future&lt;Void&gt; futureSend = Future.future();4Future&lt;Void&gt; futureCopy = Future.future();5Future&lt;Void&gt; futureDelete = Future.future();67vertx.fileSystem().writeFile(filePath, buffer, futureWrite);89futureWrite.compose(v -&gt; &#123;10 vertx.createNetClient().connect(1234, \"localhost\", futureConnect);11&#125;, futureConnect).compose(socket -&gt; &#123;12 socket.sendFile(filePath, futureSend);13&#125;, futureSend).compose(v -&gt; &#123;14 futureConnect.result().close(); // 关闭不再使用的Socket15 vertx.fileSystem().copy(filePath, backupPath, futureCopy);16&#125;, futureCopy).compose(v -&gt; &#123;17 vertx.fileSystem().delete(filePath, futureDelete);18&#125;, futureDelete).setHandler(ar -&gt; &#123;19 if (ar.succeeded()) &#123;20 logger.info(\"Hello, future compose!!!\");21 &#125; else &#123;22 if (futureConnect.succeeded()) &#123;23 futureConnect.result().close(); // 关闭不再使用的Socket24 &#125;25 logger.error(ar.cause().getMessage());26 &#125;27&#125;); 除了最后一个回调函数 ，前面的所有回调函数的参数并不是一个 AsyncResult&lt;T&gt; 对象，而是我们期望的结果，即一个类型为 T 的对象；也就是说每次 compose 只处理上一步成功的情况，失败的异常会被层层传递到最后一个回调函数处理——这就有点像传统的 try catch 结构。 总结Vertx 优点 性能强悍，始终处于 JVM 语言第一梯队。 轻量级，云原生时代很适合跟容器搭配。 跨语言，像跟 Kotlin 的协程搭配，可以写的很优雅 Vertx 缺点 有一定的开发、学习成本，想要适应全异步的编程，既需要生态支持，也需要编程思维的转变。 定位类似于脚手架而不是大而全的框架，如果需要应对的是复杂、业务规模庞大、不断变动和扩张的应用，那么 Spring 还是不可替代 参考资料： 响应式编程Wiki定义 编程范式：函数式编程&amp;防御式编程&amp;响应式编程&amp;契约式编程&amp;流式编程 Vertx-介绍及快速入门 回调地狱与 Future 对象https://gengteng.gitbooks.io/my-vertx-guide/content/chapter3/section5.html)","categories":[{"name":"vertx","slug":"vertx","permalink":"http://beritra.github.com/categories/vertx/"}],"tags":[{"name":"vertx","slug":"vertx","permalink":"http://beritra.github.com/tags/vertx/"},{"name":"响应式","slug":"响应式","permalink":"http://beritra.github.com/tags/%E5%93%8D%E5%BA%94%E5%BC%8F/"}]},{"title":"面试经验分享","slug":"面试经验分享","date":"2020-07-18T16:07:43.000Z","updated":"2020-08-02T12:22:47.917Z","comments":true,"path":"2020/07/19/面试经验分享/","link":"","permalink":"http://beritra.github.com/2020/07/19/%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/","excerpt":"历时近两个月的面试终于告一段落，终于有时间整理下自己这次的换工作心得，分享给大家。","text":"历时近两个月的面试终于告一段落，终于有时间整理下自己这次的换工作心得，分享给大家。 面试经历我的基本情况是这样：中下等 985 学校本科毕业，非计算机专业，在北京工作五年，之前一直在中小私企、国企工作。虽然学历还可以，但是技术谈不上精湛，只能说知识面还可以，技术栈广而杂。毕业以来薪资一直不太满意，尤其是上一份工作，在一个新创建的国企工作三年有余，薪资一直几乎没有变动，停留在不高的水平。刚入职的时候觉得还好，虽略低于同学但是好歹不忙，平台也不算太小。但是三年没有变化，做了诸多努力也无力改变现状，于是渐渐的有了离开的想法。 最终结果还算比较理想，如果把 offer 的岗位分为两类，好岗位是个人喜欢或者比较核心的业务，一般岗位是个人不太喜欢或者边缘业务，我一共拿到了以下 offer： 小米好岗位 京东一般岗位 美团一般岗位 跟谁学好岗位 百度一般岗位 快手好岗位 微博好岗位 其他挂掉的大厂还有：滴滴一面挂、字节二面挂、猿辅导一面挂、完美世界一面挂。 拿到的 offer 总结来看，小米跟谁学薪资稍低，京东美团百度虽然都是知名大厂，但是业务方向或者边缘或者自己不喜欢，就放弃了。最终在微博和快手中抉择，二者都是比较核心的业务，自己也挺喜欢两家公司，基本薪资也差不多，跟之前比基本实现了 double。但是考虑到快手的福利更好，而且现在正在快速发展期，最终决定去快手。 另外多说一句，虽然经常见到 HR 提到 30% 的涨幅上限，但是只要你 base 很低（比如在下，都是泪），或者面评给力且有其他 offer 拿来谈价格，涨幅并没有这么死板。常见的互联网三大抠也没压我的涨幅（可见之前有多低…怨念）。 而挂掉的几个，事后复盘的话，基本是自己准备不足，细节问题没回答好。滴滴除外，它比较有意思。我感觉回答的还不错，跟面试官谈的也挺融洽，最后聊工作职责的时候，年纪不大的面试官跟我用略有抱怨的语气说其实技术不复杂，但是业务很啰嗦，对个人成长没那么大帮助，一脸劝退的样子，然后就把我挂了。哈哈哈感觉还挺有趣的人，是友善的帮我避免踩坑。 基本思路我总结了面试的几个基本思路： 打好基础，技术不好啥都白搭 调整心态，面试其实很看运气，即使你再牛，也很难说百分百能面过某家公司 步步为营，先易后难，把小公司、不那么想去的公司放到前面练手 + 拿 offer 用来抬薪 及时复盘，总结经验，多多投递 一、打好基础首先我自己的战线其实很长，开始也说了，我的技术远远谈不上精湛，多数只是用，对原理不了解，但是这次准备的相对比较充分。 从去年萌生了离职想法之后，开始是先在公司内部做了一些努力，无果。十一二月份坚定了想法，开始准备，把市面上常见的面经、题纲都了解了下，然后逐一学习，Java 基础、MySQL、框架、Redis 等等这些都是重中之重，这里不详细说了，下面技术栈章节再说。从开始准备的时候，我就大概估摸了一个学习的时间，给自己定了个目标，就是大概半年，下定决心在五六月份开始面试，然后倒排制定计划。 然后就是努力学习，我工作不忙，多数时间六点半就能到家，然后除了吃饭洗澡之外基本都在读书记笔记写代码和刷 leetcode，因为本来就夜猫子，而且心里憋着口气，基本能学到晚上一两点，就这么坚持了半年。 二、调整心态心态非常重要。我自己的心理承受能力其实很差，而且很容易紧张，但是基本的道理心里还是想清楚了。 首先面试就是在短短的几个小时里判断一个人的技术能力和基本素质，所以其实并没有太准确，很看运气。有时候就算你技术很好，面试官就是跟你不一个频道，也是白搭。所以心里一定要记住这一点，不要计较。 其次不要把目标定的太狭窄，比如非哪里不去。东方不亮西方亮，有时候一个公司不行，不一定是自己的问题，总结总结经验下一场就有可能碰到聊得来的面试官。 不过话又说回来，其实大公司内部也有众多部门，就算你非常想去某个公司，一个部门没面上也有机会面别的部门，我就见过一个老哥面了四次字节跳动，前三次都挂了，然后又被捞起来三次，最终如愿以偿。 所以机会非常多，一次成败不要放在心上，好好准备就行了。 三、步步为营关于投哪里，我是这么做的：先根据自己的能力，定一个大概的上限，然后根据自己的接受程度，定一个下限，分开层次，从低到高依次投递。前面的过不过的无所谓，反正过了也不去，就当练手了。等手里有几个 offer，并且状态正好，面试也不怕了，掌握不太精确的技术点也补上了，就开始投最终的几个理想公司。以我个人为例，这次是打定主意去互联网公司了，所以就把市面上的大厂按照规模、认可度等等把想去的公司列了个表分了个级别，从高到低依次是： 腾讯、阿里、字节、快手（这些最想去，或者规模大技术强，或者风头正盛薪资福利好） 滴滴、美团、华为、微博（次顶级大厂，认可度高，薪资主流） 百度、京东、网易、小米、猿辅导（前面其实也都是二线大厂，只是个喜好原因往后排了下，猿辅导脱引而出是由于薪资实在太高） 其他二三线互联网公司 最后的结果，里面的公司除了华为全员 OD，网易忘了投，腾讯没找到合适岗位，其他都投了一圈。另外还投了几个不知名互联网公司练手。随着面试次数增加，面试越来越熟练，技术上也查缺补漏，所以总的来说是越来越顺利的。另外由于是一个接一个的面试，每次谈 offer 的时候都可以讨价还价下，最后的 offer 总包也比第一个高了七八万。 四、总结经验继续努力一两次的失败并不可怕，我前几个投递的小公司也不太理想，确实打击了一点自己的自信心。但是也有好的方面，我知道了自己薄弱的环节，比如缓存处理、数据库分库分表这些东西之前用的比较少，没想到面试问得很频繁，于是我赶紧补充这些知识，不说多深入，至少能应对一些简单问题免得面试尴尬。 就这么一路面试下来，自己不会的知识点也会越来越少，熟悉的东西越来越多，跟面试官就能愈加流畅的沟通，面试评价也越来越好。 技术栈其实常见的面试技术栈网上已经很常见了，这里我再重复一遍，然后补充自己在面试中频繁遇到的问题。 面试中问的技术问题大致上可以分为三类： 知识点 数据结构和算法 系统设计 由于已经工作五年，所以重点考察 1、3 两部分，对算法考察反而要低于应届生，基本都是常见问题。 对于面试中的问题，在准备的时候要注意，首先是要记忆准确，不要模棱两可。另外一定要举一反三，复习一个知识点的时候想想他的应用、扩展、原理、优劣势等等。 然后面试的时候会的就说，不会的不要含含糊糊，不会的时候硬答会大大的减分。一般遇到我不会但是有点思路的问题，一般都这么说：“我不确定实际是怎么实现的，但是在我看来可以 xxxx”，至少告诉面试官你的思路。其他题目也是这样，比如算法题开始做之前一定先跟面试官沟通，问清楚边界条件，然后先给面试官讲讲自己的思路，他认可你的思路之后再开始写，这样即便最后不能 bug free，他至少也知道你有思路，也避免了提笔写半天结果方向错了。而对于系统设计题，不会的话也可以找共同点，关联到自己会的地方。比如有一次面试官问我大并发下的计数，我正好看过 LongAdder 的源码，就拿出来讲了讲，二者虽然不完全相同，但是思路类似，也正好展示了自己其他地方的知识点。 一、知识点直接列知识点好了，下面写的比较笼统，更详细的内容应该网络上非常多，准备的时候一定要深入，比如线程池，就有面试官问我创建方式、参数含义、启动过程、内部原理等等，更深一点可能会问你为什么这么设计，有什么优劣。 Java 基础：考察 Java 集合、并发包工具、多线程、线程池、并发相关的关键字、锁和锁升级。 JVM：内存模型、常见 GC、不同 GC 之间的区别、GC 流程。有的会问的更详细，比如 CMS 是以最短停顿时间为设计目标，那么它怎么实现这个目标？G1 以可预测的停顿时间为设计目标，它怎么完成这个目标的？ MySQL：数据存储结构、索引、锁、事务、MVVC、性能优化、高可用方案。MySQL 的确非常复杂，可以问的地方非常多。比如索引，哪些情况下索引失效，联合索引怎么用，什么原理一定要清楚。但凡有多种模式可以选择，一定要想想优劣对比。比如 InnoDB 引擎和 Myisam 引擎的区别，binlog 中有 Row 格式和 Statement 格式，二者有什么区别，为什么这么设计。 Redis：Redis 问的是真的多，首先他应用广很多企业都用 Redis，其次它代表了计算机领域一个大的基础问题：缓存，可以考你的点很多。Redis 的数据结构、底层结构、持久化方式这些是基础，然后很可能会结合实际情况，问你怎么用 Redis 的，用到了哪些数据结构，为什么这么选。然后的话是缓存同步的方式。这里我整理了一篇常见的缓存处理方式总结：缓存系统常见问题总结 框架：Spring 反而考的不多，可能是满大街都是了。问我的基本就是循环依赖、生命周期、AOP 原理，还有像 ApplicationContext BeanFactory 区别这种问题。然后 Mybatis 可能会问下一级缓存二级缓存。 消息队列：消息队列也是非常常用的组件，面试中也问的比较多。主要包括消息丢失、重复投递、保证有序这三种问题，要清楚实现原理。然后要了解消息队列本身的架构、高可用部署方式。 网络相关：HTTP、TCP/IP、七层网络模型等等基础知识点。然后 Netty 和 NIO，搞清楚 select poll epoll 的区别和联系。 以上算是最常用的技术栈吧，根据不同公司，还有每个人简历的不同，我还遇到的一些问题包括： 容器相关：Docker 和 K8s，一般问点皮毛和应用，毕竟也不是做这个方向的。 前端：可能会问些 http 协议、跨域等等跟前后端都相关的问题，毕竟后端岗位，不太会问纯前端的问题。 微服务：Spring Cloud、Dubbo、Service Mesh。由于我做微服务很少，所以面试中也遇到的不多，最多就是问到 Hystrix 的熔断和其原理，还有调用链路的问题定位、监控等等。 Linux：基本就是看看你是不是实际用过 Linux，常见的 top、free、iostat、nload 等查看服务器各种状态的命令，然后就是遇到两家问我文本处理的脚本，像“输出 access.log 中频次最高的 100 个 IP 地址，去重排序后输出”，虽然做了准备但是平时很少有，现场没写出来… 分布式系统相关：几种分布式锁和要注意的问题，分布式一致性协议 Paxos、Raft 和 ZAB、Zookepper 的数据结构等等。然后就是分布式事务，2PC、3PC、TCC。 二、数据结构与算法首先这一部分也是我的弱项，而且由于已经毕业五年，虽然基础也会有考，但是大多数公司的门槛都会比较低，这么多家公司面试过来，基本都是问的 leetcode 中 easy 和比较基础的 medium 级别的题，仅仅在面百度的时候遇到了一道 hard，但是我没给出最优解，给出的解法只能算是 easy，同样也让过了。 具体应该准备哪些部分，在 github 上或者知乎上也有比较详细的指导教程，我就不详细叙述了，仅仅根据我面试的几家经验，认为可能会比较基础、常考的内容包括： 链表、二叉树、简单的字符串、栈、滑动窗口，递归比较容易解的题目要能写非递归解法，深度搜索广度搜索，简单的回溯等等。 另外我个人着重准备了一些动态规划的内容，因为觉得很难，但实际上没怎么考。 基础的集中排序算法要心里有数，能记住空间和时间复杂度、稳定性。 然后就是训练量啦，我本人也做的不多，就刷了两百多道，感觉一般的公司还能应付，就是面字节的时候的确压力大一点… 三、系统设计之前海外学子准备面试的时候讨论比较多的，感觉现在国内也渐渐重视起来了。常见的系统设计题目网上也有很多，不再赘述，但是由于这类题目往往比较开放，很可能不会问到你准备好的原题，所以一定要理解思路，掌握常见的设计技巧，沟通清楚题目要求和条件。而且系统设计没有优劣，只有合适不合适，所以一定要根据业务需求来，讲清楚自己的设计有什么优点，适合什么场景，有什么缺陷，在什么场景下不适用。 然后的话就是日常积累，比如我知道的像 infoQ、掘金等网站经常会有设计类的文章，将某某公司的某些技术，或者某些极端场景的解决方案，然后有些论坛、社区也时常会有人讨论，见多识广之后，这类题目一般也了熟于胸了。 然后前几天正好整理了一个简单的系统设计面试题总结，其实是自己复习的时候用的，比较简单，仅做参考：常见的架构设计面试题 结尾以上就是我最近半年从准备到面试的过程、技巧还有思路总结啦。学习是一辈子的事情，除了一些面试的方法策略，更重要的还是平时的日积月累，主动学习。 最后祝愿大家都能拿到自己满意的 offer。","categories":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"MySQL中常见的几种日志","slug":"MySQL中常见的几种日志","date":"2020-06-22T13:24:57.000Z","updated":"2022-05-10T18:49:13.441Z","comments":true,"path":"2020/06/22/MySQL中常见的几种日志/","link":"","permalink":"http://beritra.github.com/2020/06/22/MySQL%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E5%87%A0%E7%A7%8D%E6%97%A5%E5%BF%97/","excerpt":"MySQL 中有六种日志文件，分别是： 重做日志（redo log）、回滚日志（undo log）、二进制日志（binlog）、错误日志（errorlog）、慢查询日志（slow query log）、一般查询日志（general log），中继日志（relay log）。 之前没认真学习过，上次去面试被问死了，痛定思痛整理下相关的知识。","text":"MySQL 中有六种日志文件，分别是： 重做日志（redo log）、回滚日志（undo log）、二进制日志（binlog）、错误日志（errorlog）、慢查询日志（slow query log）、一般查询日志（general log），中继日志（relay log）。 之前没认真学习过，上次去面试被问死了，痛定思痛整理下相关的知识。 binglog最早接触 binlog 是做数据库主从同步的时候，知道是通过同步 binlog 实现的。binlog 是 没有 MySQL sever 层维护的一种二进制日志，与 innodb 引擎中的 redo/undo log 是完全不同的日志。其主要是用来记录对 MySQL 数据更新或潜在发生更新的 SQL 语句，并以 “事务”的形式保存在磁盘中。 binlog 主要有以下作用： 复制：MySQL 主从复制在 Master 端开启 binlog，Master 把它的二进制日志传递给 slaves 并回放来达到 master-slave 数据一致的目的 数据恢复：通过 mysqlbinlog 工具恢复数据 增量备份 几个知识点： binlog 不会记录不修改数据的语句，比如Select或者Show binlog 会重写日志中的密码，保证不以纯文本的形式出现 MySQL 8 之后的版本可以选择对 binlog 进行加密 具体的写入时间：在事务提交的时候，数据库会把 binlog cache 写入 binlog 文件中，但并没有执行fsync()操作，即只将文件内容写入到 OS 缓存中。随后根据配置判断是否执行 fsync。 删除时间：保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。 管理有这几个常用的命令可以查看 binlog 的状态： binlog 的配置信息：show variables like &#39;%log_bin%&#39;; binlog 的格式：show variables like &#39;binlog_format&#39;; 日志的文件列表：show binary logs; 当前日志的写入状态：show master status; 清空 binlog 日志：reset master; 格式binlog 日志有 Row、Statement、Mixed 三种格式。可以通过 my.cnf 配置文件及 set global binlog_format=&#39;ROW/STATEMENT/MIXED&#39;进行修改，命令行 show variables like &#39;binlog_format&#39; 命令查看 binglog 格式。 Row格式Row 格式仅保存被修改的记录的细节，不记录 SQL 语句上下文相关信息。新版本的 MySQL 默认是 Row 格式。 优点：能非常清晰的记录下每行数据的修改细节，不需要记录上下文相关信息，因此不会发生某些特定情况下的存储过程、函数或者触发器的调用触发无法被正确复制的问题，任何情况都可以被复制，且能加快从库重放日志的效率，保证从库数据的一致性 缺点：由于所有的执行的语句在日志中都将以每行记录的修改细节来记录，因此，可能会产生大量的日志内容，干扰内容也较多。比如一条 update 语句，如修改多条记录，则 binlog 中每一条修改都会有记录，这样造成 binlog 日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中，实际等于重建了表。 Statement格式每一条会修改数据的 SQL 都会记录在 binlog 中。 优点：只需要记录执行的语句的细节和上下文环境，而不需要记录每一行的变化，在一些修改记录较多的情况下相比 Row 格式能大大减少 binlog 日志量，节约 IO，提高性能。 另外还可以用于实时的还原。 主从版本可以不一样，从服务器版本可以比主服务器版本高。 缺点：为了保证 sql 语句能在 slave 上正确执行，必须记录上下文信息，以保证所有语句能在 slave 得到和在 master 端执行时候相同的结果。 另外，主从复制时，存在部分函数（如 sleep）及存储过程在 slave 上会出现与 master 结果不一致的情况，而相比 Row 记录每一行的变化细节，绝不会发生这种不一致的情况。 Mixed格式以上两种格式混合。 经过前面的对比，可以发现 Row 和 Statement 各有优势，如果可以根据 SQL 语句取舍可能会有更好地性能和效果。Mixed 便是以上两种形式的结合。不过，新版本的 MySQL 对 Row 模式也做了优化，并不是所有的修改都会完全以 Row 形式来记录，像遇到表结构变更的时候就会以 Statement 模式来记录，如果 SQL 语句确实就是 update 或者 delete 等修改数据的语句，那么还是会记录所有行的变更；因此，现在一般使用 Row 即可。 主从复制复制是 MySQL 最重要的功能之一，MySQL 集群的高可用、负载均衡和读写分离都是基于复制来实现。复制步骤如下： Master 将数据改变记录到二进制日志(binary log)中。 Slave 上面的 IO 进程连接上 Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。 Master 接收到来自 Slave 的 IO 进程的请求后，负责复制的 IO 进程会根据请求信息读取日志指定位置之后的日志信息，返回给 Slave 的 IO 进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到 Master 端的 binlog 文件的名称以及 binlog 的位置。 Slave 的 IO 进程接收到信息后，将接收到的日志内容依次添加到 Slave 端的 relaylog 文件的最末端，并将读取到的 Master 端的 binlog 的文件名和位置记录到 masterinfo 文件中，以便在下一次读取的时候能够清楚的告诉 Master 从某个 binlog 的哪个位置开始往后的日志内容 Slave 的 SQL 进程检测到 relaylog 中新增加了内容后，会马上解析 relaylog 的内容成为在 Master 端真实执行时候的那些可执行的内容，并在自身执行。 undo log 和 redo logundo log 和 redo log 其实都不是 MySQL 数据库层面的日志，而是 InnoDB 存储引擎的日志。二者的作用联系紧密，事务的隔离性由锁来实现，原子性、一致性、持久性通过数据库的 redo log 或 redo log 来完成。redo log 又称为重做日志，用来保证事务的持久性，undo log 用来保证事务的原子性和 MVCC。 redo log功能和大多数关系型数据库一样，InnoDB 记录了对数据文件的物理更改，并保证总是日志先行，也就是所谓的 WAL，即在持久化数据文件前，保证之前的 redo 日志已经写到磁盘。由于 redo log 是顺序整块写入，所以性能要更好。 重做日志两部分组成：一是内存中的重做日志缓冲(redo log buffer)，是易失的；二是重做日志文件(redo log file)，是持久的。redo log 记录事务操作的变化，记录的是数据修改之后的值，不管事务是否提交都会记录下来。 写入过程在一条语句进行执行的时候，InnoDB 引擎会把新记录写到 redo log 日志中，然后更新内存，更新完成后就算是语句执行完了，然后在空闲的时候或者是按照设定的更新策略将 redo log 中的内容更新到磁盘中。 更详细的步骤，需要了解两个关键词：checkpoint 和 LSN(Log Sequence Number)，前者检查点简单来说就是把脏页刷到磁盘的时间点，这个时间点之前的数据都已经保存到了持久存储。而 LSN 是 InnoDB 使用的一个版本标记的计数，它是一个单调递增的值。数据页和 redo log 都有各自的 LSN。每次把 redo log 中的内容写入到实际的数据页之后，就会把 LSN 也同步过去。如果发生了宕机，我们可以根据数据页中的 LSN 值和 redo log 中 LSN 的值判断需要恢复的 redo log 的位置和大小。redo log 同样也有自己的缓存，所以也涉及到刷盘策略，是通过innodb_flush_log_at_trx_commit这个参数控制的。 当对应事务的脏页写入到磁盘之后，redo log 的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。 存储结构这一块应该就没必要深入了，redo log 的存储都是以块(block)为单位进行存储的，每个块的大小为 512 字节。同磁盘扇区大小一致，可以保证块的写入是原子操作。 另外 redo log 占用的空间是固定的，会循环写入。文件大小由innodb_log_file_size参数控制。 undo logundo log 有两个作用：提供回滚和多版本并发控制下的读(MVCC)，也即非锁定读 在数据修改的时候，不仅记录了redo，还记录了相对应的 undo，如果因为某些原因导致事务失败或回滚了，可以借助该 undo 进行回滚。 undo log 和 redo log 记录物理日志不一样，它是逻辑日志。可以认为当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录，反之亦然，当 update 一条记录时，它记录一条对应相反的 update 记录。 有时候应用到行版本控制的时候，也是通过 undo log 来实现的：当读取的某一行被其他事务锁定时，它可以从 undo log 中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。 undo log 是采用段(segment)的方式来记录的，每个 undo 操作在记录的时候占用一个 undo log segment。 另外，undo log 也会产生 redo log，因为 undo log 也要实现持久性保护。 当事务提交的时候，InnoDB 不会立即删除 undo log，因为后续还可能会用到 undo log，如隔离级别为 repeatable read 时，事务读取的都是开启事务时的最新提交行版本，只要该事务不结束，该行版本就不能删除，即 undo log 不能删除。 当事务提交之后，undo log 并不能立马被删除，而是放入待清理的链表，由 purge 线程判断是否有其他事务在使用 undo 段中表的上一个事务之前的版本信息，决定是否可以清理 undo log 的日志空间。 在 MySQL 5.7 之前，undo log 存储在共享表空间中，因此有可能大大增加表空间的占用，5.7 之后可以通过配置选择存储在独立的表空间中。 三种日志总结首先 InnoDB 完成一次更新操作的具体步骤： 开启事务 查询待更新的记录到内存，并加 X 锁 更改内存中的数据记录 记录 undo log 到内存 buffer，undo log 本身也有对应的redo log，后面一起写入buffer 记录 redo log 到内存 buffer，PREPARE阶段 提交事务，触发 redo log 刷盘 记录 bin log 事务提交 其中 redo log 和 bin log 的数据是有一致性保证的，通过两阶段提交实现。 慢查询日志几个配置参数： slow_query_log 慢查询开启状态 slow_query_log_file 慢查询日志存放的位置（这个目录需要 MySQL 的运行帐号的可写权限，一般设置为 MySQL 的数据存放目录） long_query_time 查询超过多少秒才记录 log_queries_not_using_indexes：未使用索引的查询也被记录到慢查询日志中（可选项） 修改参数可以通过配置文件，也可以在数据库中通过SET关键字来设置。 参考文章 腾讯工程师带你深入解析 MySQL binlog 详细分析MySQL事务日志(redo log和undo log) MySQL中的重做日志（redo log），回滚日志（undo log），以及二进制日志（binlog）的简单总结 MySQL innoDB——redo log/undo log 15.6.6撤消日志 15.6.5重做日志 基于Redo Log和Undo Log的MySQL崩溃恢复流程 mysql为什么需要undo log？","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"}]},{"title":"分布式协议Paxos、Raft和ZAB","slug":"分布式协议Paxos、Raft和ZAB","date":"2020-06-11T16:22:20.000Z","updated":"2020-06-11T16:24:20.428Z","comments":true,"path":"2020/06/12/分布式协议Paxos、Raft和ZAB/","link":"","permalink":"http://beritra.github.com/2020/06/12/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AEPaxos%E3%80%81Raft%E5%92%8CZAB/","excerpt":"首先得放在开头，分布式系统的一致性协议一直是分布式系统的难题，本人没有阅读过 Lamport 老人家的论文原文（估计直接读也未必读的懂），以下所有内容来自网络博文、书籍和网站的整理，加上自己的理解润色。水平有限不敢说全都正确，仅供参考。","text":"首先得放在开头，分布式系统的一致性协议一直是分布式系统的难题，本人没有阅读过 Lamport 老人家的论文原文（估计直接读也未必读的懂），以下所有内容来自网络博文、书籍和网站的整理，加上自己的理解润色。水平有限不敢说全都正确，仅供参考。 Paxos说到分布式一致性协议，Paxos 肯定是绕不开的，关于它和其作者 Lamport 的传奇故事也是有很多，这里就不啰嗦了，感兴趣的可以自行搜索。作为几种常见协议的基础，Paxos 提供了“选举” 的思想，Lamport 为了简化 Paxos，也为了讲述这个算法，假想了一个叫做 Paxos 的希腊城邦进行选举的情景，这个算法也是因此而得名。 Paxos 算法的步骤是这样： 首先有两种角色，一个是“提议者”，一个是“接受者”。提议者可以向接受者提出提议，然后接受者表达意见。 因为存在多个提议者，如果同时表达意见会出现意见不一致的情况，所以首先需要尽快选出一个领导者，让意见统一。 然后领导者会给接受者发出提议，如果一个提议被大多数接受者接纳，这个提议就通过了。 大致的流程是这样，再完善一下细节： 如何明确领导者？这里会有两个阶段： 第一阶段是先达成一致，协商出领导者，具体方法就是通过编号，提议者会先报告一个编号，谁的编号最大谁就是领导（Lamport 巧妙的比喻为“贿选”，谁出的钱多就选谁）。 然后第二阶段就是上一轮胜出的领导提出提议，发送给各个接受者。 跟常识不太一样的是，每个提议者不会执着于当领导，而是谋求尽快达成共识，所以如果在一个提议者提议的时候，如果发现接受者已经接受了其他领导者的提议，也会默默的把自己的提议改为前面领导者的提议。 编号不能太小，很小的话前两个阶段都会直接失败，接受者拒绝接受你的提议。 在步骤 2 中，还有可能出现冲突：比如如果一个提议者在对接受者 A 提议的时候，发现 A 接受了领导者 L1 的提议，然后又去接受者 B 那里提议，发现 B 接受了领导者 L2 的提议，已知这个提议者肯定会跟随前面已经存在的提议，那么他会把自己的提议改为上面两个的哪个呐？答案是数值更大的那个。 在达成共识之前，这整个阶段，哪个提议者先来，哪个后来，接受者什么时候收到提议者的信息，都是不可控的。所以有可能产生这样一种情况：一个提议者已经晋升为领导者，但是还没发起提议，这时候另外一个“土豪”提议者过来，疯狂“贿选”，还是存在机会让自己胜出的。这时就形成了一种博弈： 上一个领导者要赶在土豪提议者贿赂到接受者前，赶到接受者面前让他接受自己的提议，否则会因为自己的之前贿赂的钱比土豪少而被拒绝。 土豪“提议者”要赶在上一个领导者将提议传达给接受者前，贿赂到接受者，否则土豪提议者即便贿赂成功，也要默默的将自己的提议改为前任意见领袖的提议。 这整个博弈的过程，最终就看这两个提议者谁的进展快了。但最终一定会有一个领导者，先得到多数接受者的认可，那他的提议就胜出了。 总结来看，Paxos 包括以下几个原则： Paxos 算法包括两个阶段：第一个阶段主要是贿选，还没有提出提议；第二个阶段主要根据第一阶段的结果，明确接受谁的提议，并明确提议的内容是什么（这个提议可能是贿选胜出提议者自己的提议，也可能是前任意见领袖的提议，具体是哪个提议，往下看，见下面第 3 点的内容）。 编号（贿赂金额）很重要，无论在哪个阶段，编号（贿赂金额）小的，都会被拒绝。 在第一阶段中，一旦接受者已经接受了之前领导者的提议，那后面再来找这个接受者的提议者，即便在贿赂中胜出，也要被洗脑，默默将自己的提议改为前任意见领袖的提议，然后他会在第二阶段提出该提议（也就是之前意见领袖的提议，以力争让大家的意见趋同）。如果接受者之前没有接受过任何提议，那贿选胜出的提议者就可以提出自己的提议了。 以上，我们说的 Paxos 其实又叫 Basic Paxos，只具备理论基础，实际上实现起来很麻烦而且效率低。因为每次同步一个信息，就要进行上述繁杂的达成共识阶段，无异会产生巨大的开销。 于是后来又有了进阶版的 Multi Paxos 协议。只要 Leader 是相对稳定不变的，第 1 阶段就变得不必要。 这样，系统可以在接下来的 Paxos 算法实例中，跳过的第 1 阶段，直接使用同样的 Leader。 Multi Paxos 本身对一些边缘情况没有定义，所以大多数实际应用是基于 Multi Paxos 进行补充，或者使用它的变种 Raft。 从网上查询得知，工业界中三种协议的应用情况基本如下： 微信背后的高可用存储系统 PaxosStore 是基于 Multi Paxos 开发的 广为人知的高可靠的 kv 存储系统 etcd 用的 Raft 阿里的高性能存储 PolarFS 用到的 Raft 变种 ParallelRaft 协议 TiDB 用的 Raft ZAB 的名字就叫 ZooKeeper Atomic Broadcast，自然是 ZooKeeper 在用，没找到其他在用的，但是考虑到动物管理员恐怖的占有率，ZAB 协议也不容轻视 Raft 协议Raft 感觉是最容易理解的一个了，也很有意思。先说它的逻辑： Raft 协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。 Leader：所有请求的处理者，Leader 副本接受 client 的更新请求，本地处理后再同步至多个其他副本Follower：请求的被动更新者，从 Leader 接受更新请求，然后写入本地日志文件Candidate：如果 Follower 副本在一段时间内没有收到 Leader 副本的心跳，则判断 Leader 可能已经故障，此时启动选主过程，此时副本会变成 Candidate 状态，直到选主结束。 可以看出，跟 Paxos 的基本理念一样，首先最基本的 Flollow（接受者），如果 Follower 接收不到 Leader 的心跳，就会全部转化为Candidate（提议者），然后提议者开始“贿选”，直到选出 Leader 之后，所有没选上的 Candidate 退回到 Follower 状态，统一接收 Leader 领导。 就是说只要 Leader 不挂掉，只要选举一次就行了，后面大家默认信任选出来的 Leader。 另外，每一个副本都会维护一个 term，类似于一个逻辑时钟，每发生一个动作就会递增，通过比较每个提议的 term，副本会默认使用最新的 term，防止发生冲突。如果一个 Leader 或者 Candidate 发现自己的 term 不是最新的了，就会自动降级到 Follower，而如果一个 Follower 接收到低于自己当前 term 的提议，就会直接抛弃。 基本原则了解之后，我们完善一下细节。 在强 Leader 的帮助下，Raft 将一致性问题分解为了三个子问题： Leader 选举：当已有的 Leader 故障时必须选出一个新的 Leader。 日志复制：Leader 接受来自客户端的命令，记录为日志，并复制给集群中的其他服务器，并强制其他节点的日志与 Leader 保持一致。 安全 safety 措施：通过一些措施确保系统的安全性，如确保所有状态机按照相同顺序执行相同命令的措施。 另外丢两个非常清晰的 Raft 全流程动画演示，看完之后很容易理解： http://thesecretlivesofdata.com/raft/ https://raft.github.io/ 选举过程我们从最初始的状态来模拟，假设一个集群有三个副本，刚启动的时候，大家都是 Follower。然后每个 Follower 会有一个倒计时（election timeout），在倒计时结束之前，如果没有收到任何 Leader 的心跳，或者其他 Candidate 的投票请求，就会转化为 Candidate，开始选举。 变成 Candidate 之后，会先投自己一票，同时开启一个倒计时，然后向所有其他节点发起投票请求。如果在倒计时完成之前，没有成为 Leader 或者接收到其他 Leader 的消息，就会发起新一轮选举。 当一个副本处于 Candidate 状态时，如果收到来自 Leader 的心跳消息，就会立即变身为 Follower。如果发出去的投票请求得到了半数节点的成功回应，就会立即变身为 Leader，并周期性地向其它节点广播心跳消息，以尽可能长期维持自己的统治地位。 关于选举的更多细节： election timeout 会是一个一定范围内的随机值，因为如果所有节点的倒计时时间都一样，大家就会同时变成 Candidate，然后同时互相投票选举，加大了达成共识的难度，所以倒计时会稍微错开，就很容易率先选出来一个 Leader。 成功选举 Leader 之后，Leader 会向所有节点发送心跳，然后心跳会重置每个节点的 election timeout 倒计时时间。 即便错开了倒计时，仍然有可能出现多个 Candidate 同时竞争，如果两个 Candidate 获得的票数不一致还好说，其中一个必然是多数，变成了 Leader。但是如果恰巧节点总数是偶数，就有可能出现票数一样僵持的情况。这时候就会重新选举。（这里我觉得每个 Candidate 发一个随机数过去，谁更大听谁的也行…，当然即便这样也有可能一样大） 数据同步节点选出来了，下面就应该进行数据同步了。当一个数据修改的请求过来，会直接找到 Leader 节点，所有的增删改查都由 Leader 受理。然后同步给各个 Follower。 每次数据同步操作同时也是一个心跳，会更新 Follower 的 election timeout。另外只有当多数节点返回同步成功之后，Leader 才会给客户端返回操作成功。 分区容错然后是最麻烦的部分，如果出现了网络分区怎么办？比如原本五个节点的集群，被分成了双节点和三节点的两个集群。 假设原本的 Leader 在双节点的集群里面，那么这个集群会照常运作。而新出现的三个节点的集群，由于没有收到心跳，会开始选举，然后选出新的 Leader。这时候，如果有客户端发起请求，有可能发送到两个不同的 Leader 上面，如果发送到原来的那个 Leader 上，即双节点的集群中，Leader 把操作同步给 Follower，会发现收不到足够多的 Follower 响应（因为这个 Follower 还以为自己的集群是五个节点），然后就没办法同步数据。而三节点的新集群，就可以顺利更新数据。 如果这时候网络恢复了，各个节点又可以正常通信，三节点集群中的 Leader 和 双节点集群中的 Leader 会互相通信，然后会发现三节点的 Leader 由于一直正常运行，term 值会不断增大，所以大家会采信他的数据。于是双节点的两台机器会回滚，然后全部接受新 Leader 的数据同步。 ZAB 协议了解了上面两种协议之后， ZAB 协议学习起来也不复杂。ZAB 协议定义了选举（election）、发现（discovery）、同步（sync）、广播（Broadcast）四个阶段。 选举（election）是选出哪台为主机； 发现（discovery）、同步（sync）当主选出后，要做的恢复数据的阶段； 广播（Broadcast）当主机和从选出并同步好数据后，正常的主写同步从写数据的阶段。 跟其他协议类似，集群副本有三种状态： Leader 也就是领导者 Follower 也就是接受提议的跟随者 Observer 可以认为是领导者的的 Copy，不参与投票，在这可以忽略 与之对应的，一个 ZK 集群中的某个节点也有三种状态： Looking：选举状态，当前群龙无首 Leading：Leader 节点才有的状态 Following：Follower 节点才有的状态 Observing：观察者状态。表明当前服务器角色是 Observer，与 Folower 唯一的不同在于不参与选举，也不参与集群写操作时的投票。 每次写成功的消息，都有一个全局唯一的标识，叫 zxid。是 64 bit 的正整数，高 32 为叫 epoch 表示选举纪元，低 32 位是自增的 id，每写一次加一。一个博主巧妙地比喻为为中国古代的年号，非常形象，例如万历十五年，万历是 epoch，十五年是 id。 ZK 集群一般都是奇数个机器（2n+1）,只有一个领导者 Leader，其余都是跟随者 Follower。选主还是写数据，要有大于等于 n+1 台选举相同，才能执行选举的操作。所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。 选举当集群新建，或者主机死机，或者主机与一半或以上的从机失去联系后，都会触发选择新的主机操作。 选举有多种算法，到3.4.10版本为止，可选项有： 0 基于UDP的LeaderElection 1 基于UDP的FastLeaderElection 2 基于UDP和认证的FastLeaderElection 3 基于TCP的FastLeaderElection 在3.4.10版本中，默认值为 3，也即基于 TCP 的 FastLeaderElection。另外三种算法已经被弃用，并且有计划在之后的版本中将它们彻底删除而不再支持。 FastLeaderElection这是 ZAB 默认采用的算法。 每次选举都要把选举轮数加一，类似于 zxid 里的 epoch 字段，防止不同轮次的选举互相干扰。 每个进入 Looking 状态的节点，会先把投票箱清空，然后通过广播投票给自己，再把投票消息发给其它机器，同时也在接受其他节点的投票。投票信息包括：轮数、被投票节点的 zxid，被投票节点的编号等等。 其他 Looking 状态的节点收到后： 首先判断票是否有效。是否有效的方法为看票的投票轮数和本地记载的投票轮数是否相等： 如果比本地投票轮数的小，丢弃。 如果比本地投票轮数的大，证明自己投票过期了，清空本地投票信息，更新投票轮数和结果为收到的内容。通知其他所有节点新的投票方案。 如果和本地投票轮数相等，按照投票的优先级比较收到的选票和自己投出去的选票： 如果收到的优先级大，则更新自己的投票为对方发过来投票方案，把投票发出去。 如果收到的优先级小，则忽略该投票。 如果收到的优先级相等，则更新对应节点的投票。 每收集到一个投票后，查看已经收到的投票结果记录列表，看是否有节点能够达到一半以上的投票数。如果有达到，则终止投票，宣布选举结束，更新自身状态。然后进行发现和同步阶段。否则继续收集投票。 投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为 Leading，否则将自己的状态更新为 Following。 广播——主从同步主从同步数据比较简单，当有写操作时，如果是从机接收，会转到主机，保证写都是在主机上进行。Leader 会先提议事务，收到过半回复后，再发提交。 当 Leader 收到写操作时，先本地生成事务为事务生成 zxid，然后发给所有 Follower 节点。 当 Follower 收到事务时，先把提议事务的日志写到本地磁盘，成功后返回给 Leader。 Leader 收到过半反馈后对事务提交。再通知所有的 Follower 提交事务， Follower 收到后也提交事务，提交后就可以对客户端进行分发了。 Raft\\ZAB共同点和区别首先，二者都是通过选举一个 Leader 来简化复杂度，后续的工作都是由 Leader 来做。 投票的时候，二者都需要定义一个轮次 Raft 定义了 term 来表示选举轮次 ZooKeeper 定义了 electionEpoch 来表示 同步数据的时候，都希望选举出来的 Leader 至少包含之前全部已提交的日志。 那如何能包含之前的全部日志？我们可以通过判断 Leader 节点中日志的逻辑时间序列，包含越新、越多日志的节点，越有可能包含之前全部的已提交日志。对于两种协议： Raft：term 大的优先，然后 entry 的 index 大的优先 ZooKeeper：peerEpoch 大的优先，然后 zxid 大的优先 ZooKeeper 有 2 个轮次，一个是选举轮次 electionEpoch，另一个是日志的轮次 peerEpoch（即表示这个日志是哪个轮次产生的）。而 Raft 则是只有一个轮次，相当于日志轮次和选举轮次共用了。 但是有一个问题，日志越新越大的比较方式能满足我们“Leader 至少包含之前全部已提交的日志”的愿望吗？ 对于 Raft 协议，特殊情况下不能。对于 Raft 协议，通过两个约束来保证一致性： 当前 term 的 Leader 不能“直接”提交之前 term 的 entries。 必须要等到当前 term 有 entry 过半了，才顺便一起将之前 term 的 entries 进行提交。 至于为什么必须这样，在什么特殊情况下会出问题，这篇文章中给了详细说明：Raft算法赏析建议直接看里面的例子，有点长我就不抄过来了。 但是对于 ZooKeeper 是不会出现这种情况的，因为 ZooKeeper 在每次 Leader 选举完成之后，都会进行数据之间的同步纠正，所以每一个轮次，大家都日志内容都是统一的。 继续对比，二者的选举效率也不同： Raft 中的每个节点在某个 term 轮次内只能投一次票，哪个 Candidate 先请求投票谁就可能先获得投票，这样就可能造成分区，即各个 Candidate 都没有收到过半的投票，Raft 通过 Candidate 设置不同的超时时间，来快速解决这个问题，使得先超时的Candidate（在其他人还未超时时）优先请求来获得过半投票。 ZooKeeper 中的每个节点，在某个 electionEpoch 轮次内，可以投多次票，只要遇到更大的票就更新，然后分发新的投票给所有人。这种情况下不存在分区现象，同时有利于选出含有更新更多的日志的 Server，但是选举时间理论上相对 Raft 要花费的多。 在一个节点启动后，如何加入一个集群（这里是说本来就在集群配置内的一个节点）： Raft：比较简单，该节点启动后，会收到 Leader 的 AppendEntries RPC，在这个 RPC 里面包含 Leader 信息，可以直接识别。 ZooKeeper：启动后，会向所有的其他节点发送投票通知，然后收到其他节点的投票。该节点只需要判断上述投票是否过半，过半则可以确认 Leader。 关于 Leader 选举的触发： 首先集群启动的时候，二者肯定都要先进行选举。 如果选举完成后，发生了超时： Raft：目前只是 Follower 在检测。如过 Follower 在倒计时时间内未收到 Leader 的心跳信息，则 Follower 转变成 Candidate，自增 term 发起新一轮的投票。 ZooKeeper：Leader 和 Follower 都有各自的检测超时方式，Leader 是检测是否过半 Follower 心跳回复了，Follower 检测 Leader 是否发送心跳了。一旦 Leader 检测失败，则 Leader 进入 Looking 状态，其他 Follower 过一段时间因收不到 Leader 心跳也会进入 Looking 状态，从而出发新的 Leader 选举。一旦 Follower 检测失败了，则该 Follower 进入 Looking 状态，此时 Leader 和其他 Follower 仍然保持良好，则该 Follower 仍然是去学习上述 Leader 的投票，而不是触发新一轮的 Leader 选举。 关于上一轮次 Leader 残存的数据怎么处理： 包括两种数据： 已过半复制的日志 未过半复制的日志 Raft：对于之前 term 的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当前 term 的日志过半了，才会顺便将之前 term 的日志进行提交 ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中 Raft 的保守策略更多是因为 Raft 在 Leader 选举完成之后，没有同步更新过程来保持和 Leader 一致（在可以对外处理请求之前的这一同步过程）。而 ZooKeeper 是有该过程的。 在对正常请求的处理方式上，二者都是基本相同的，大致过程都是过半复制。 对于正常请求的消息顺序保证： Raft：对请求先转换成 entry，复制时，也是按照 Leader 中 log 的顺序复制给 Follower 的，对 entry 的提交是按 index 进行顺序提交的，是可以保证顺序的 ZooKeeper：在提交议案的时候也是按顺序写入各个 Follower 对应在 Leader 中的队列，然后 Follower 必然是按照顺序来接收到议案的，对于议案的过半提交也都是一个个来进行的 如果是 Leader 挂了之后，重新选举出 Leader，会不会有乱序的问题？ Raft：Raft 对于之前 term 的 entry 被过半复制暂不提交，只有当本 term 的数据提交了才能将之前 term 的数据一起提交，也是能保证顺序的 ZooKeeper：ZooKeepe r每次 Leader 选举之后都会进行数据同步，不会有乱序问题 在出现网络分区情况下的应对措施，二者都是相同的： 目前 ZooKeeper 和 Raft 都是过半即可，所以对于分区是容忍的。如5台机器，分区发生后分成 2 部分，一部分 3 台，另一部分 2 台，这 2 部分之间无法相互通信。 其中，含有 3 台的那部分，仍然可以凑成一个过半，仍然可以对外提供服务，但是它不允许有节点再挂了，一旦再挂一台则就全部不可用了。 含有 2 台的那部分，则无法提供服务，即只要连接的是这 2 台机器，都无法执行相关请求。 所以 ZooKeeper 和 Raft 在一旦分区发生的情况下是是牺牲了高可用来保证一致性，即 CAP 理论中的 CP，二者都是 CP 系统。 必须需要重申，无论上面哪一种协议，都只是概念模型，还有很多细节需要补充完善，比如具体的数据存储方式，日志处理，具体的节点通信方式等等，距离实现一个工业可用的框架还有一段距离，想了解具体细节可以参考各个大厂开源的产品，或者自己尝试实现一下。 参考文章: 浅显易懂地解读Paxos算法 Raft对比ZAB协议 CAP一致性协议及应用解析 Raft协议原理详解 十分钟了解ZAB协议 实例详解ZooKeeper ZAB协议、分布式锁与领导选举","categories":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://beritra.github.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"分布式系统","slug":"分布式系统","permalink":"http://beritra.github.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"},{"name":"架构","slug":"架构","permalink":"http://beritra.github.com/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"常见的架构设计面试题","slug":"常见的架构设计面试题","date":"2020-06-11T16:21:59.000Z","updated":"2022-02-20T11:27:13.892Z","comments":true,"path":"2020/06/12/常见的架构设计面试题/","link":"","permalink":"http://beritra.github.com/2020/06/12/%E5%B8%B8%E8%A7%81%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"整理了一些常见的架构设计面试题，主要记录关键点，具体细节就不详细叙述了，案例慢慢补充。目前想起以下问题： 秒杀系统 短链接生成 高并发的红包系统 分布式ID生成 分布式限流 分布式定时任务 新浪微博怎么推送微博 大文件有限内存排序","text":"整理了一些常见的架构设计面试题，主要记录关键点，具体细节就不详细叙述了，案例慢慢补充。目前想起以下问题： 秒杀系统 短链接生成 高并发的红包系统 分布式ID生成 分布式限流 分布式定时任务 新浪微博怎么推送微博 大文件有限内存排序 秒杀系统秒杀系统基本面试被问烂了，网上资料也很多，基本整理了内容如下： 设计难点：并发量大，应用、数据库都承受不了。另外难控制超卖。 设计要点： 将请求尽量拦截在系统上游，html尽量静态化，部署到cdn上面。按钮及时设置为不可用，禁止用户重复提交请求。 设置页面缓存，针对同一个页面和uid一段时间内返回缓存页面。 数据用缓存抗，不直接落到数据库。 读数据的时候不做强一致性教研，写数据的时候再做。 在每台物理机上也缓存商品信息等等变动不大的相关的数据 像商品中的标题和描述这些本身不变的会在秒杀开始之前全量推送到秒杀机器上并一直缓存直到秒杀结束。 像库存这种动态数据会采用被动失效的方式缓存一定时间（一般是数秒），失效后再去Tair缓存拉取最新的数据。 如果允许的话，用异步的模式，等缓存都落库之后再返回结果。 如果允许的话，增加答题教研等验证措施。 其他业务和技术保障措施： 业务隔离。把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就是已知热点，当真正开始时我们可以提前做好预热。 系统隔离。系统隔离更多是运行时的隔离，可以通过分组部署的方式和另外 99% 分开。秒杀还申请了单独的域名，目的也是让请求落到不同的集群中。 数据隔离。秒杀所调用的数据大部分都是热数据，比如会启用单独 cache 集群或 MySQL 数据库来放热点数据，目前也是不想0.01%的数据影响另外99.99%。 另外需要复习缓存穿透、雪崩等等问题，主要的流量都落在了缓存数据库上，需要针对缓存数据库的高可用作保障。 短链接生成这个应该是比较公认的方案了： 分布式ID生成器产生ID ID转62进制字符串 记录数据库，根据业务要求确定过期时间，可以保留部分永久链接 主要难点在于分布式ID生成。鉴于短链一般没有严格递增的需求，可以使用预先分发一个号段，然后生成的方式。 看了下新浪微博的短链接，8位，理论上可以保存超过200万亿对关系，具体怎么存储的还有待研究。 红包系统红包系统其实很像秒杀系统，只不过同一个秒杀的总量不大，但是全局的并发量非常大，比如春晚可能几百万人同时抢红包。 主要技术难点也类似，主要在数据库，减库存的时候会抢锁。另外由于业务需求不同，没办法异步，也不能超卖，事务更加严格。 不能采用的方式： 乐观锁：手慢会失败，DB 面临更大压力，所以不能采用。 直接用缓存顶，涉及到钱，一旦缓存挂掉就完了。 建议的方式： 接入层垂直切分，根据红包ID，发红包、抢红包、拆红包、查详情详情等等都在同一台机器上处理，互不影响，分而治之。 请求进行排队，到数据库的时候是串行的，就不涉及抢锁的问题了。 为了防止队列太长过载导致队列被降级，直接打到数据库上，所以数据库前面再加上一个缓存，用CAS自增控制并发，太高的并发直接返回失败。 红包冷热数据分离，按时间分表。 分布式ID分布式ID生成大概也算老生常谈的问题了，主要关键在于是否需要严格递增，严格递增的话效率必然大降。 不需要递增的话比较简单: 一种方式是预先分片，比如十台机器，每台先分一千个ID，一号机从0开始，二号从1000开始等等。缺点是大致上可以被人看出来业务量。 另一种方式是类似雪花算法，每个机器有个id，然后基于时间算一个id，再加上一个递增id。比如如下美团的方案。缺点是机器的时间戳不能回拨，回拨的话会出现问题。 如果要求严格递增，我没找到现成的很好的方案，大概只能单机生成，不能分布式了，然后都去单机上取号。效率的话，类似Redis的数据库大概能到每秒十几二十几万的速度。 分布式限流常见的限流方法： 固定窗口计数器：按照时间段划分窗口，有一次请求就+1，最为简单的算法，但这个算法有时会让通过请求量允许为限制的两倍。 滑动窗口计数器：通过将窗口再细分，并且按照时间“滑动”来解决突破限制的问题，但是时间区间的精度越高，算法所需的空间容量就越大。 漏桶：请求类似水滴，先放到桶里，服务的提供方则按照固定的速率从桶里面取出请求并执行。缺陷也很明显，当短时间内有大量的突发请求时，即便此时服务器没有任何负载，每个请求也都得在队列中等待一段时间才能被响应。 令牌桶：往桶里面发放令牌，每个请求过来之后拿走一个令牌，然后只处理有令牌的请求。令牌桶满了则多余的令牌会直接丢弃。令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。 Google 的开源项目 guava 提供了 RateLimiter 类，实现了单点的令牌桶限流。 分布式环境下，可以考虑用 Redis+Lua 脚本实现令牌桶。 如果请求量太大了，Redis 也撑不住怎么办？我觉得可以类似于分布式 ID 的处理方式，Redis 前面在增加预处理，比如每台及其预先申请一部分令牌，只有令牌用完之后才去 Redis。如果还是太大，是否可以垂直切分？按照流量的来源，比如地理位置、IP 之类的再拆开。 分布式定时任务任务轮询或任务轮询+抢占排队方案 每个服务器首次启动时加入队列； 每次任务运行首先判断自己是否是当前可运行任务，如果是便运行； 如果不是当前运行的任务，检查自己是否在队列中，如果在，便退出，如果不在队列中，进入队列。 微博推送主要难点：关系复杂，数据量大。一个人可以关注非常多的用户，一个大 V 也有可能有几千万的粉丝。 先介绍最基本的方案： 推模式：推模式就是，用户A关注了用户 B，用户 B 每发送一个动态，后台遍历用户B的粉丝，往他们粉丝的 feed 里面推送一条动态。 拉模式：推模式相反，拉模式则是，用户每次刷新 feed 第一页，都去遍历关注的人，把最新的动态拉取回来。 一般采用推拉结合的方式，用户发送状态之后，先推送给粉丝里面在线的用户，然后不在线的那部分等到上线的时候再来拉取。 另外冷热数据分离，用户关系在缓存里面可以设置一个过期时间，比如七天。七天没上线的可能就很少用这个 APP。 大文件排序对于远高于内存的文件排序。 外归并排序： 对文件分割，然后分别排序 排好序的文件依次读取一个缓冲区的大小，然后进行排序，输出到输出缓冲区，然后保存到结果文件。 如果是数字，可以用位图排序，但是要求比较苛刻： 数字不重复 知道最大值 相对密集，因为没出现的数字也会占用空间 比较适合电话号之类的。 总结和方法论架构设计题目远不止这些，我觉得主要从以下几个方面准备： 先了解常用算法，针对解决各种问题能用哪些算法，比如大文件排序用外排序，大量数据中的命中判断用位图/布隆过滤器等等。 注意扩展性、多考虑极端情况，多问自己几个为什么。比如说起单机的限流算法想想分布式的怎么做。 实在不知道怎么弄的叙述自己的思考过程，着重展示自己考虑周全、思维缜密。 比如之前一个同事面阿里被问整个机房网络挂了/断电了之类的极端问题，相信很多人不可能真的遇到这种情况，而且可用性一般也到不了这个级别。如果是我的话，我会着重考虑数据一致性、数据恢复、脏数据处理之类的问题，是否有其他机房提供服务，如果有的话涉及到网络分区，是不是可以引申谈谈 zab、raft 算法，另外原本两个或者三个机房提供服务，现在瞬间少了一个，其他机房的负载瞬间上升，怎么做削峰、降级，这就有回到我们会的问题上了。另外等到网络恢复了，怎么恢复服务？之前处理到一半的数据怎么处理？这些引申开来都有太多可以聊。 另外有其他人总结的经验如下： 如何答好面试中的系统设计题？@董飞 Cache：缓存，万金油，哪里不行优先考虑 Queue：消息队列，常见使用Linkedin的kafka Asynchronized：批处理＋异步，减少系统IO瓶颈 Load Balance: 负载均衡，可以使用一致性hash技术做到尽量少的数据迁移 Parallelization：并行计算，比如MapReduce Replication：提高可靠性，如HDFS，基于位置感知的多块拷贝 Partition：数据库sharding，通过hash取摸 如何答好面试中的系统设计题？@罗辑 交流沟通和理解能力 - 跟面试官充分交流理解所设计系统的目标，方便做设计中的tradeoff，在厂里干过的就知道日常工作中这个非常重要 设计和架构能力 - 很多我见过的面试者都只注重在这块而忽略了其他，很可惜 扩展性，容错性，延迟要求 - 跟Opeartion相关的要求，如今Dev和Ops不分家，希望面试者了解系统今后能如何扩展，易于maintain。 资源需求 - 对于我们所要求的QPS和latency，需要多少台机器，其中CPU, 内存，硬盘等资源都是如何配置 如何解决系统设计题 注意陷阱⚠️千万不要张口就来 。我们上面的分析已经表明，遇到问题，我们必须先明确问题，然后才能给出有效的解决方案，系统设计题也是如此。 而往往面试官会在这里面挖坑，就是，给出一个条件相当模糊的系统设计题。例如：请设计一个短链接系统、请设计一个压测工具。 如果我们张口就来，那么就中了面试官的套路。在工作中也是如此，如果你碰到了一个产品经理，产品经理还没有想清楚，就叫你写代码， 那么后面一定是反复修改需求，作为工程师的你一定是痛不欲生。因此，我们需要做的第一步就是 明确问题，不要张口就来。 千万不要自己一个人闷头想，记得一边想一边把你的思路说出来 由于别人是不知道你咋想什么的，如果你一个人皱着眉头思考，面试官 也很着急，他也不知道你是想不出还是怎么的。所以，一边把你的思路描述出来一边想，这是最好的方式。 明确问题拿到了问题之后，我们只有一个大概的方向，即我们要做一个什么。但是并不清楚具体条件，而不同的的业务场景所需要的架构也是不一样的。 例如，每天只有10000次访问的短链接系统和每秒钟有10000次访问的短链接系统在技术要求上是不一样的。 因此，第一个我们需要问面试官的问题： 这个系统一天的请求量是多少？面试官回答，一天2万。 虽然现在我们有了更加明确的条件，但是还是不够具体，我们需要进一步确定： 这个短链接系统的峰值是多少？面试官回答：峰值1万/s。 此刻，我们得到的条件已经清晰很多，我们知道，QPS位10k，相对来说，已经算是高并发了，但是我们还需要继续确定。 这个短链系统的短链接是永久保存的吗？面试官回答：是。 生成的锻炼字符长度有要求吗？越短越好。 录入的长链接大概是多少个字符呢？有长有短。 那么，平均是多少个字符呢？假设是50个字节。 通过这些我们可以了解到，我们需要对短链接进行持久化。而到目前为止，我们可以得出一些计算： 峰值QPS为10k 一天的请求量为20k，因此，我们的系统需要为并发考虑，所以我们很可能需要用到缓存 一天为20k，那么一年就是 20k * 365 = 7300k 如果短链接生成的链接长度为 6个字符，那么一年下来，需要的磁盘大小至少是：7300k * (6 + 50) / 1024 = 399.21875M 然后我们需要了解一些常见的性能指标，这是我们需要记在脑子里的： Nginx：能轻松的处理c100k问题，内存越大，能处理的并发量越高 Redis: https://redis.io/topics/benchmarks 表明，对于GET/SET来说，QPS 10-100k没啥大问题 MySQL: https://www.mysql.com/why-mysql/benchmarks/ 表明，对于只读，QPS 几百k没啥问题，对于写，MySQL 5.7 QPS 100k 几乎是上限 PG: https://www.percona.com/blog/2017/01/06/millions-queries-per-second-postgresql-and-mysql-peaceful-battle-at-modern-demanding-workloads/ 也是差不多 这些数据是不准确的，因为： 和怎么用关系很大 和硬件配置关系很大 但是我们心里还是要有个大概印象。 现在我们就可以知道了，这个短链系统，并发是10k/s，但是我们还漏掉了一个问题，就是没有明确是读的QPS是10k还是写，还是读+写。 这个时候我们仍然可以向面试官提问。但是，不论读还是写，QPS为10k，只要硬件配置过得去，不加缓存也ok。但是通常来说，我们还是会 选择加上，原因很简单：此刻都有QPS都有10k，保不准半年之后更高了。 到此为止，我们已经知晓了解决系统设计题的一个大概思路流程，但是这远远不够，因为系统设计题对知识的广度要求也很高，因此这里 我还会提供一些常见的知识，希望能够对大家有所帮助。 常见知识 最简单粗暴也是实践中最常用的应对方案就是：升级机器、加机器(所以架构的时候要考虑好水平扩展) 大多数应用都是读大于写，解决方案很简单：加缓存+读写分离 对于写大于读的方案，见 (关系型)数据库优化总结 Nginx：能轻松的处理c100k问题，内存越大，能处理的并发量越高 Redis: https://redis.io/topics/benchmarks 表明，对于GET/SET来说，QPS 10-100k没啥大问题 MySQL: https://www.mysql.com/why-mysql/benchmarks/ 表明，对于只读，QPS 几百k没啥问题，对于写，MySQL 5.7 QPS 100k 几乎是上限 PG: https://www.percona.com/blog/2017/01/06/millions-queries-per-second-postgresql-and-mysql-peaceful-battle-at-modern-demanding-workloads/ 也是差不多 B, M, G, T, PB之间的关系换算要清楚 需要长时间处理的任务或者是强依赖网络(而网络不确定性大的问题)，妥妥的用队列，例如消息推送 性能优化套路：加机器 - 加缓存 - 优化数据库索引 - 垂直拆数据库表 - 水平拆数据库表 - 垂直分库 - 水平分库 参考资料 这一次，彻底弄懂“秒杀系统” 淘宝大秒杀系统设计详解 百亿级微信红包的高并发资金交易系统设计方案 全面解密QQ红包技术方案：架构、技术实现、移动端优化、创新玩法等 Leaf——美团点评分布式ID生成系统 微信序列号生成器架构设计及演变 分布式定时任务（一）","categories":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"架构","slug":"架构","permalink":"http://beritra.github.com/tags/%E6%9E%B6%E6%9E%84/"}]},{"title":"缓存系统常见问题总结","slug":"缓存系统常见问题总结","date":"2020-06-11T16:21:48.000Z","updated":"2022-02-20T11:27:13.888Z","comments":true,"path":"2020/06/12/缓存系统常见问题总结/","link":"","permalink":"http://beritra.github.com/2020/06/12/%E7%BC%93%E5%AD%98%E7%B3%BB%E7%BB%9F%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/","excerpt":"关于缓存的知识太多了，先整理一点，慢慢补充吧。","text":"关于缓存的知识太多了，先整理一点，慢慢补充吧。 缓存同步策略首先需要明确，除了把整个缓存和持久化存储都做串行化，或者操作一个的时候直接锁死另一个，不然必然会有产生数据不一致的可能。选择哪种缓存策略是根据自己的业务场景进行取舍。 经常见到的一种方式，是写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。 但是这种方式也有问题，假如两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 典型的缓存模式，一般有如下几种： Cache Aside Read/Write Through Write Around Write Behind Cache Aside经常用到的一种策略模式。这种模式主要流程如下： 失效：应用程序先从 Cache 取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从 Cache 中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 这种缓存策略会在如下情况出现脏数据： 线程 A 查询一个数据，在缓存中没有找到，于是去数据库查，拿到数据之后还没放到缓存中。 这时候线程 B 过来更新这一条数据，先更新数据库之后准备令缓存失效，由于缓存还没放进来所以不需要失效。 线程 A 把数据放到了缓存中。但是这个数据其实已经是脏数据了，跟数据库中不同步。 这种情况发生概率要稍微低，因为需要在数据库的查询操作和缓存的更新操作之间，插入一个数据库的修改和缓存的修改操作，一般情况下，后者需要的时间是要远高于前者的。当然也不能忽视这种情况确实存在。 Read/Write ThroughRead/Write Through 套路是把更新数据库（Repository）的操作由缓存自己代理了，对应用层来说是透明的，应用不再需要关心数据同步问题。 Read ThroughRead Through 就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或 LRU 换出），Cache Aside 是由调用方负责把数据加载入缓存，而 Read Through 则用缓存服务自己来加载，从而对应用方是透明的。 Write ThroughWrite Through 套路和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 Cache 自己更新数据库（这是一个同步操作）。 这种方式的风险也显而易见，如果追求落库之后再返回成功，效率必然降低很多，缓存的意义就不大了。如果追求落到缓存上就算成功，那问题又抛给了缓存的丢失风险上。 Write Around这种策略下，数据直接写入数据库，只有读取的数据才能进入缓存。Write Around 可以与 Read Through 结合使用，并在数据只写一次、读取次数较少或从不读的情况下提供良好的性能。例如，实时日志或聊天室消息。同样，这个模式也可以与 Cache Aside 组合使用。 Write BehindWrite Behind 又叫 Write Back。就是 Linux 文件系统的 Page Cache 的算法。 Write Back 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的 I/O 操作飞快无比（因为直接操作内存嘛 ），因为异步，Write Back 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。 性能很强悍，问题也很明显，数据不是强一致性的，而且可能会丢失。另外 Write Behind 实际上的逻辑还比较复杂，因为需要追踪定位哪些数据需要做持久化。 常见面试题下面是几个老生常谈面经常见问题了，该背的还是得背啊… 缓存穿透缓存穿透的意思：请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。 可能引发的问题：请求全落在数据库上，数据库压力过大，直接崩了。 解决办法： 缓存空值，之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。所以我们把空值也给缓存了，下次请求就会走数据库了。要记得设定过期时间。 布隆过滤器，定义不解释了，就是可以用来判断超大数据量中单个数据的存在性问题。但是存在误差，如果布隆过滤器判断不存在，那肯定不存在，但是如果判断存在，也有一定的可能误判。另外基础的布隆过滤器没办法删除元素，需要另外的逻辑辅助处理，或者依靠其他变种。 缓存雪崩缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到数据库，数据库瞬时压力过重雪崩。 可能引发的问题：跟上面一样，数据库压力过大崩了。 一个简单方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值。 缓存击穿对于一些设置了过期时间的 key，如果这些 key 可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一 key 缓存，雪崩则是很多 key。 可能引发的问题：单个 key 过期的瞬间大量请求过来，导致数据库压力过大崩了。 解决办法： 加锁，单个请求去更新缓存就行了，别的排队等着。 双层的超时值，保存一个用于更新缓存的超时值，到达这个值之后去延长一下真是的超时时间。 热点 Key 问题缓存中的某些 Key（可能对应用与某个促销商品）对应的 value 存储在集群中一台机器，使得所有流量涌向同一机器，成为系统的瓶颈。 解决办法： 客户端缓存（多级缓存），将热点 key 对应 value 并缓存在客户端本地，并且设置一个失效时间。 将单个热点分散为多个子 key，时期请求的时候 hash 到不同的机器上处理。 热点集中失效不解释了，跟缓存雪崩一个样，就是说热点数据全都到期，然后全落到数据库上。 解决办法是过期时间加随机值，或者加互斥锁排队。 参考资料 缓存更新的套路 缓存穿透，缓存击穿，缓存雪崩解决方案分析","categories":[{"name":"架构","slug":"架构","permalink":"http://beritra.github.com/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"架构","slug":"架构","permalink":"http://beritra.github.com/tags/%E6%9E%B6%E6%9E%84/"},{"name":"缓存","slug":"缓存","permalink":"http://beritra.github.com/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"Java高级特性：泛型、反射、动态代理和注解","slug":"Java高级特性：泛型、反射、动态代理和注解","date":"2020-05-10T14:22:04.000Z","updated":"2022-02-20T11:27:13.892Z","comments":true,"path":"2020/05/10/Java高级特性：泛型、反射、动态代理和注解/","link":"","permalink":"http://beritra.github.com/2020/05/10/Java%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%EF%BC%9A%E6%B3%9B%E5%9E%8B%E3%80%81%E5%8F%8D%E5%B0%84%E3%80%81%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E5%92%8C%E6%B3%A8%E8%A7%A3/","excerpt":"Java 高级特性有挺多，但是这几个一直没搞太通透，只会简单用用，为什么这么设计，有没有什么有意思的玩法都没探究过，今天就来整理一下。","text":"Java 高级特性有挺多，但是这几个一直没搞太通透，只会简单用用，为什么这么设计，有没有什么有意思的玩法都没探究过，今天就来整理一下。 泛型说到泛型，肯定很熟悉了，我们天天用的 List: 1List&lt;String&gt; list=new ArrayList&lt;&gt;(); ArrayList就是个泛型类，我们通过设定不同的类型，可以往集合里面存储不同的数据类型（而且只能存储设定的数据类型，这是泛型的优势之一）。“泛型”简单的意思就是泛指的类型（参数化类型）。 有人灵机一动，就问这里为什么不用Object，到时候再转呐？ 问得好，在泛型出现之前，的确是这么做的。但是这样的有一个问题：如果集合里面数据很多，某一个数据转型出现错误，在编译期是无法发现的，但是在运行期会发生java.lang.ClassCastException。 泛型一方面让我们只能往集合中添加一种类型的数据，同时可以让我们在编译期就发现这些错误，避免运行时异常的发生，提升代码的健壮性。 我们可以从以下几个方面理解泛型： 泛型通配符 泛型类 泛型接口 泛型方法 泛型擦除 泛型数组 泛型符号可能有人注意到有不同的泛型符号，其实泛型可以使用任何大写字母定义，把 T 换成 A 也一样，这里 T 只是名字上的意义而已。以下是一般约定俗成的符号意义： E - Element (在集合中使用，因为集合中存放的是元素) T - Type（Java 类） K - Key（键） V - Value（值） N - Number（数值类型） ？- 表示不确定的 Java 类型 对于？类型的泛型，我们称之为通配符，又有以下三种情况： 无限通配符&lt;?&gt;无限通配符可以表示所有的类型。可能一般会有疑惑，泛型本来就具备泛化的功能，可以表示所有类型，那么&lt;T&gt;和&lt;?&gt;区别是什么？无限通配符的主要作用就是让泛型能够接受未知类型的数据。 ？和 T 都表示不确定的类型，区别在于我们可以对 T 进行操作，但是对 ？不行，比如如下这种 ： 1// 可以2T t = operate();34// 不可以5？ car = operate(); T 是用于定义的时候，而 ? 用于使用的时候，我们可以这样： 1class Test&lt;T&gt; &#123;2 T t;3&#125; 但是不可以： 1class Test&lt;?&gt; &#123;2 ? t;3&#125; 在调用的时候想法，我们可以用 ? 表示一个未知的泛型： 1Test&lt;?&gt; test = new Test&lt;&gt;(); 这时候就不能用 T 了。但是注意，这里Test&lt;?&gt;类型的test独享是不能对其属性进行赋值的，也就是说下面的操作是不允许的： 1Test&lt;?&gt; test = new Test&lt;String&gt;();2test.t=\"123\";//Incompatible types. Found: 'java.lang.String', required: 'capture&lt;?&gt;' 虽然我们创建了一个泛型类型为 String 的对象，但是不能对其赋值。可以看出Test&lt;?&gt;只是用于声明变量的时候用，你不能用它来实例化，尤其是用于当你不知道声明的泛型类型的时候。 上界通配符&lt;? extends T&gt;使用固定上边界的通配符的泛型，就能够接受指定类及其子类类型的数据。要声明使用该类通配符，采用&lt;? extends E&gt;的形式，这里的 T 就是该泛型的上边界。注意：这里虽然用的是extends关键字，却不仅限于继承了父类 T 的子类，也可以代指显现了接口 T 的类。 举个栗子，我们定义两个类，水果和苹果，水果是苹果的父类。然后定义一个泛型类： 1class Test&lt;T&gt; &#123;2&#125;34class Fruit&#123;5&#125;67class Apple extends Fruit&#123;8&#125; 测试一下上界通配符可以发现： 1Test&lt;Fruit&gt; fruit = new Test&lt;&gt;();2Test&lt;Apple&gt; apple = new Test&lt;&gt;();3Test&lt;Object&gt; object = new Test&lt;&gt;();45Test&lt;? extends Fruit&gt; newTest;6newTest=fruit;//可以7newTest=apple;//可以8newTest=object;//ide报错 下界通配符&lt;? extends T&gt;跟上界通配符是相对应的，只接受指定类及其父类，很好理解。直接看栗子，还是用苹果和水果举例： 1Test&lt;Fruit&gt; fruit = new Test&lt;&gt;();2Test&lt;Apple&gt; apple = new Test&lt;&gt;();3Test&lt;Object&gt; object = new Test&lt;&gt;();4Test&lt;String&gt; string = new Test&lt;&gt;();56Test&lt;? extends Fruit&gt; newTest;7newTest=fruit;//可以8newTest=apple;//可以9newTest=object;//可以10newTest=string;//ide报错 跟上面有一点点不同的是，由于Object是所有类的父类，所以也可以。 另外跟上界通配符不同的是，下界通配符&lt;? super T&gt;不影响往里面存储，但是读取出来的数据只能是 Object 类型。 原因是，下界通配符规定了元素最小的粒度，必须是 T 及其基类，那么我往里面存储 T 及其派生类都是可以的，因为它都可以隐式的转化为 T 类型。但是往外读就不好控制了，里面存储的都是 T 及其基类，无法转型为任何一种类型，只有 Object 基类才能装下。 最后简单介绍下 Effective Java 这本书里面介绍的 PECS 原则。 上界&lt;? extends T&gt;不能往里存，只能往外取，适合频繁往外面读取内容的场景。 下界&lt;? super T&gt;不影响往里存，但往外取只能放在Object对象里，适合经常往里面插入数据的场景。 泛型类类结构是面向对象中最基本的元素，如果我们的类需要有很好的扩展性，那么我们可以将其设置成泛型的。 泛型类定义时只需要在类名后面加上类型参数即可，当然你也可以添加多个参数，类似于&lt;K,V&gt;,&lt;T,E,K&gt;等。这样我们就可以在类里面使用定义的类型参数。当然需要注意，泛型的类型参数只能是 Object 类（包括自定义类），不能是基本类型。 泛型接口泛型接口与泛型类的定义及使用基本相同。泛型接口常被用在各种类的生产器中，如下： 1//定义一个泛型接口2public interface Generator&lt;T&gt; &#123;3 public T next();4&#125; 当实现泛型接口的类，未传入泛型实参时，需将泛型的声明也一起加到类中： 1/**2 * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中3 * 即：class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;4 * 如果不声明泛型，如：class FruitGenerator implements Generator&lt;T&gt;，编译器会报错：\"Unknown class\"5 */6class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123;7 @Override8 public T next() &#123;9 return null;10 &#125;11&#125; 在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型： 1/**2 * 传入泛型实参时：3 * 在实现类实现泛型接口时，如已将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型4 * 即：Generator&lt;T&gt;，public T next();中的的T都要替换成传入的String类型。5 */6public class FruitGenerator implements Generator&lt;String&gt; &#123;78 @Override9 public String next() &#123;10 return \"Fruit\";11 &#125;12&#125; 泛型方法在java中,泛型类的定义非常简单，但是泛型方法就比较复杂了。 泛型类，是在实例化类的时候指明泛型的具体类型；泛型方法，是在调用方法的时候指明泛型的具体类型。 需要注意的是： public与返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 &lt;T&gt;表明该方法将使用泛型类型 T，此时才可以在方法中使用泛型类型 T。 与泛型类的定义一样，此处 T 可以随便写为任意标识，常见的如 T、E、K、V 等形式的参数常用于表示泛型。 1/**2 * 泛型方法的基本介绍3 * @param tClass 传入的泛型实参4 * @return T 返回值为T类型5 */6public &lt;T&gt; T genericMethod(Class&lt;T&gt; tClass)throws InstantiationException ,7 IllegalAccessException&#123;8 T instance = tClass.newInstance();9 return instance;10&#125; 泛型类中的泛型方法如果在泛型类中声明了一个泛型方法，使用泛型 E，这种泛型 E 可以为任意类型。可以类型与 T 相同，也可以不同，比如一个最刁钻的情况： 1class Test&lt;T&gt; &#123;2 &lt;E&gt; void func1(T t) &#123;//这里传入的参数 t 跟类的泛型保持一致。所以这里的 E 是没有意义的，idea 会提示可以直接删掉。3 &#125;45 &lt;T&gt; void func2(T t) &#123;//这里的两个泛型 T 都是新的泛型，和类的泛型 T 没有关系。这么写的话很容易引起误会，所以 idea 也会提示建议重命名。6 &#125;78 &lt;E&gt; void func3(E t) &#123;//这个很清晰，E 和 T 是无关的泛型。9 &#125;10&#125; 静态方法与泛型静态方法有一种情况需要注意一下，那就是在类中的静态方法使用泛型：静态方法无法访问类上定义的泛型，如果静态方法操作的引用数据类型不确定的时候，必须要将泛型定义在方法上。 1public class Test&lt;T&gt; &#123;2 /**3 * 如果在类中定义使用泛型的静态方法，需要添加额外的泛型声明（将这个方法定义成泛型方法）4 * 即使静态方法要使用泛型类中已经声明过的泛型也不可以。5 * 如：public static void show(T t)&#123;..&#125;,此时编译器会提示错误信息：6 \"StaticGenerator cannot be refrenced from static context\"7 */8 public static &lt;T&gt; void show(T t)&#123;9 &#125;10&#125; 泛型擦除泛型擦除，或者叫泛型的类型擦除，出现的根本原因是为了保证兼容性。 泛型是 Java 1.5 版本才引进的概念，在这之前是没有泛型的概念的，但显然，泛型代码能够很好地和之前版本的代码很好地兼容。这是因为，泛型信息只存在于代码编译阶段，在进入 JVM 之前，与泛型相关的信息会被擦除掉，专业术语叫做类型擦除。 在泛型类被类型擦除的时候，之前泛型类中的类型参数部分如果没有指定上限，如&lt;T&gt;则会被转译成普通的 Object 类型，如果指定了上限如&lt;T extends String&gt;则类型参数就被替换成类型上限。 如在代码中定义的List&lt;object&gt;和List&lt;String&gt;等类型，在编译后都会编程List。JVM 看到的只是List，而由泛型附加的类型信息对 JVM 来说是不可见的。Java 编译器会在编译时尽可能的发现可能出错的地方，但是仍然无法避免在运行时刻出现类型转换异常的情况。类型擦除也是 Java 的泛型实现方法与 C++ 模版机制实现方式之间的重要区别。 泛型数组关于泛型数组，还要提醒一下，在 Java 中是“不能创建一个确切的泛型类型的数组”的。 也就是说下面的这个例子是不可以的： 1List&lt;String&gt;[] ls = new ArrayList&lt;String&gt;[10]; 而使用通配符创建泛型数组是可以的，如下面这个例子： 1List&lt;?&gt;[] ls = new ArrayList&lt;?&gt;[10]; 这样也是可以的： 1List&lt;String&gt;[] ls = new ArrayList[10]; 反射关于反射，基本的用法应该都熟悉了，可以通过全类名找到对应的类，然后实例化一个对象，还可以访问其变量，调用他的方法。甚至可以绕过private关键字的限制等等。 虽然有一点基础了解，但是一直不知道反射什么原理，为什么要设计这样一个功能。 首先我们先明确反射提供的功能： 在运行时判断任意一个对象所属的类； 在运行时构造任意一个类的对象； 在运行时判断任意一个类所具有的成员变量和方法（通过反射甚至可以调用private方法）； 在运行时调用任意一个对象的方法。 首先为什么需要有反射？我们看下动态编译与静态编译的概念： 静态编译：在编译时确定类型，绑定对象。即通过new关键字实例化一个对象。 动态编译：运行时确定类型，绑定对象。动态编译最大限度发挥了 Java 的灵活性，体现了多态的应用，有以降低类之间的藕合性。 另外不妨用大家接触最早的一个反射用例作为例子。刚接触 JDBC 的时候可能就有疑问，为什么连接数据库一定要先注册驱动，也就是用Class.forName(驱动类)来加载驱动类？ 这里的原因就是包括： 解耦：所有不同数据库的驱动类都是 JDK 提供的通用接口NonRegisteringDriver的实现，使用反射可以方便解耦，后需更换数据库驱动不需要修改代码，改个配置文件就行了。另外这里也是桥接模式的一个体现。 节省资源：我们加载驱动其实只是需要执行其静态代码块里的初始化代码，其他的都不会马上用到，所以更节省资源。 另外，其实用new com.mysql.jdbc.Driver();的方式加载驱动，也不是不可以，只不过不是最优选择。 另外，使用代理的优缺点也很明显： 优点可以实现动态创建对象和编译，体现出很大的灵活性。采用静态的话，需要把整个程序重新编译一次才可以实现功能的更新，而采用反射机制的话，它就可以不用卸载，只需要在运行时才动态的创建和编译，就可以实现该功能。 缺点对性能有影响。使用反射基本上是一种解释操作，我们可以告诉 JVM，我们希望做什么并且它满足我们的要求。这类操作总是慢于只直接执行相同的操作。 用途关于反射主要的用途，有一个知乎回答说得很好： 比如你在开发一个 xxfreamwork，后续要初始化并管理其他开发者创建的类 Y y 的对象。你肯定不会在 freamwork 开发阶段就知道将来大家定义的类名，这时候可以把类名做参数初始化对象。 像 Spring Framework 就是大量用到了反射，之前用 xml 配置 bean，Spring 框架就是通过反射来创建一个 bean 的实例，然后放到池子里进行管理。这时候就不能用 new 关键字来创建了，以为你根本不知道对方会有哪些类。 除此之外，下面我们要说的动态代理也是另外一大重要用途，这个在下面详细说。 原理对于最简单的一次反射使用样例： 1Class actionClass=Class.forName(\"MyClass\");2Object action=actionClass.newInstance();3Method method = actionClass.getMethod(\"myMethod\",null);4method.invoke(action,null); 前两行实现了类的加载、链接和初始化（newInstance方法实际上也是使用反射调用了&lt;init&gt;方法），后两行实现了从 class 对象中获取到 method 对象然后执行反射调用。 从上面的代码可以看出，如果我们自己想要实现invoke方法，其实只要实现这样一个 Method 类即可： 1Class Method&#123;2 public Object invoke(Object obj,Object[] param)&#123;3 MyClass myClass=(MyClass)obj;4 return myClass.myMethod();5 &#125;6&#125; 看起来很简单吧，那么实际上 JVM 是怎么做的呐？ 首先来看一下Method对象是如何生成的： 上面的 Class 对象是在加载类时由 JVM 构造的，JVM 为每个类管理一个独一无二的 Class 对象，这份 Class 对象里维护着该类的所有 Method，Field，Constructor 的 cache，这份 cache 也可以被称作根对象。每次getMethod获取到的 Method 对象都持有对根对象的引用，因为一些重量级的 Method 的成员变量（主要是 MethodAccessor ），我们不希望每次创建 Method 对象都要重新初始化，于是所有代表同一个方法的 Method 对象都共享着根对象的 MethodAccessor，每一次创建都会调用根对象的 copy 方法复制一份。 获取到Method对象之后，调用invoke方法的流程如下： 可以看到，调用Method.invoke之后，会直接去调MethodAccessor.invoke。MethodAccessor 就是上面提到的所有同名 method 共享的一个实例，由ReflectionFactory创建。创建机制采用了一种名为 inflation 的方式（JDK 1.4 之后）：如果该方法的累计调用次数&lt;=15，会创建出NativeMethodAccessorImpl，它的实现就是直接调用 native 方法实现反射；如果该方法的累计调用次数&gt;15，会由 Java 代码创建出字节码组装而成的MethodAccessorImpl。（是否采用 inflation 和 15 这个数字都可以在 JVM 参数中调整）。 更加细致的过程R大有一篇博文：关于反射调用方法的一个log 动态代理说道动态代理，就必须得回顾下代理模式这种设计模式了： 代理模式：给某一个对象提供一个代理，并由代理对象来控制对真实对象的访问。代理模式是一种结构型设计模式。 代理模式角色分为 3 种： Subject（抽象主题角色）：定义代理类和真实主题的公共对外方法，也是代理类代理真实主题的方法； RealSubject（真实主题角色）：真正实现业务逻辑的类； Proxy（代理主题角色）：用来代理和封装真实主题； 代理模式的结构比较简单，其核心是代理类，为了让客户端能够一致性地对待真实对象和代理对象，在代理模式中引入了抽象层。 简单来说，代理模式就在真实的角色外面包装一层代理，可以在代理方法中执行真实的方法，还可以额外做一些逻辑判断和处理。 而动态代理，就是区别于静态代理的一种代理模式实现方式。二者根据字节码的创建时机来分类： 所谓静态也就是在程序运行前就已经存在代理类的字节码文件，代理类和真实主题角色的关系在运行前就确定了。 而动态代理的源码是在程序运行期间由JVM根据反射等机制动态的生成，所以在运行前并不存在代理类的字节码文件。 静态代理我们先用更好理解的静态代理来了解一下代理的过程，然后理解静态代理的缺点，再来学习动态代理。 编写一个接口 UserService ，以及该接口的一个实现类 UserServiceImpl。 1public interface UserService &#123;2 public void select(); 3 public void update();4&#125;56public class UserServiceImpl implements UserService &#123; 7 public void select() &#123; 8 System.out.println(\"查询 select 方法\");9 &#125;10 public void update() &#123;11 System.out.println(\"更新 update 方法\");12 &#125;13&#125; 我们将通过静态代理对 UserServiceImpl 进行功能增强，在调用 select 和 update 之前记录一些日志。写一个代理类 UserServiceProxy，代理类需要实现 UserService： 1public class UserServiceProxy implements UserService &#123;2 private UserService target; // 被代理的对象34 public UserServiceProxy(UserService target) &#123;5 this.target = target;6 &#125;7 public void select() &#123;8 before();9 target.select(); // 这里才实际调用真实主题角色的方法10 after();11 &#125;12 public void update() &#123;13 before();14 target.update(); // 这里才实际调用真实主题角色的方法15 after();16 &#125;1718 private void before() &#123; // 在执行方法之前执行19 System.out.println(String.format(\"log start time [%s] \", new Date()));20 &#125;21 private void after() &#123; // 在执行方法之后执行22 System.out.println(String.format(\"log end time [%s] \", new Date()));23 &#125;24&#125; 通过静态代理，我们达到了功能增强的目的，而且没有侵入原代码，这是静态代理的一个优点。 虽然静态代理实现简单，且不侵入原代码，但是，当场景稍微复杂一些的时候，静态代理的缺点也会暴露出来。 1、当需要代理多个类的时候，由于代理对象要实现与目标对象一致的接口，有两种方式： 只维护一个代理类，由这个代理类实现多个接口，但是这样就导致代理类过于庞大； 新建多个代理类，每个目标对象对应一个代理类，但是这样会产生过多的代理类。 2、 当接口需要增加、删除、修改方法的时候，目标对象与代理类都要同时修改，不易维护。 如何改进？就是使用动态代理。动态代理就是想办法，根据接口或目标对象，计算出代理类的字节码，然后再加载到 JVM 中使用。 动态代理常见的字节码操作类库有如下几种： 这里有一些介绍：java-source.net/open-source… Apache BCEL (Byte Code Engineering Library)：是 Java classworking 广泛使用的一种框架，它可以深入到 JVM 汇编语言进行类操作的细节。 ObjectWeb ASM：是一个Java字节码操作框架。它可以用于直接以二进制形式动态生成 stub 根类或其他代理类，或者在加载时动态修改类。 CGLib(Code Generation Library)：是一个功能强大，高性能和高质量的代码生成库，用于扩展 JAVA 类并在运行时实现接口。 Javassist：是 Java 的加载时反射系统，它是一个用于在 Java 中编辑字节码的类库；它使 Java 程序能够在运行时定义新类，并在 JVM 加载之前修改类文件。 … 为了让生成的代理类与目标对象（真实主题角色）保持一致性，实际使用中我们最常见的两种实现方式是： 通过实现接口的方式 -&gt; JDK动态代理 通过继承类的方式 -&gt; CGLib动态代理 JDK 动态代理JDK 动态代理主要涉及两个类：java.lang.reflect.Proxy 和 java.lang.reflect.InvocationHandler。还是以上面的例子，我们用动态代理的方式实现对 UserService 的日志记录。 1public class LogProxy implements InvocationHandler &#123;2 Object target;//被代理的对象，实际的方法执行者。34 public LogProxy(Object target) &#123;5 this.target = target;6 &#125;78 // 调用invoke方法之前执行9 private void before() &#123;10 System.out.println(String.format(\"log start time [%s] \", new Date()));11 &#125;1213 // 调用invoke方法之后执行14 private void after() &#123;15 System.out.println(String.format(\"log end time [%s] \", new Date()));16 &#125;1718 @Override19 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123;20 before();21 Object result = method.invoke(target, args); // 调用 target 的 method 方法22 after();23 return result; // 返回方法的执行结果24 &#125;25&#125; 这个就是日志记录代理类了，他的内部变量 target 是实际执行方法的对象，我们在执行对象的前后添加了日志记录方法。不同于静态代理中直接调用对象的方法，基于 JDK 的动态代理是利用反射来执行相应的方法。 执行代理的步骤如下： 1 // 1. 创建被代理的对象，UserService接口的实现类2 UserServiceImpl userServiceImpl = new UserServiceImpl();34 // 2. 获取对应的 ClassLoader5 ClassLoader classLoader = userServiceImpl.getClass().getClassLoader();67 // 3. 获取所有接口的Class，这里的UserServiceImpl只实现了一个接口UserService，8 Class&lt;?&gt;[] interfaces = userServiceImpl.getClass().getInterfaces();910 // 4. 创建一个将传给代理类的调用请求处理器，处理所有的代理对象上的方法调用。这里创建的是一个自定义的日志处理器，须传入实际的执行对象 userServiceImpl11 InvocationHandler logHandler = new LogProxy(userServiceImpl);12 /*13 5.根据上面提供的信息，创建代理对象。在这个过程中：14 a.JDK会通过根据传入的参数信息动态地在内存中创建和.class 文件等同的字节码15 b.然后根据相应的字节码转换成对应的class，16 c.然后调用newInstance()创建代理实例17*/18 UserService proxy = (UserService) Proxy.newProxyInstance(classLoader, interfaces, logHandler);19 // 调用代理的方法20 proxy.select();21 proxy.update(); JDK 动态代理最主要的几个方法如下： java.lang.reflect.InvocationHandler Object invoke(Object proxy, Method method, Object[] args) 定义了代理对象调用方法时希望执行的动作，用于集中处理在动态代理类对象上的方法调用 java.lang.reflect.Proxy static InvocationHandler getInvocationHandler(Object proxy) 用于获取指定代理对象所关联的调用处理器 static Class getProxyClass(ClassLoader loader, Class... interfaces) 返回指定接口的代理类 static Object newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h) 构造实现指定接口的代理类的一个新实例，所有方法会调用给定处理器对象的 invoke 方法 static boolean isProxyClass(Class cl) 返回 cl 是否为一个代理类 在newProxyInstance中顺着代码可以看到整个动态代理的流程，简单来说就是对参数进行校验，然后生成一个代理类的字节码文件，如果你修改 JVM 参数jdk.proxy.ProxyGenerator.saveGeneratedFiles为 true 的话，还可以保存生成的字节码文件。 打印字节码文件我们可以看到生成的文件结构： 1public final class $Proxy0 extends Proxy implements UserService &#123;2 static &#123;3 try &#123;4 m1 = Class.forName(\"java.lang.Object\").getMethod(\"equals\", Class.forName(\"java.lang.Object\"));5 m2 = Class.forName(\"java.lang.Object\").getMethod(\"toString\");6 m4 = Class.forName(\"com.beritra.jdk.proxy.UserService\").getMethod(\"select\");7 m0 = Class.forName(\"java.lang.Object\").getMethod(\"hashCode\");8 m3 = Class.forName(\"com.beritra.jdk.proxy.UserService\").getMethod(\"update\");9 &#125; catch (NoSuchMethodException var2) &#123;10 throw new NoSuchMethodError(var2.getMessage());11 &#125; catch (ClassNotFoundException var3) &#123;12 throw new NoClassDefFoundError(var3.getMessage());13 &#125;14 &#125;15 public final void update() throws &#123;16 try &#123;17 super.h.invoke(this, m3, (Object[])null);18 &#125; catch (RuntimeException | Error var2) &#123;19 throw var2;20 &#125; catch (Throwable var3) &#123;21 throw new UndeclaredThrowableException(var3);22 &#125;23 &#125;24 public final void select() throws &#123;25 try &#123;26 super.h.invoke(this, m4, (Object[])null);27 &#125; catch (RuntimeException | Error var2) &#123;28 throw var2;29 &#125; catch (Throwable var3) &#123;30 throw new UndeclaredThrowableException(var3);31 &#125;32 &#125;33 //其他部分没贴34&#125; 从这个生成的代理的代码中我们可以发现： 继承了 Proxy 类，并且实现了被代理的所有接口，以及 equals、hashCode、toString 等方法 由于继承了 Proxy 类，所以每个代理类都会关联一个 InvocationHandler 方法调用处理器 类和所有方法都被 public final 修饰，所以代理类只可被使用，不可以再被继承 每个方法都有一个 Method 对象来描述，Method 对象在static静态代码块中创建，以 m + 数字 的格式命名 调用方法的时候通过 super.h.invoke(this, m4, (Object[])null); 调用，其中的 super.h.invoke 实际上是在创建代理的时候传递给 Proxy.newProxyInstance 的 LogHandler 对象，它继承 InvocationHandler 类，负责实际的调用处理逻辑。 而 LogHandler 的 invoke 方法接收到 method、args 等参数后，进行一些处理，然后通过反射让被代理的对象 target 执行方法 流程如下： CGLib 动态代理在 maven 依赖中加入 CGLib 的库： 1&lt;!-- https://mvnrepository.com/artifact/CGLib/CGLib --&gt;2&lt;dependency&gt;3 &lt;groupId&gt;CGLib&lt;/groupId&gt;4 &lt;artifactId&gt;CGLib&lt;/artifactId&gt;5 &lt;version&gt;3.3.0&lt;/version&gt;6&lt;/dependency&gt; 如果我们用 CGLib 的方式实现动态代理，代码更简单一点。还是跟上面 JDK 动态代理类似的例子，我们复用上面的 UserService 和 UserServiceImpl 两个类，但是重新写代理： 1public class LogInterceptor implements MethodInterceptor &#123;2 /**3 * @param object 表示要进行增强的对象4 * @param method 表示拦截的方法5 * @param objects 数组表示参数列表，基本数据类型需要传入其包装类型，如int--&gt;Integer、long-Long、double--&gt;Double6 * @param methodProxy 表示对方法的代理，invokeSuper方法表示对被代理对象方法的调用7 * @return 执行结果8 * @throws Throwable9 */10 @Override11 public Object intercept(Object object, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123;12 before();13 Object result = methodProxy.invokeSuper(object, objects); // 注意这里是调用 invokeSuper 而不是 invoke，否则死循环，methodProxy.invokesuper执行的是原始类的方法，method.invoke执行的是子类的方法14 after();15 return result;16 &#125;1718 private void before() &#123;19 System.out.println(String.format(\"log start time [%s] \", new Date()));20 &#125;2122 private void after() &#123;23 System.out.println(String.format(\"log end time [%s] \", new Date()));24 &#125;25&#125; 然后调用的时候： 1LogInterceptor logInterceptor= new LogInterceptor();2Enhancer enhancer = new Enhancer();3enhancer.setSuperclass(UserServiceImpl.class); // 设置超类，CGLib是通过继承来实现的4enhancer.setCallback(logInterceptor);56UserService service = (UserService) enhancer.create(); // 创建代理类7service.update();8service.select(); 执行代码实现了类似的效果。CGLib 还提供了更多的功能，比如我们实现 CallbackFilter 接口的话，可以执行回调。 CGLib 创建动态代理类的模式是： 查找目标类上的所有非 final 的 public 类型的方法定义； 将这些方法的定义转换成字节码； 将组成的字节码转换成相应的代理的 class 对象； 实现 MethodInterceptor 接口，用来处理对代理类上所有方法的请求。 JDK 动态代理与 CGLib 动态代理对比JDK 动态代理：基于 Java 反射机制实现，必须要实现了接口的业务类才能用这种办法生成代理对象。 CGLib 动态代理：基于 ASM 机制实现，通过生成业务类的子类作为代理类，所以代理的类不能是 final 修饰的。 JDK Proxy 的优势： 最小化依赖关系，减少依赖意味着简化开发和维护，JDK 本身的支持，可能比 CGLib 更加可靠。 平滑进行 JDK 版本升级，而字节码类库通常需要进行更新以保证在新版 Java 上能够使用。 代码实现简单。 基于类似 CGLib 框架的优势： 无需实现接口，达到代理类无侵入。 只操作我们关心的类，而不必为其他相关类增加工作量。 高性能。 Java 动态代理适合于那些有接口抽象的类代理，而 CGLib 则适合那些没有接口抽象的类代理。 关于二者的效率区别，有一条博客这么说： 1、CGLib 底层采用 ASM 字节码生成框架，使用字节码技术生成代理类，在 jdk6 之前比使用 Java 反射效率要高。唯一需要注意的是，CGLib 不能对声明为 final 的方法进行代理，因为 CGLib 原理是动态生成被代理类的子类。 2、在 jdk6、jdk7、jdk8 逐步对 JDK 动态代理优化之后，在调用次数较少的情况下，JDK 代理效率高于 CGLib 代理效率，只有当进行大量调用的时候，jdk6 和 jdk7 比 CGLib 代理效率低一点，但是到 jdk8 的时候，jdk 代理效率高于 CGLib 代理。 Spring 框架怎么对二者进行选择的？ 当 Bean 实现接口时，Spring 就会用 JDK 的动态代理。 当 Bean 没有实现接口时，Spring 使用 CGlib 实现。 可以强制使用 CGlib（在 spring 配置中加入&lt;aop:aspectj-autoproxy proxy-target-class=&quot;true&quot;/&gt;）。 注解注解（Annotation）在 JDK 1.5 之后增加的一个新特性，注解的引入意义很大，有很多非常有名的框架，比如 Hibernate、Spring 等框架中都大量使用注解。注解对于开发人员来讲既熟悉又陌生，熟悉是因为只要你是做开发，都会用到注解（常见的@Override）。陌生是因为即使不使用注解也照常能够进行开发，注解不是必须的。 本质Java.lang.annotation.Annotation接口中有这么一句话，用来描述注解。 The common interface extended by all annotation types 所有的注解类型都继承自这个普通的接口（Annotation） 这句话有点抽象，但却说出了注解的本质。我们看一个 JDK 内置注解的定义： 1@Target(ElementType.METHOD)2@Retention(RetentionPolicy.SOURCE)3public @interface Override &#123;4&#125; 其实这个注解的本质就是： 1public interface Override extends Annotation&#123; 2&#125; 只不过是继承了Annotation接口的接口。如果想验证，你可以去反编译任意一个注解类，就会得到相同的结论。 所以注解说白了就是一个标签，甚至是一种特殊的注释，他本身不起作用，没有功能，需要额外的工具进行解析，实现它的功能。 解析一个类或者方法的注解往往有两种形式，一种是编译期直接的扫描，一种是运行期反射。反射的方式后面详细叙述，而编译器的扫描指的是编译器在对 Java 代码编译字节码的过程中，会检测到某个类或者方法被一些注解修饰，这时它就会对于这些注解进行某些处理。 典型的就是注解 @Override，一旦编译器检测到某个方法被修饰了 @Override 注解，编译器就会检查当前方法的方法签名是否真正重写了父类的某个方法，也就是比较父类中是否具有一个同样的方法签名。 这一种情况只适用于那些编译器已经熟知的注解类，比如 JDK 内置的几个注解，而你自定义的注解，编译器是不知道你这个注解的作用的，当然也不知道该如何处理，往往只是会根据该注解的作用范围来选择是否编译进字节码文件，仅此而已。 元注解什么东西只要一带上“元”就瞬间高大上了起来，类似“元数据”的意思是用来描述数据的数据。“元注解”就是用来修饰注解的注解，通常用在注解的定义上。 还是看 @Override 的定义： 1@Target(ElementType.METHOD)2@Retention(RetentionPolicy.SOURCE)3public @interface Override &#123;4&#125; 其中的 @Target，@Retention 两个注解就是元注解。 JAVA 中有以下几个元注解： @Target：注解的作用目标 @Retention：注解的生命周期 @Documented：注解是否应当被包含在 JavaDoc 文档中 @Inherited：是否允许子类继承该注解 @Target 用于指明被修饰的注解最终可以作用的目标是谁，也就是指明，你的注解到底是用来修饰方法的？修饰类的？还是用来修饰字段属性的。一共有以下几个属性： 被这个 @Target 注解修饰的注解将只能作用在成员字段上，不能用于修饰方法或者类。他的值 ElementType 是一个枚举类型，有以下一些值： ElementType.TYPE：允许被修饰的注解作用在类、接口和枚举上 ElementType.FIELD：允许作用在属性字段上 ElementType.METHOD：允许作用在方法上 ElementType.PARAMETER：允许作用在方法参数上 ElementType.CONSTRUCTOR：允许作用在构造器上 ElementType.LOCAL_VARIABLE：允许作用在本地局部变量上 ElementType.ANNOTATION_TYPE：允许作用在注解上 ElementType.PACKAGE：允许作用在包上 @Retention 用于指明当前注解的生命周期，他的值 RetentionPolicy 也是枚举类型，包括以下几种： RetentionPolicy.SOURCE：当前注解编译期可见，不会写入 class 文件 RetentionPolicy.CLASS：类加载阶段丢弃，会写入 class 文件 RetentionPolicy.RUNTIME：永久保存，可以反射获取 剩下两种类型的注解我们很少用，也比较简单。 @Documented 注解修饰的注解，当我们执行 JavaDoc 文档打包时会被保存进 doc 文档，反之将在打包时丢弃。 @Inherited 注解修饰的注解是具有可继承性的，也就说我们的注解修饰了一个类，而该类的子类将自动继承父类的该注解。 写一个注解现在我们尝试自己写一个注解，以一个最简单的为例，假设我们写的注解叫PrintMethods，作用在类上，作用就是打印这个类所有的方法。然后仿照官方的注解定义该注解如下： 1@Documented2@Target(ElementType.TYPE)3@Retention(RetentionPolicy.RUNTIME)4public @interface PrintMethods &#123;5&#125; 然后找一个测试类加上注解： 1@PrintMethods2public class AnnotationTest &#123;34 public static void main(String[] args) &#123;5 AnnotationTest main = new AnnotationTest();6 main.print();7 &#125;89 private void print() &#123;10 System.out.println(\"print\");11 &#125;12&#125; 执行一下，看看发生了什么。 答案是什么都没发生。之前说过了，注解就像一个标签，本身没什么功能。我们需要手动扫描注解： 1Class&lt;?&gt; clazz = AnnotationTest.class;2Annotation annotation = clazz.getAnnotation(PrintMethods.class);3if (annotation != null) &#123;4 for (Method method : clazz.getDeclaredMethods()) &#123;5 System.out.println(method.getName());6 &#125;7&#125; else8 System.out.println(\"No Annotation\"); 执行这段代码，就会发现AnnotationTest这个类中的注解被顺利的打印了出来，包括main和print两个方法。 其实在框架中也是这样的，比如 SpringBoot 的@Componen注解，把一个类标注为 bean，让 Spring 去管理，原理就是我们先通过@ComponentScan注解指定了包，然后 Spring 去把所有包下面的类都扫描一遍，然后找到带有@Componen注解的，然后进行后续处理。 参考文章： Java 泛型详解-绝对是对泛型方法讲解最详细的，没有之一 java泛型（二）、泛型的内部原理：类型擦除以及类型擦除带来的问题 Java基础与提高干货系列——Java反射机制 Java反射原理简析 Java 动态代理详解 JAVA 注解的基本原理","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"}]},{"title":"Minio+Nginx搭建图床一键上传博客图片","slug":"Minio-Nginx搭建图床一键上传博客图片","date":"2020-05-09T18:27:39.000Z","updated":"2022-02-20T11:27:13.888Z","comments":true,"path":"2020/05/10/Minio-Nginx搭建图床一键上传博客图片/","link":"","permalink":"http://beritra.github.com/2020/05/10/Minio-Nginx%E6%90%AD%E5%BB%BA%E5%9B%BE%E5%BA%8A%E4%B8%80%E9%94%AE%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2%E5%9B%BE%E7%89%87/","excerpt":"本方案要求：有一台有公网 IP 的服务器，可以是云服务器或者有公网 IP 的家庭网络。 如果事先了解 Docker、Nginx 等可能会更方便。","text":"本方案要求：有一台有公网 IP 的服务器，可以是云服务器或者有公网 IP 的家庭网络。 如果事先了解 Docker、Nginx 等可能会更方便。 前因写博客的时候一大痛点就是图片的处理，MarkDown 格式好用是好用，但是没办法直接附带图片信息，只能放一个链接，图片存在本地的话发给别人的文档里图就没了，存在网上就涉及图床的选择问题，又是另一个麻烦事。大家分享出来的解决方案也是五花八门，有用公有云块存储的，有用收费图床的，前些年还有用微博当图床的，现在微博禁止外链了该方案也不行了，还有其他奇技淫巧比如放到 GitHub 上。 作为不折腾会死星人+对广大乱七八糟免费服务抱着怀疑心态，我决定还是自己搭建，牺牲一点点稳定性和易用性（其实故障率也没多高）。自己的宽带是北京联通，恰好有公网 IP，只不过不是固定的，之前找了个 ddns 的脚本自己更新 IP 倒也没多大问题。 开始我使用的方案是用的一个叫 Lychee 的图床，功能简单，用起来感觉还行，就是跟写博客的流程不能很好的配合，往往插入图片的时候步骤是这样的： 准备好图片，剪切板上的话先保存到本地，网络图片粘贴一下url 打开 Lychee，选择相应的方式上传 上传完毕之后点开图片，右键选择图片地址 把地址粘回 Typora 想要插入图片的位置 可以看出来这个流程太蛋疼了，插入一张图片就得十几秒，这一点都不 geek。 针对这个问题，Typora 最近更新了个功能，就是检测到插入图片之后会自动上传到指定的位置，可以是几种内置的服务，也可以自己写脚本。这种方式大大减少了插入图片的工作量，无论是剪切板上的图，还是网图，你只要粘贴到 Typora 上，就可以自动上传到图床，然后把图床中的链接替换掉开始的链接。这个功能的详细介绍可以去官网查看，或者看其他博主的介绍。下面分享下我用 Minio+Python 脚本实现的整个流程。 MinioMinio 是亚马逊开源的一个文件存储系统，官方介绍这么说： MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。 他的 API 非常简洁，而且如果你要求高的话，很方便做高可用，又提供了官方 Docker 镜像和一个简单的管理界面，所以我选择了他作为图床。 废话少说，直接贴我的 Docker 命令： 1docker run -itd -p 9000:9000 --name minio -v ~/docker/minio/data:/data -v ~/docker/minio/config:/root/.minio -e \"MINIO_ACCESS_KEY=你的账号\" -e \"MINIO_SECRET_KEY=你的密码\" minio/minio server /data 把里面 data 和 config 文件夹改成你想要存储数据和配置文件的文件夹，然后把账号密码填进去就行了。 如果想用 docker-compose 或者 k8s 启动，请参考官方文档，挺详细的而且竟然还有中文版。 启动之后，打开 9000 端口之后就可以看到自带的管理界面了。 Minio 的存储逻辑很简单，分成多个 bucket（桶），没个桶内部就是以文件夹为层级，而且所有文件是直接存储在文件系统上的，没有分成小文件块之类的，因此你可以直接复制到你的 data 文件夹内。 但是，你没办法直接用链接访问放在 Minio 中的文件，虽然它提供了分享等功能，但是提供的是一个下载链接，我们需要的是类似于图片预览，因此需要一个 Nginx 作为文件服务器。 NginxNginx 就不用赘述了，我也是用 Docker 启动的，命令如下： 1docker run -itd --name image -p 8081:80 -v ~/docker/image/conf:/etc/nginx/conf.d -v ~/docker/minio/data:/data nginx 可以看到，我把 Minio 的数据文件夹挂载进去了，挂载到了 Nginx 容器的/data目录。然后修改 Nginx 配置文件如下： 1server &#123;2 listen 80;3 server_name localhost;4 charset koi8-r;5 access_log /var/log/nginx/host.access.log main;6 location ^~ /blog/ &#123;7 root /data/;8 &#125;9&#125; 这里，我是在 Minio 中创建了一个桶，名字叫 blog，它其实对应着 Minio 存储目录中的 blog文件夹。配置完 Nginx 之后，其实我如果想查看 Minio 中 blog 桶中的/testpath/test.png，只需要访问http://localhost:8081/blog/testpath/test.png就可以了。 于是只要我向 blog 这个桶中上传文件，文件就可以直接通过 Nginx 预览了。 Typora 和 Python 脚本现在只需要准备一个脚本，让 Typora 能够向 Minio 上传文件就行了。选择 Python 是因为 Minio 官方提供了 API，而且我正好熟悉。 Typora 上传的规则是这样，它会让你填写你的命令，比如你填写python upload.py，它就会执行python upload.py &quot;image1-path&quot; &quot;image2-path&quot;等等把图片位置作为参数穿进去。然后约定好需要你传回来上传完毕后的每张图片位置，每张图片占一行。比如上面的例子，传了两张，它就会自动从后往前查两行，作为 URL 替换掉原本的图片链接。 于是我写了个脚本如下，写的比较糙，可以作为参考： 1import os2import time3import uuid4import sys5import requests6from minio import Minio7from minio.error import ResponseError8import warnings910warnings.filterwarnings('ignore')11images = sys.argv[1:]12minioClient = Minio(\"这里写你的Minio地址，格式为域名：端口，不带http://\",13 access_key='你的Minio账号', secret_key='你的Minio密码', secure=False)//secure为True的话第一项会填充为https://14result = \"Upload Success:\\n\"15date = time.strftime(\"%Y%m\", time.localtime())161718def download(image_url):19 local_path = os.getcwd() + \"/temp\"20 r = requests.get(image_url, verify=False)21 with open(local_path, \"wb\") as code:22 code.write(r.content)23 return local_path242526for image in images:27 if os.path.isfile(image):28 file_type = os.path.splitext(image)[-1]29 new_file_name = str(uuid.uuid1()).replace('-', '') + file_type30 elif image.startswith(\"https://\") or image.startswith(\"http://\")://处理网络图片31 if image.endswith(\".png\") or image.endswith(\".jpg\") or image.endswith(\".jpeg\") or image.endswith(\".gif\"):32 url = image.split(\"/\")33 if len(url) &gt; 1:34 image = download(image)//先把网图下载到本地了，然后在传到Minio35 new_file_name = url[-1]36 else:37 result = result + \"error:parsing image error!\"38 continue39 else:40 result = result + \"error:parsing image error!\"41 continue42 else:43 result = result + \"error:parsing image error!\"44 continue45 try:46 minioClient.fput_object(bucket_name='blog', object_name=date + \"/\" + new_file_name, file_path=image)47 if image.endswith(\"temp\"):48 os.remove(image)49 result = result +\"你的Nginx地址，比如http://yourdomain:8081\" + \"/blog/\" + date + \"/\" + new_file_name + \"\\n\"50 except ResponseError as err:51 result = result + \"error:\" + err.message + \"\\n\"5253print(result) 我是把博客图片都放到了 blog 这个桶，然后按照月份创建文件夹，每个月的都放在同一个文件夹，文件名是用的 uuid。具体的逻辑想要修改的话自己改上面的脚本吧。 搞定之后在 Typora 里面填上脚本，然后点击测试，就可以看看结果了，测试通过就万事大吉了，写个博客插入图片看看吧！","categories":[{"name":"Blog","slug":"Blog","permalink":"http://beritra.github.com/categories/Blog/"}],"tags":[{"name":"Blog","slug":"Blog","permalink":"http://beritra.github.com/tags/Blog/"},{"name":"Typora","slug":"Typora","permalink":"http://beritra.github.com/tags/Typora/"}]},{"title":"SpringBoot中的事务","slug":"SpringBoot中的事务","date":"2020-04-21T15:13:37.000Z","updated":"2022-02-20T11:27:13.896Z","comments":true,"path":"2020/04/21/SpringBoot中的事务/","link":"","permalink":"http://beritra.github.com/2020/04/21/SpringBoot%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1/","excerpt":"SpringBoot 是怎么处理事务？事务注解怎么用？有哪些需要注意的问题？我们自己动手一步一步的从实验中学习。","text":"SpringBoot 是怎么处理事务？事务注解怎么用？有哪些需要注意的问题？我们自己动手一步一步的从实验中学习。 概览众所周知 SpringBoot 支持声明式事务和编程式事务，本文讨论的是基于声明式事务，或者叫注解式事务。 为了方便同步，文章中的代码用到了以下框架和类库： spring-boot-starter spring-boot-starter-web mybatis-plus-boot-starter mysql-connector-java 首先我们复习下前置知识点。 数据库事务有 ACID 四大原则： 原子性（Atomicity）指事务包含的所有操作要么全部成功，要么全部失败回滚，这和前面两篇博客介绍事务的功能是一样的概念，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency）指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 隔离性（Isolation）隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 持久性（Durability）指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 还有数据库经常出现的几种错误情况： 脏读：是指在一个事务处理过程里读取了另一个未提交的事务中的数据。比如一个人的名字叫张三，事务 A 这时候修改为了李四，但是没有提交事务，事务 B 此时读取姓名，如果读到了张三，就产生了脏读，一旦事务 A 回滚了，B 读到的李四就是脏数据。 不可重复读：是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。比如事务 A，读取id=1的人的名字，得到是张三，这时候事务 B 把id=1的人的名字改为了李四，并且提交了事务。这时候事务 A 再查询一遍id=1的人的名字，发现变成了李四。 幻读(虚读)：幻读是事务非独立执行时发生的一种现象，是指操作记录数的变化。幻读的定义其实经常引起争议。 高性能 mysql 中这么定义幻读： 事务A读取某个范围的记录，事务B在该范围插入了新的记录，事务A再次读取该范围的记录，会产生幻行。 InnoDB 使用 mvcc 解决了幻读的问题。 在这种意义下，InnoDB 中的 Repeatable Read 隔离级别是直接解决了幻读的，但是还有如下情况： 事务 A 对先查询一条不存在的数据，结果显示为空，然后插入该数据，在事务 A 插入之前，事务 B 率先插入了数据，导致事务 A 插入失败，就现了幻觉。也有人将这种情况称之为幻读，但是我觉得高性能 MySQL 中的定义更准确，另外后面这种情况可以用for update解决。 然后就是事务的隔离级别，MySQL 中的隔离级别和 SpringBoot 是一样的（或者应该反过来说），为了解决以上三种问题，有四种不同的隔离级别，具体作用如图： Transactional 注解Spring 事务最简单方便的使用方式是使用Transactional注解，可以被应用于接口定义和接口方法、类定义和类的 public 方法上。但是不推荐用在接口上面，因为一旦标注在 Interface 上并且配置了Spring AOP 使用 CGLib 动态代理，将会导致Transactional注解失效。 Transactional 注解的原理是这样： 在应用系统调用声明@Transactional的目标方法时，Spring Framework 默认使用 AOP 代理，在代码运行时生成一个代理对象，根据@Transactional的属性配置信息，这个代理对象决定该声明@Transactional的目标方法是否由拦截器 TransactionInterceptor来使用拦截，在TransactionInterceptor拦截时，会在在目标方法开始执行之前创建并加入事务，并执行目标方法的逻辑, 最后根据执行情况是否出现异常，利用抽象事务管理器AbstractPlatformTransactionManager操作数据源 DataSource 提交或回滚事务。 注解的所有属性如下： 第一个属性value的作用是：当在配置文件中有多个TransactionManager，可以用该属性指定选择哪个事务管理器。 事务的传播和隔离级别下面详细叙述。 readOnly和timeout都很好理解。 最后面的四个是指定事务回滚规则，事务回滚规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，只有未检查异常（RuntimeException和Error类型的异常）会导致事务回滚。 事务的隔离级别Spring 事务支持四种隔离级别： ISOLATION_DEFAULT： 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与 JDBC 的隔离级别相对应。MySQL 默认的事务处理级别是REPEATABLE-READ。 ISOLATION_READ_UNCOMMITTED： 这是事务最低的隔离级别，它充许令外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 ISOLATION_READ_COMMITTED：保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。这种隔离级别有可能产生不可重复读和幻读。 ISOLATION_REPEATABLE_READ： 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。 ISOLATION_SERIALIZABLE： 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻像读。 好，原理看完了，开始实验。SpringBoot 的项目搭建就不赘述了，我们先准备一个表： 图省事只有三个字段，自增主键id和姓名年龄。 我们先看默认的情况，添加一个开启事务的插入操作： 1@Transactional()2public void testInsert() &#123;3 TestTable data = new TestTable();4 data.setName(\"张三\");5 data.setAge(20);6 int result = testTableMapper.insert(data);7 if (result == 1)8 logger.info(\"添加数据&#123;&#125;成功\", data);9 throw new RuntimeException(\"RuntimeException\");10&#125; 可以看到日志提示添加成功，然后去数据库里没有这条，因为抛出异常回滚了。 如果在这个事务已经添加，但是没提交的情况下，另一个会话去读的话会发生什么？ 我们添加一个读去全部数据的方法： 1public void testSelect() &#123;2 List&lt;TestTable&gt; list = testTableMapper.selectList(Wrappers.emptyWrapper());3 logger.info(\"读取数据：&#123;&#125;\", list.toString());4&#125; 然后在上面那个插入的方法中添加线程休眠，模拟事务未提交的情况。 1@Transactional()2public void testInsert() &#123;3 TestTable data = new TestTable();4 data.setName(\"张三\");5 data.setAge(20);6 int result = testTableMapper.insert(data);7 if (result == 1)8 logger.info(\"添加数据&#123;&#125;成功\", data);9 try &#123;10 Thread.sleep(10000000);11 &#125; catch (InterruptedException e) &#123;12 e.printStackTrace();13 &#125;14 throw new RuntimeException(\"RuntimeException\");15&#125; 这时候你会发现，不光第一个插入的方法阻塞了，查询的方法也阻塞了。这是因为虽然你没开启事务，但是数据库实际上用了默认的隔离级别。 我们切换到最低的隔离级别：READ_UNCOMMITTED。就是说一个事务可以看到另一个事务未提交的数据。 把查询方法也加上事务，并且切换隔离级别： 1@Transactional(isolation = Isolation.READ_UNCOMMITTED)2public void testSelect() &#123;3 List&lt;TestTable&gt; list = testTableMapper.selectList(Wrappers.emptyWrapper());4 logger.info(\"读取数据：&#123;&#125;\", list.toString());5&#125; 可以看到，这次就读到了插入事务里未提交的张三。READ_UNCOMMITTED作为级别最低的隔离级别，一般很少会用。 然后我们来测试READ_COMMITTED，还是上面完全一样的代码，把查询方法的隔离级别改成READ_COMMITTED，就会发现张三看不到了。 1@Transactional(isolation = Isolation.READ_COMMITTED)2public void testSelect() &#123;3 List&lt;TestTable&gt; list = testTableMapper.selectList(Wrappers.emptyWrapper());4 logger.info(\"读取数据：&#123;&#125;\", list.toString());5&#125; 然后我们插入一条张三的数据： 写一个更新的方法，把张三改名为李四： 1@Transactional2public void testUpdate() &#123;3 int result = testTableMapper.update(null, Wrappers.lambdaUpdate(TestTable.class)4 .set(TestTable::getName,\"李四\").eq(TestTable::getName, \"张三\"));5 if (result == 1)6 logger.info(\"更新数据成功\");7&#125; 我们把查询方法查询两次，分别打印查询到的数据，中间休眠五秒，休眠期间执行更新方法，看看两次有什么区别（注意，要把 mybatis-plus 的一个配置local-cache-scope改成statement，不然通过 id 查询数据的话，第二次会直接走缓存，没有实际去数据库查。）： 1@Transactional(isolation = Isolation.READ_COMMITTED)2public void testSelect() &#123;3 TestTable data = testTableMapper.selectById(1);4 logger.info(\"读取数据：&#123;&#125;\", data);5 try &#123;6 Thread.sleep(3000);7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 data = testTableMapper.selectById(1);11 logger.info(\"读取数据：&#123;&#125;\", data);12&#125; 从日志里可以看出，查询方法第一次查询的时候，名字还是张三，执行更新方法之后，就变成了李四。也就是说，在事务还没提交的时候，受到了其他事务的影响。 当我们把查询方法的隔离级别改成REPEATABLE_READ的时候，这个问题就不复存在了，你会发现两次查询都是张三。 由于实质上的幻读已经被 InnoDB 干掉了，我们就先不看怎么用SERIALIZABLE级别来解决幻读了，而是直接看SERIALIZABLE级别能做什么吧。 这个代码比较简单 1@Transactional(isolation = Isolation.SERIALIZABLE)2public void testSelect() &#123;3 logger.info(\"进入testSelect方法\");4 List&lt;TestTable&gt; list = testTableMapper.selectList(Wrappers.emptyWrapper());5 logger.info(\"读取数据：&#123;&#125;\", list.toString());6 try &#123;7 Thread.sleep(5000);8 &#125; catch (InterruptedException e) &#123;9 e.printStackTrace();10 &#125;11&#125; 连续执行两次这个方法，你就会发现在第一次的线程休眠结束之前，第二次会阻塞。SERIALIZABLE的作用就是把事务串行化，所有该类型的事务都会排队一个一个的执行。同样的，如果另一个方法也是SERIALIZABLE级别，他和testSelect方法也是只能串行执行。比如下面这种，我们增加一个查询方法： 1@Transactional(isolation = Isolation.SERIALIZABLE)2public void testSelect2() &#123;3 List&lt;TestTable&gt; list = testTableMapper.selectList(Wrappers.emptyWrapper());4 logger.info(\"读取数据：&#123;&#125;\", list.toString());5&#125; 同样的，testSelect2也会阻塞等待前一个事务的执行完毕。但是注意当我们把testSelect2的隔离级别改回默认，即REPEATABLE_READ，就不会阻塞等待了。 事务的传播级别首先要清楚什么是事务的传播级别：用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。 1public void methodA()&#123;2 methodB();3 //doSomething4&#125;56@Transaction(Propagation=XXX)7public void methodB()&#123;8 //doSomething9&#125; 代码中methodA()方法嵌套调用了methodB()方法，methodB()的事务传播行为由@Transaction(Propagation=XXX)设置决定。这里需要注意的是methodA()并没有开启事务，某一个事务传播行为修饰的方法并不是必须要在开启事务的外围方法中调用。 Spring 中有七大传播级别： PROPAGATION_REQUIRED：如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。如果被调用端发生异常，那么调用端和被调用端事务都将回滚。这是最常见的选择。 PROPAGATION_SUPPORTS：支持当前事务，如果当前没有事务，就以非事务的方式执行。 PROPAGATION_MANDATORY：使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW：新建事务，如果外部存在事务，把外部事务挂起。这个内部的事务将被完全提交或回滚而不依赖于外部事务，它拥有自己的隔离范围，自己的锁，等等。 PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，就抛出异常。 PROPAGATION_NESTED：如果当前方法正有一个事务在运行中，则该方法应该运行在一个嵌套事务中，被嵌套的事务可以独立于被封装的事务中进行提交或者回滚。 好，我们开始一个一个的测试，加深理解。 注意，由于内部方法抛出运行时异常，会在外层方法里面继续抛出，导致预期外的结果，所以我们不能用抛出异常的方式进行使用。所以下面的测试都是手动回滚。 首先新建两个ServiceA和ServiceB，A 中注入 B，当然还有两个mapper，然后还是用之前的数据库表，先把表清空。两个类文件如下： 1@Service2public class ServiceA &#123;3 @Resource4 TestTableMapper testTableMapper;5 @Resource6 ServiceB serviceB;7&#125;8@Service9public class ServiceB &#123;10 @Resource11 TestTableMapper testTableMapper;12&#125; 为什么需要两个Service而不是在同一个里面进行方法调用，这个涉及到Spring的依赖注入机制，这里先不展开。 我们用插入方法来测试，先插入一条记录，然后抛出异常，通过看数据库中是否真正执行了插入，来判断事务的执行状态。当然也可以打开日志的debug级别，直接看数据库的执行日志。 PROPAGATION_REQUIRED如果没有事务，就新建事务，如果已经有了就加入。 我们先测试 A 没有事务，B 有事务的情况： 1//这是ServiceA2 public void test() &#123;3 serviceB.insert();4 &#125;5 6 //这是ServiceB7 @Transactional(propagation = Propagation.REQUIRED)8 public void insert() &#123;9 TestTable someOne = new TestTable();10 someOne.setId(1);11 someOne.setName(\"法外狂徒张三\");12 someOne.setAge(19);13 testTableMapper.insert(someOne);14 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();15 &#125; 可以看出数据没有实际插入进去，事务进行了回滚。然后我们把ServiceA中的方法加上事务。 1@Transactional2public void test() &#123;3 serviceB.insert();4&#125; 一样的结果。 如果调用端出错了呢？我们把ServiceB中的回滚去掉，让ServiceA中回滚试试： 1@Transactional2public void test() &#123;3 serviceB.insert();4 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();5&#125; 可以看出同样发生了回滚，两个事务其实合为一个了。 PROPAGATION_SUPPORTS如果不存在外层事务，就不开启事务，如果有就加入外部事务运行。 我们去掉ServiceB的事务，A 的还带着，在 A 中回滚。 1@Transactional2public void test() &#123;//ServiceA3 serviceB.insert();4 throw new RuntimeException(\"出错啦\");5&#125;67 @Transactional(propagation = Propagation.SUPPORTS)8 public void insert() &#123;//ServiceB9 TestTable someOne = new TestTable();10 someOne.setId(1);11 someOne.setName(\"法外狂徒张三\");12 someOne.setAge(19);13 testTableMapper.insert(someOne);14&#125; 可以看出还是发生了回滚，证明 B 虽然自身没有异常，但是由于加入了 A 的事务，所以一起跟着回滚了。 我们把 A 的事务也去掉。 1public void test() &#123;//ServiceA2 serviceB.insert();3 throw new RuntimeException(\"出错啦\");4&#125; 可以看到数据成功插入了，没有发生回滚。 PROPAGATION_MANDATORY必须要有事务，没有事务就抛异常。 我们还是去掉 A 的事务和异常抛出，把 B 的传播级别改成PROPAGATION_MANDATORY。 1public void test() &#123;//ServiceA2 serviceB.insert();3&#125;45@Transactional(propagation = Propagation.MANDATORY)6public void insert() &#123;//ServiceB7 TestTable someOne = new TestTable();8 someOne.setId(1);9 someOne.setName(\"法外狂徒张三\");10 someOne.setAge(19);11 testTableMapper.insert(someOne);12&#125; 可以看到直接抛出了异常 1org.springframework.transaction.IllegalTransactionStateException: No existing transaction found for transaction marked with propagation &#39;mandatory&#39; 我们给ServiceA加上事务： 1@Transactional2public void test() &#123;//ServiceA3 serviceB.insert();4&#125; 整个事务顺利完成，没有问题。 PROPAGATION_REQUIRES_NEW无论当前事务上下文中有没有事务，都会开启一个新的事务，如果有了外部事务就挂起，内部的事务将被完全提交或回滚而不依赖于外部事务。 我们先写一个外部的事务 ServiceA，调用了内部事务 ServiceB，让事务 A 进行回滚： 1@Transactional2public void test() &#123;//ServiceA 中的方法，外部事务3 System.out.println(\"执行ServiceA\");4 serviceB.insert();5 TestTable someOne = new TestTable();6 someOne.setId(2);7 someOne.setName(\"法外狂徒李四\");8 someOne.setAge(28);9 testTableMapper.insert(someOne);10 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();11&#125;1213@Transactional(propagation = Propagation.REQUIRES_NEW)14public void insert() &#123;//ServiceB 的方法，内部事务15 System.out.println(\"执行ServiceB\");16 TestTable someOne = new TestTable();17 someOne.setId(1);18 someOne.setName(\"法外狂徒张三\");19 someOne.setAge(18);20 testTableMapper.insert(someOne);21&#125; 执行可以看到，外部事务 ServiceA 插入的内容被回滚了，内部事务 ServiceB 的动作执行不受影响。 类似的，如果我们反过来，B 中回滚而 A 不回滚，B 也不会影响到 A，二者相互独立。 PROPAGATION_NOT_SUPPORTED这个传播级别下，内部事务总是以非事务的形式运行，不管外面有没有事务，自身都是没有事务。 我们还是做测试，先让内部事务抛出异常： 1@Transactional(propagation = Propagation.NOT_SUPPORTED)2public void insert() &#123;//ServiceB 内部事务3 System.out.println(\"执行ServiceB\");4 TestTable someOne = new TestTable();5 someOne.setId(1);6 someOne.setName(\"法外狂徒张三\");7 someOne.setAge(18);8 testTableMapper.insert(someOne);9 throw new RuntimeException(\"RuntimeException\");10&#125;1112@Transactional13public void test() &#123;//ServiceA 外部事务14 System.out.println(\"执行ServiceA\");15 TestTable someOne = new TestTable();16 someOne.setId(2);17 someOne.setName(\"法外狂徒李四\");18 someOne.setAge(28);19 testTableMapper.insert(someOne);20 serviceB.insert();21&#125; 可以看出内部事务由于没有事务，所以虽然抛出异常但是数据库操作正常结束，没有回滚。当然外面的事务收到了影响，因为 catch 到了内部事务的异常。 类似的，如果外面事务抛出异常，内部事务也不会回滚。 PROPAGATION_NEVER这个级别更简单，外部不能有事务，有就抛异常。示例都不用写了，非常好理解，抛出的异常如下： 1org.springframework.transaction.IllegalTransactionStateException: Existing transaction found for transaction marked with propagation &#39;never&#39; PROPAGATION_NESTED这个级别很有意思，如果外面没有事务就创建事务，如果有的话就嵌套进去。这个嵌套的意思是指的“暂存点”，如果子事务发生异常，会直接回滚到这个暂存点，而不会导致整体事务的回滚。 废话少说看测试： 1@Transactional(propagation = Propagation.NESTED)2public void insert() &#123;//ServiceB，内部事务，插入然后抛出异常3 System.out.println(\"执行ServiceB\");4 TestTable someOne = new TestTable();5 someOne.setId(1);6 someOne.setName(\"法外狂徒张三\");7 someOne.setAge(18);8 testTableMapper.insert(someOne);9 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly();10&#125;1112@Transactional13public void test() &#123;//ServiceA，外部事务，在调用 B 的前后分别插入。14 System.out.println(\"执行ServiceA\");15 TestTable someOne = new TestTable();16 someOne.setId(2);17 someOne.setName(\"法外狂徒李四\");18 someOne.setAge(28);19 testTableMapper.insert(someOne);20 serviceB.insert();21 someOne.setId(3);22 someOne.setName(\"王五\");23 someOne.setAge(66);24 testTableMapper.insert(someOne);25&#125; 这个例子里面，内部事务回滚了，但是外部没有回滚。但是外部事务回滚，内部事务也会跟着回滚。 看出来跟REQUIRED，REQUIRES_NEW之间的不同了吗？区别如下： REQUIRED：内外是一个整体，无论内部还是外部的回滚，都会导致二者全回滚。 REQUIRES_NEW：内外相互独立，互相完全不影响。 NESTED：内部加入外部，但是外部不受内部影响。内部回滚的话外部正常，外部回滚的话内部会跟着回滚。 注意事项 在需要事务管理的地方加@Transactional注解。@Transactional注解可以被应用于接口定义和接口方法、类定义和类的 public方法上。 @Transactional注解只能应用到public可见度的方法上。 如果你在protected、private或者package-visible的方法上使用@Transactional注解，它也不会报错， 但是这个被注解的方法将不会展示已配置的事务设置。 @Transactional的事务开启 ，或者是基于接口的 或者是基于类的代理被创建。所以在同一个类中一个方法调用另一个方法有事务的方法，事务是不会起作用的。 在接口上使用@Transactional注解，只能当你设置了基于接口的代理时它才生效。 面试常见问题//TODO 后续补充","categories":[{"name":"Spring","slug":"Spring","permalink":"http://beritra.github.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://beritra.github.com/tags/Spring/"},{"name":"framework","slug":"framework","permalink":"http://beritra.github.com/tags/framework/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://beritra.github.com/tags/SpringBoot/"}]},{"title":"Java并发包使用指南","slug":"Java并发包使用指南","date":"2020-03-22T16:22:06.000Z","updated":"2022-02-20T11:27:13.892Z","comments":true,"path":"2020/03/23/Java并发包使用指南/","link":"","permalink":"http://beritra.github.com/2020/03/23/Java%E5%B9%B6%E5%8F%91%E5%8C%85%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/","excerpt":"我们常说的“并发包”指的是java.util.concurrent这个包，后面简称 J.U.C，里面包含大量多线程和并发编程的工具。J.U.C 包是 JDK 1.5 版本引入，由 Doug Lea 和众多其他大神合力完成，包含大量对并发编程的思考精华，仔细观摩思考他们的设计思路，对于我们学习多线程和并发编程有非常大的帮助。本文所有内容基于 JDK11。","text":"我们常说的“并发包”指的是java.util.concurrent这个包，后面简称 J.U.C，里面包含大量多线程和并发编程的工具。J.U.C 包是 JDK 1.5 版本引入，由 Doug Lea 和众多其他大神合力完成，包含大量对并发编程的思考精华，仔细观摩思考他们的设计思路，对于我们学习多线程和并发编程有非常大的帮助。本文所有内容基于 JDK11。 概述先从两个角度看下并发包的层次结构，一个是继承关系图： 一个是功能结构： 并发包非常庞大，结构复杂，个人能力有限也不可能总结非常清楚，我们先从常见、常用的开始看起吧。 工具类工具包内主要有这几个工具：计数器CountDownLatch、回环栅栏（光看名字估计一头雾水）CyclicBarrier、信号量Semaphore、创建线程池的Executors，最后一个交换数据的Exchanger。本章几个工具的原理都是基于 AQS，关于 AQS 和ReentrantLock，都有单独的文章解读，就不专门介绍了。 CountDownLatch用法一个计数器，常见这样一种场景：多个任务分发个不同的线程去执行，全部执行完毕后回到主线程。当然有不同的实现方式，用CountDownLatch实现起来就很简单： 1int taskNum = 20;2CountDownLatch countLatch = new CountDownLatch(taskNum);34for (int i = 0; i &lt; taskNum; i++) &#123;5 new Thread(() -&gt; &#123;6 try &#123;7 Thread.sleep(200); //假装做了点什么8 System.out.println(Thread.currentThread().getName() + \"执行完毕\");9 &#125; catch (InterruptedException e) &#123;10 e.printStackTrace();11 &#125;12 countLatch.countDown();13 &#125;).start();14&#125;1516try &#123;17 countLatch.await();18 System.out.println(\"全部任务执行完毕\");19&#125; catch (InterruptedException e) &#123;20 e.printStackTrace();21&#125; Seamphore使用信号量，类似于控制并发的时候用到的“令牌桶”算法，通过控制信号总数，不断释放和回收信号来控制并发数量。假设有这么一个场景，假设我们有五个通道可以执行任务，任务总数是 40，所以同一时刻只能有最多五个线程执行，其余的要等待，因此我们使用Seamphore来不断方法许可和收回许可： 1Semaphore semaphore = new Semaphore(5);23for (int i = 0; i &lt; 40; i++) &#123;4 new Thread(() -&gt; &#123;5 try &#123;6 semaphore.acquire();7 System.out.println(Thread.currentThread().getName() + \"取得许可，开始执行任务\");8 Thread.sleep(new Random().nextInt(2000));9 System.out.println(Thread.currentThread().getName() + \"任务完成，释放许可\");10 semaphore.release();11 &#125; catch (InterruptedException e) &#123;12 e.printStackTrace();13 &#125;14 &#125;).start();15&#125; CyclicBarrier使用回环栅栏光看名字不好理解，其实作用很上面的计数器类似，有点类似于 Java 虚拟机中的“安全点”。执行一个任务的时候，所有线程到达栅栏之后停止，等待所有其他线程都到达这个点，然后一起进入下一阶段。与计数器不同的是，CyclicBarrier可以重复使用，举个栗子： 1CyclicBarrier cyclicBarrier = new CyclicBarrier(20);23 for (int i = 0; i &lt; 20; i++) &#123;4 new Thread(() -&gt; &#123;5 long timeStamp = System.currentTimeMillis();6 try &#123;7 Thread.sleep(new Random().nextInt(2000));8 System.out.println(Thread.currentThread().getName() + \":一阶段任务完成，花费了\" + (System.currentTimeMillis() - timeStamp) + \"毫秒，开始等待其他线程\");9 cyclicBarrier.await();10 System.out.println(Thread.currentThread().getName() + \":所有线程执行完成，开始下一阶段\");11 timeStamp = System.currentTimeMillis();12 Thread.sleep(new Random().nextInt(2000));13 System.out.println(Thread.currentThread().getName() + \":二阶段任务完成，花费了\" + (System.currentTimeMillis() - timeStamp) + \"毫秒，开始等待其他线程\");14 cyclicBarrier.await();15 System.out.println(Thread.currentThread().getName() + \":所有线程任务完成\");16 &#125; catch (InterruptedException | BrokenBarrierException e) &#123;17 e.printStackTrace();18 &#125;19 &#125;).start();20 &#125; Exchanger使用交换器顾名思义，就是用来交换数据，理解和使用起来是最简单的，但是内部实现很精巧复杂。使用很简单，只有两个方法，作用就是两个线程在一个安全点交换数据，产生数据慢的那个会阻塞等待。 1Exchanger&lt;Integer&gt; exchanger = new Exchanger&lt;&gt;();23new Thread(() -&gt; &#123;4 int num = new Random().nextInt(1000);5 System.out.println(\"交换之前：Thread1:\" + num);6 try &#123;7 num = exchanger.exchange(num);8 System.out.println(\"交换完毕：Thread1:\" + num);9 &#125; catch (InterruptedException e) &#123;10 e.printStackTrace();11 &#125;12&#125;).start();13new Thread(() -&gt; &#123;14 int num = new Random().nextInt(1000);15 System.out.println(\"交换之前：Thread2:\" + num);16 try &#123;17 Thread.sleep(2000);18 num = exchanger.exchange(num);19 System.out.println(\"交换完毕：Thread2:\" + num);20 &#125; catch (InterruptedException e) &#123;21 e.printStackTrace();22 &#125;23&#125;).start(); 线程池在 J.U.C 包中，创建线程池有两种方式，一种是手动创建，一种是通过Executors工厂类创建预设的几种线程池。 基础的线程有如下几个： 而Executors默认提供了六种线程池，但是不推荐在生产环境中直接使用，因为默认的设置对数据量没有进行限制，有可能出现问题。 ThreadPoolExecutorThreadPoolExecutor提供了四个构造方法： 1public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue)2public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit,BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler)3public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize,long keepAliveTime,TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory)4public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 可以看出参数都类似，首先要确定初始化线程数量和最大数量，然后是存活时间和时间单位。如果线程池已满，就会把多余的任务放到一个阻塞队列中，你需要定义这么一个队列。如果队列也满了，还需要指定拒绝策略。同时还可以传入一个线程工厂来确定线程生成策略。 一个最简单的线程池就可以这么创建了： 1ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 20, 10, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(10)); 这里要注意线程池的扩容策略，在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这 2 个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中，只有当缓存队列也满了才会出发扩容，创建新的线程，然后当线程池容量扩建到设定的最大值之后，如果还有新的任务，就会触发拒绝策略，返回相应的结果或者默认抛出异常。 keepAliveTime是线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime。 关于拒绝策略，除了自行实现以外，提供了四种预设的策略： 1ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 2ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 3ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程）4ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 线程中的任务通过execute()或者submi()方法来提交，二者的区别是有没有返回值。 execute()方法实际上是Executor中声明的方法，在ThreadPoolExecutor进行了具体的实现，这个方法是ThreadPoolExecutor的核心方法，通过这个方法可以向线程池提交一个任务，交由线程池去执行。 submit()方法是在ExecutorService中声明的方法，在AbstractExecutorService就已经有了具体的实现，在ThreadPoolExecutor中并没有对其进行重写，这个方法也是用来向线程池提交任务的，但是它和execute()方法不同，它能够返回任务执行的结果，去看submit()方法的实现，会发现它实际上还是调用的execute()方法，只不过它利用了Future来获取任务执行结果。 执行任务还可以通过invokeAll方法，需要预先创建Callable的集合，然后放到线程池中执行，invokeAll会创建线程依次执行任务，主线程阻塞等待结果。但是，需要注意一种特殊情况： 当批量执行的任务数量大于线程池数量+队列数量，这时根据拒绝策略不同，会产生不同的结果，DiscardPolicy，DiscardOldestPolicy 这两种策略会导致线程池锁住。所以要是使用这两种拒绝策略的时候，就要控制任务数量，或者准备足够大的线程池。 ForkJoinPoolForkJoinPool主要思想就是分而治之，在能够用分治算法的场景下，ForkJoinPool有很高的效率。 ForkJoinPool的本质就是两点： 如果任务很小：直接计算得出结果 如果任务很大 拆分成N个子任务 调用子任务的fork()进行计算 调用子任务的join()合并结果 我们要使用 ForkJoin 框架，必须首先创建一个 ForkJoin 任务。它提供在任务中执行 fork() 和 join() 操作的机制，通常情况下我们不需要直接继承 ForkJoinTask 类，而只需要继承它的子类，ForkJoin 框架提供了以下两个子类： RecursiveAction：用于没有返回结果的任务。 RecursiveTask ：用于有返回结果的任务。 比如这样一个场景：我们想要计算从 1 到 2000 的数字累加和，假设这个任务很消耗资源，我们打算交给多个线程分开去计算，最后把每个线程的结果加到一起。拆分任务的规则是每个线程计算不超过 20 个数字的累加和，我们就可以创建这么一个对象： 1class MyTask extends RecursiveTask&lt;Integer&gt; &#123;2 int start;3 int end;45 public MyTask() &#123;6 &#125;78 public MyTask(int start, int end) &#123;9 this.start = start;10 this.end = end;11 &#125;1213 @Override14 protected Integer compute() &#123;15 if (end - start &lt; 20) &#123;16 int count = 0;17 for (int i = start; i &lt;= end; i++) &#123;18 count += i;19 &#125;20 return count;21 &#125;22 int middle = (start + end) / 2;23 MyTask leftTask = new MyTask(start, middle);24 MyTask rightTask = new MyTask(middle + 1, end);25 invokeAll(leftTask, rightTask);26 leftTask.fork();27 rightTask.fork();28 return leftTask.join() + rightTask.join();29 &#125;30&#125; 可以看出，当计算量小于 20 的时候，开始计算，然后返回结果。如果计算量大于 20，就拆分任务，然后再创建两个子任务，等待子任务返回。这里的例子只是一个简单类比，实际上ForkJoinPool更适合计算密集型的任务，像这种小规模的简单累加线程调度的开销比计算本身大多了。 使用ForkJoinPool有几点需要注意： 活跃线程数被控制在 CPU 核心数以内，所以不需要创建过多的线程，线程池内部调度的时候就会做限制。 最大线程数被限制在0x7fff，即 32767。 可以选择队列模式是 FOFO 或者 LIFO。 任务队列的初始化容量是 8192，最大容量限制是 67108864 即 64M，超过的话会抛异常。\\ 相比ThreadPoolExecutor，ForkJoinPool的优势是什么？ 使用ForkJoinPool能够使用数量有限的线程来完成非常多的具有父子关系的任务，比如使用 4 个线程来完成超过 200 万个任务。但是，使用ThreadPoolExecutor时，是不可能完成的，因为ThreadPoolExecutor中的Thread无法选择优先执行子任务，需要完成 200 万个具有父子关系的任务时，也需要 200 万个线程，显然这是不可行的。 ScheduledPoolExecutor自 JDK1.5 开始，JDK 提供了ScheduledThreadPoolExecutor类来支持周期性任务的调度。在这之前的实现需要依靠Timer和TimerTask或者其它第三方工具来完成。 ScheduledThreadPoolExecutor继承ThreadPoolExecutor来重用线程池的功能，它的实现方式如下： 将任务封装成ScheduledFutureTask对象，ScheduledFutureTask基于相对时间，不受系统时间的改变所影响； ScheduledFutureTask实现了java.lang.Comparable接口和java.util.concurrent.Delayed接口，所以有两个重要的方法：compareTo和getDelay。compareTo方法用于比较任务之间的优先级关系，如果距离下次执行的时间间隔较短，则优先级高；getDelay方法用于返回距离下次任务执行时间的时间间隔； ScheduledThreadPoolExecutor定义了一个DelayedWorkQueue，它是一个有序队列，会通过每个任务按照距离下次执行时间间隔的大小来排序； ScheduledFutureTask继承自FutureTask，可以通过返回Future对象来获取执行的结果。 ScheduledThreadPoolExecutor的构造函数有以下几个： 1// 使用给定核心池大小创建一个新 ScheduledThreadPoolExecutor。2ScheduledThreadPoolExecutor(int corePoolSize) 3// 使用给定初始参数创建一个新 ScheduledThreadPoolExecutor。4ScheduledThreadPoolExecutor(int corePoolSize, RejectedExecutionHandler handler) 5// 使用给定的初始参数创建一个新 ScheduledThreadPoolExecutor。6ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) 7// 使用给定初始参数创建一个新 ScheduledThreadPoolExecutor。8ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory, RejectedExecutionHandler handler) ScheduledThreadPoolExecutor最多支持 3 个参数：核心线程数量，线程工厂，拒绝策略。 为什么没有最大线程数量？由于 ScheduledThreadPoolExecutor 内部是个无界队列，maximumPoolSize 也就没有意思了。 常用的方法有以下几个： 1// 创建并执行在给定延迟后启用的一次性操作。 2ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) 3 4// 创建并执行一个在给定初始延迟后首次启用的定期操作，后续操作具有给定的周期；也就是将在 initialDelay 后开始执行，然后在 initialDelay+period 后执行，接着在 initialDelay + 2 * period 后执行，依此类推。 5ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) 67// 创建并执行一个在给定初始延迟后首次启用的定期操作，随后，在每一次执行终止和下一次执行开始之间都存在给定的延迟。 8ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) schedule方法很好理解，是一次性操作，只不过加了个延迟。后面两个方法的区别就是： cheduleAtFixedRate是两次执行开始时间的间隔固定，不管单次执行的时长，有可能上次还没执行完毕下次就开始了。 scheduleWithFixedDelay是上次结束和下次开始之间的间隔固定，永远不会出现上次还没执行完毕下次就开始的情况。 需要注意的是，任务的排序是通过 ScheduledFutureTask的 compareTo方法排序的，规则是先比较执行时间，如果时间相同，再比较加入时间。 还要注意一点就是：如果任务执行过程中异常了，那么将不会再次重复执行。因为 ScheduledFutureTask的 run方法没有做catch处理。所以程序员需要手动处理，相对于Timer异常就直接费了调度系统来说，要好很多。 Executors除了以上三种创建方式，J.U.C 包中还提供了Executors工厂方法直接创建几种预设好的线程池，包括以下几类： newCachedThreadPool() 缓存型线程池，先查看有没有以前建立的线程，如果有，就 reuse 如果没有，就建一个新的线程加入池中。 缓存型线程池通常用于执行一些生存期很短的异步型任务。 能 reuse 的线程，必须是timeout IDLE内的池中线程，缺省timeout是 60s,超过这个时长，线程实例将被终止及移出池。 newFixedThreadPool(int) 固定大小的线程池。 每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，在提交新任务，任务将会进入等待队列中等待。 如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newScheduledThreadPool(int) 创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 newSingleThreadScheduledExecutor也是类似的延迟任务线程池，只不过只是单线程执行。 SingleThreadExecutor() 一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。 如果唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newWorkStealingPool() 一个基于Fork/Join模型的线程池，内部通过ForkJoinPool创建。 用法跟ForkJoinPool一样，体现“分而治之” 的思想。 但是，虽然鼓励使用线程池而不是直接新建线程，但是在生产系统中不建议直接使用Executors创建的线程池，阿里巴巴Java手册中是这么解释的： 并发集合Map 系列最知名也最常用ConcurrentHashMap本文就先不讲了，已经单独分析过，请参考那篇文章。 ConcurrentSkipListMapSkip List（跳表）是一种可以代替平衡树的数据结构，默认是按照 Key 值升序的。Skip List 让已排序的数据分布在多层链表中，以 0-1 随机数决定一个数据的向上攀升与否，通过“空间来换取时间”的一个算法，在每个节点中增加了向前的指针，在插入、删除、查找时可以忽略一些不可能涉及到的结点，从而提高了效率。 简单介绍跳表的原理，这里盗用了博客J.U.C 之 ConcurrentSkipListMap上的解释： 我们先看一个简单的链表，如下： 如果我们需要查询9、21、30，则需要比较次数为3 + 6 + 8 = 17 次，那么有没有优化方案呢？有！我们将该链表中的某些元素提炼出来作为一个比较“索引”，如下： 我们先与这些索引进行比较来决定下一个元素是往右还是下走，由于存在“索引”的缘故，导致在检索的时候会大大减少比较的次数。当然元素不是很多，很难体现出优势，当元素足够多的时候，这种索引结构就会大显身手。 当然，实际上 Skip List 的原理要更复杂，就不在这详细叙述了。 总之，跟ConcurrentHashMap相比，ConcurrentSkipListMap的key是有序的。有的文章提到ConcurrentSkipListMap支持更高的并发，线程越多性能越强，但是经过我实际测试，从 10 个线程到 100 个线程，无论put还是get，都没有快于ConcurrentHashMap，只有超过三百个线程之后，put操作会略微快一点点。 CopyOnWrite 系列CopyOnWrite，或者叫写入时复制，其实是一种策略，以下是维基百科的说明： 其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的（transparently）。 通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对 CopyOnWrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 CopyOnWrite 容器也是一种读写分离的思想，读和写不同的容器。 J.U.C 包中提供了两个 CopyOnWrite 容器，分别是CopyOnWriteArrayList和CopyOnWriteArraySet。这两个的性质很相似，只不过在原本的 List 和 Set 上使用了 COW 的思想，而且CopyOnWriteArraySet的实现原理就是在内部维护了一个CopyOnWriteArrayList。 CopyOnWrite 并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景。 CopyOnWriteArrayList容器的原理也不复杂，只是需要在add方法中加锁，添加完成之后用新的数组替代原有的数组，而get操作不需要加锁 1public boolean add(E e) &#123;2 synchronized (lock) &#123;3 Object[] es = getArray();4 int len = es.length;5 es = Arrays.copyOf(es, len + 1);6 es[len] = e;7 setArray(es);8 return true;9 &#125;10&#125; 代码很简单，但是使用CopyOnWriteMap需要注意两件事情： 减少扩容开销。根据实际需要，初始化CopyOnWriteMap的大小，避免写时CopyOnWriteMap扩容的开销。 使用批量添加。因为每次添加，容器每次都会进行复制，所以减少添加次数，可以减少容器的复制次数。 CopyOnWrite 容器有很多优点，但是同时也存在两个问题，即内存占用问题和数据一致性问题。所以在开发的时候需要注意一下。 Queue 系列从 J.U.C 的导图就可以看出，提供给我们最多的集合类就是队列，主要分为三个方面：ConcurrentLinkedQueue、BlockingQueue和Deque ConcurrentLinkedQueueConcurrentLinkedQueue底层使用单链表存储数据，增加了空的头尾节点，是非阻塞、无界的线程安全队列。它采用先进先出的规则对节点进行排序，当我们添加一个元素的时候，它会添加到队列的尾部，当我们获取一个元素时，它会返回队列头部的元素。由于是无界队列，所以add()和offer()方法没有区别，都不会抛出异常。 跟线程不安全的队列比如PriorityQueue相比，由于数据存储是链表而不是数组，因此没有并发扩容的问题，但是入队出队是通过 CAS 实现的，大并发下有可能有效率问题，而且遍历的时候数据不是准确的。 BlockingQueue BlockingQueue是一个接口，定义了阻塞队列的基本方法，J.U.C 包中有多个该接口的实现类。定义的常用方法如下： BlockingQueue的特点： BlockingQueue可以是限定容量的。它在任意给定时间都可以有一个remainingCapacity，超出此容量，便无法无阻塞地put附加元素。没有任何内部容量约束的BlockingQueue总是报告Integer.MAX_VALUE的剩余容量。 BlockingQueue实现主要用于生产者-使用者队列，但它另外还支持Collection接口。因此，举例来说，使用remove(x)从队列中移除任意一个元素是有可能的。然而，这种操作通常不会有效执行，只能有计划地偶尔使用，比如在取消排队信息时。 BlockingQueue实现是线程安全的。所有排队方法都可以使用内部锁或其他形式的并发控制来自动达到它们的目的。然而，大量的Collection操作（addAll、containsAll、retainAll 和removeAll）没有 必要自动执行，除非在实现中特别说明。因此，举例来说，在只添加了c中的一些元素后，addAll(c)有可能失败（抛出一个异常）。 BlockingQueue实质上不 支持使用任何一种“close”或“shutdown”操作来指示不再添加任何项。这种功能的需求和使用有依赖于实现的倾向。例如，一种常用的策略是：对于生产者，插入特殊的 end-of-stream 或 Poison 对象，并根据使用者获取这些对象的时间来对它们进行解释。 BlockingQueue的实现类： BlockingDeque：阻塞的双端队列，LinkedBlockingDeque是其基于链表的实现类，如果没有设置容量，那么容量将是Int的最大值。LinkedBlockingQueue可以同时有两个线程在两端执行操作，这点与LinkedBlockingQeque不同。 TransferQueue：基于队列扩展的一种有趣的生产-消费者模型，其实现类是BlockingTransferQueue，能够实现元素在线程之间的传递。 SynchronousQueue：跟TransferQueue很类似，线程 A 通过put方法存入数据到队列中，如果没有别的线程通过take方法去获取这个数据，那线程 A 进入阻塞状态；当有别的线程获取了这个值之后，线程 A 就恢复执行。这个特点跟TransferQueue很像，Doug Lea说从功能角度来讲，LinkedTransferQueue实际上是ConcurrentLinkedQueue、SynchronousQueue（公平模式）和LinkedBlockingQueue的超集。而且LinkedTransferQueue更好用，因为它不仅仅综合了这几个类的功能，同时也提供了更高效的实现。所以SynchronousQueue的使用场景就很少了。 PriorityBlokcingQueue：可以理解成PriorityQueue的线程安全版本，基于优先堆的一个无界队列。不允许 null 值，不允许不可比较的对象。 DelayQueue：一个有趣的延迟队列，它的特殊之处在于队列的元素必须实现Delayed接口，该接口需要实现compareTo和getDelay方法。这个在下面用法里再详细介绍。 ArrayBlockingQueue：基于数组的有界阻塞队列，必须指定大小。 LinkedBlockingQueue：基于链表的有界阻塞队列，可以不指定队列大小，默认是Integer.MAX_VALUE ArrayBlockingQueue 和 LinkedBlockingQueue由于这两个类的相关性，就放在一起说。同样是阻塞的有界队列，使用方法也基本一致，就着重说明两者间的区别： 队列中锁的实现不同 ArrayBlockingQueue实现的队列中的锁是没有分离的，即生产和消费用的是同一个锁，但是两个条件 LinkedBlockingQueue实现的队列中的锁是分离的，即生产用的是putLock，消费是takeLock 在生产或消费时操作不同 ArrayBlockingQueue实现的队列中在生产和消费的时候，是直接将枚举对象插入或移除的 LinkedBlockingQueue实现的队列中在生产和消费的时候，需要把枚举对象转换为Node&lt;E&gt;进行插入或移除，会影响性能 队列大小初始化方式不同 ArrayBlockingQueue实现的队列中必须指定队列的大小 LinkedBlockingQueue实现的队列中可以不指定队列的大小，但是默认是Integer.MAX_VALUE 为了对两个队列的效率有个更直观的认识，我进行了一个简单的测试，对一千万的数据量进行操作，首先是单线程存，然后单线程取： 1public void blockingQueueBench(BlockingQueue&lt;Integer&gt; queue) &#123;2 int count = 1000 * 10000;3 long timestamp = System.currentTimeMillis();4 for (int i = 0; i &lt; count; i++) &#123;5 try &#123;6 queue.put(i);7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 &#125;11 System.out.println(System.currentTimeMillis() - timestamp);12 timestamp = System.currentTimeMillis();13 for (int i = 0; i &lt; count; i++) &#123;14 try &#123;15 queue.take();16 &#125; catch (InterruptedException e) &#123;17 e.printStackTrace();18 &#125;19 &#125;20 System.out.println(System.currentTimeMillis() - timestamp);21&#125; 然后分别对比两个队列存和取的效率： 1new QueueTest().blockingQueueBench(new LinkedBlockingQueue&lt;&gt;(10000000));2//结果存1909毫秒，取196毫秒3new QueueTest().blockingQueueBench(new ArrayBlockingQueue&lt;&gt;(10000000));4//结果存237毫秒，取135毫秒 然后是两个线程，一个存一个取： 1public void blockingQueueBench(BlockingQueue&lt;Integer&gt; queue) &#123;2 int count = 1000 * 10000;3 long timestamp = System.currentTimeMillis();4 for (int i = 0; i &lt; count; i++) &#123;5 try &#123;6 queue.put(i);7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 &#125;11 Thread thread = new Thread(() -&gt; &#123;12 for (int i = 0; i &lt; count; i++) &#123;13 try &#123;14 queue.take();15 &#125; catch (InterruptedException e) &#123;16 e.printStackTrace();17 &#125;18 &#125;19 &#125;);20 thread.start();21 try &#123;22 thread.join();23 &#125; catch (InterruptedException e) &#123;24 e.printStackTrace();25 &#125;26 System.out.println(System.currentTimeMillis() - timestamp);27&#125; 然后看存取都完成的时间，LinkedBlockingQueue是 2141 毫秒，ArrayBlockingQueue是 370 毫秒。单线程的情况下，无论如何ArrayBlockingQueue都是要快的。 同样的，我测试了 10 线程和 100 线程下的效率，总的数据量也是一千万。也是分为先存后取和同时存取。结果如下 LinkedBlockingQueue ArrayBlockingQueue 10 线程先存后取 存2147ms 取624ms 存466ms 取339ms 10 线程同时存取 1974ms 724ms 100 线程先存后取 存2347ms 取1040ms 存464ms 取306ms 100 线程同时存取 1957ms 976ms 代码基于下面这个，稍有改动： 1public void blockingQueueMultiBench(BlockingQueue&lt;Integer&gt; queue, int threadNum) &#123;2 int count = 1000 * 10000;3 long timestamp = System.currentTimeMillis();4 CountDownLatch latch = new CountDownLatch(threadNum);56 for (int i = 0; i &lt; threadNum; i++) &#123;7 new Thread(() -&gt; &#123;8 for (int j = 0; j &lt; count / threadNum; j++) &#123;9 try &#123;10 queue.put(j);11 &#125; catch (InterruptedException e) &#123;12 e.printStackTrace();13 &#125;14 &#125;15 latch.countDown();16 &#125;).start();17 &#125;18 try &#123;19 latch.await();20 &#125; catch (InterruptedException e) &#123;21 e.printStackTrace();22 &#125;23 System.out.println(System.currentTimeMillis() - timestamp);24 CountDownLatch latch2 = new CountDownLatch(threadNum);2526 timestamp = System.currentTimeMillis();27 for (int i = 0; i &lt; threadNum; i++) &#123;28 new Thread(() -&gt; &#123;29 for (int j = 0; j &lt; count / threadNum; j++) &#123;30 try &#123;31 queue.take();32 &#125; catch (InterruptedException e) &#123;33 e.printStackTrace();34 &#125;35 &#125;36 latch2.countDown();37 &#125;).start();38 &#125;39 try &#123;4041 latch2.await();42 &#125; catch (InterruptedException e) &#123;43 e.printStackTrace();44 &#125;45 System.out.println(System.currentTimeMillis() - timestamp);46&#125; 可以看出，对于效率来讲大多数时候用ArrayBlockingQueue比较合适。 BlockingDeque就跟 Queue 与 Deque 的区别一样，BlockingDeque只是单端队列变成了双端队列，添加了在两端存取元素的方法。比如Quque原本的offer、poll、put、take等方法，都扩展了offerFirst、offerLast等等在对首位元素分别进行操作的方法。内部使用ReentrantLock保证了线程安全，其他没有什么特殊之处，就不详细讲了。 TransferQueue 和 SynchronousQueue这两个也是性质非常相似的队列，用法也基本一样，主要用于不同线程一对一的传递数据。Doug Lea说从功能角度来讲，LinkedTransferQueue实际上是ConcurrentLinkedQueue、SynchronousQueue（公平模式）和LinkedBlockingQueue的超集。而且LinkedTransferQueue更好用，因为它不仅仅综合了这几个类的功能，同时也提供了更高效的实现。所以我们就尽量使用LinkedTransferQueue吧。 TransferQueue的transfer()方法是这样，一次只能传递一个东西过去，如果上次穿的没有被消费掉，新的方法就就会阻塞。同样的，SynchronousQueue的put()方法也是一样。 1public void testTransferQueue() &#123;2 LinkedTransferQueue&lt;Integer&gt; transferQueue = new LinkedTransferQueue&lt;&gt;();3 new Thread(() -&gt; &#123;4 try &#123;5 while (true) &#123;6 System.out.println(transferQueue.take());7 Thread.sleep(1000);8 &#125;9 &#125; catch (InterruptedException e) &#123;10 e.printStackTrace();11 &#125;12 &#125;).start();13 try &#123;14 transferQueue.transfer(1);15 transferQueue.transfer(2);16 transferQueue.transfer(3);17 &#125; catch (InterruptedException e) &#123;18 e.printStackTrace();19 &#125;20&#125; 从这个示例中可以看出，虽然三行transfer方法是连续的，但是在take方法取走之前的元素之前是阻塞的。另外还有个有趣的地方，由于内部没有维护容器，所以LinkedTransferQueue的size()方法和迭代器都是没有意义的。 PriorityBlockingQueue是线程安全阻塞版本的PriorityQueue，对比PriorityQueue而言在存取删除元素和扩容的时候都有ReentrantLock锁，所以实现了线程安全。元素为空的时候再取元素会阻塞。有内部结构是基于完全二叉树的小顶堆，所以不允许 null 值，不允许不可比较的对象。 DelayQueueDelayQueue是一个延迟队列，想要用它存取元素，必须实现Delayed接口，可以看出其定义如下： 1public class DelayQueue&lt;E extends Delayed&gt; extends AbstractQueue&lt;E&gt; implements BlockingQueue&lt;E&gt; 而Delayed接口继承自Comparable，所以其实现类需要实现getDelay和compareTo方法。 getDelay方法是这样的：当返回值小于 0 的时候，该元素才能被取出。我们创建一个实现类： 1public class TestDelayed implements Delayed &#123;2 int num;3 long time;45 public TestDelayed() &#123;6 &#125;78 public TestDelayed(int num, int delay) &#123;9 this.num = num;10 this.time = System.currentTimeMillis() + delay;11 &#125;1213 @Override14 public long getDelay(TimeUnit unit) &#123;15 return time - System.currentTimeMillis();16 &#125;1718 @Override19 public int compareTo(Delayed o) &#123;20 return Long.compare(getDelay(TimeUnit.SECONDS), o.getDelay(TimeUnit.SECONDS));21 &#125;22 &#125; 这个类的作用就是在指定的时间之后取回元素，时间单位是毫秒，然后进行测试： 1public void testDelayQueue() &#123;2 DelayQueue&lt;TestDelayed&gt; delayQueue = new DelayQueue&lt;&gt;();3 new Thread(() -&gt; &#123;4 try &#123;5 while (true) &#123;6 int num = delayQueue.take().num;7 System.out.println(System.currentTimeMillis() + \",take:\" + num);8 &#125;9 &#125; catch (InterruptedException e) &#123;10 e.printStackTrace();11 &#125;12 &#125;).start();13 delayQueue.put(new TestDelayed(1, 1000));14 delayQueue.put(new TestDelayed(2, 2000));15 delayQueue.put(new TestDelayed(6, 6000));16&#125; 可以看出，三个元素分别在第一秒、第二秒和第六秒被取出，然后线程阻塞等待取回下一个元素。 锁并法包中的接口有Lock、ReadWriteLock、Condition、LockSupport等。 Lock 接口和其实现类ReentrantLock已经专门讲过，就不赘述。我们先看看读写锁的用法。 ReadWriteLock该接口只有两个方法，读锁和写锁。也就是说，我们在写文件的时候，可以将读和写分开，分成 2 个锁来分配给线程，从而可以做到读和读互不影响，读和写互斥，写和写互斥，提高读写文件的效率。该接口也有一个实现类ReentrantReadWriteLock，下面我们就来学习下这个类。 读写锁的用法很简单，以ReentrantReadWriteLock为例，使用lock.writeLock()和lock.readLock()分别创建锁对象。我们编写一个测试类来验证读写锁的互斥逻辑，先写两个方法，线程休眠来模拟耗时的读写操作： 1private void read() &#123;2 System.out.println(Thread.currentThread().getName() + \"正在进行读操作\");3 try &#123;4 Thread.sleep(1000);5 &#125; catch (InterruptedException e) &#123;6 e.printStackTrace();7 &#125;8 System.out.println(Thread.currentThread().getName() + \"读操作完毕\");9&#125;1011private void write() &#123;12 System.out.println(Thread.currentThread().getName() + \"正在进行写操作\");13 try &#123;14 Thread.sleep(1000);15 &#125; catch (InterruptedException e) &#123;16 e.printStackTrace();17 &#125;18 System.out.println(Thread.currentThread().getName() + \"写操作完毕\");19&#125; 然后分别创建读写锁，然后进行 5 线程同时读、同时写、同时读写等等操作： 1ReadWriteLock lock = new ReentrantReadWriteLock();23Lock writeLock = lock.writeLock();4Lock readLock = lock.readLock();5for (int i = 0; i &lt; 5; i++) &#123;6 new Thread(() -&gt; &#123;7 readLock.lock();8 lockTest.read();9 readLock.unlock();10 &#125;).start();11 new Thread(() -&gt; &#123;12 writeLock.lock();13 lockTest.write();14 writeLock.unlock();15 &#125;).start();16&#125; 可以看出，读锁加锁之后，可以同时进行读操作，但是写锁加锁之后不能同时写。而读写互相之间也是互斥的，不能在读的同时写。 ConditionCondition是在 Java 1.5 中才出现的，它用来替代传统的wait()、notify()实现线程间的协作，相比使用Object的wait()、notify()，使用Condition的await()、signal()这种方式实现线间协作更加安全和高效。因此通常来说比较推荐使用Condition，阻塞队列实际上是使用了Condition来模拟线程间协作。 Condition是与Lock绑定的，所以就有Lock的公平性特性：如果是公平锁，线程为按照 FIFO 的顺序从Condition.await中释放，如果是非公平锁，那么后续的锁竞争就不保证 FIFO 顺序了。 调用Condition的await()和signal()方法，都必须在持有锁，就是说必须在lock.lock()和lock.unlock之间才可以使用。 一个简单的示例： 1Lock lock = new ReentrantLock();2 Condition condition1 = lock.newCondition();34 new Thread(() -&gt; &#123;5 try &#123;6 lock.lock();7 condition1.await();8 lock.unlock();9 System.out.println(Thread.currentThread().getName() + \"被唤醒\");10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 &#125;).start();1415 new Thread(() -&gt; &#123;16 try &#123;17 Thread.sleep(2000);18 System.out.println(Thread.currentThread().getName() + \"开始唤醒\");19 &#125; catch (InterruptedException e) &#123;20 e.printStackTrace();21 &#125;22 lock.lock();23 condition1.signalAll();24 lock.unlock();25 &#125;).start(); LockSupport在之前介绍 AQS 的底层实现，已经在介绍 Java 中的 Lock 时，比如ReentrantLock，ReentReadWriteLocks，已经在介绍线程间等待/通知机制使用的 Condition 时都会调用LockSupport.park()方法和LockSupport.unpark()方法。LockSupport 主要有以下几个方法： void park()：阻塞当前线程，如果调用unpark方法或者当前线程被中断，从能从park()方法中返回 void park(Object blocker)：功能同方法 1，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查； void parkNanos(long nanos)：阻塞当前线程，最长不超过nanos纳秒，增加了超时返回的特性； void parkNanos(Object blocker, long nanos)：功能同方法 3，入参增加一个 Object 对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查； void parkUntil(long deadline)：阻塞当前线程， 直到 deadline； void parkUntil(Object blocker, long deadline：功能同方法 5，入参增加一个Object对象，用来记录导致线程阻塞的阻塞对象，方便进行问题排查； Object getBlocker(Thread t)：获取线程park入参的对象。 注意，LockSupport 是不可重入的，如果一个线程连续 2 次调用LockSupport.park()，那么该线程一定会一直阻塞下去。 LockSupport 很类似于二元信号量(只有 1 个许可证可供使用)，如果这个许可还没有被占用，当前线程获取许可并继续执行；如果许可已经被占用，当前线程阻塞，等待获取许可。 比如我们写一个最简单的例子： 1LockSupport.park();2System.out.println(\"xxx\"); 线程会被阻塞，因为许可默认是被占用的，相当于许可证为 0，线程阻塞等待直到许可证为 1。 而我们可以先unpark，释放一个许可，相当于把许可证添加为 1： 1Thread thread = Thread.currentThread();2LockSupport.unpark(thread);3LockSupport.park();// 获取许可4System.out.println(\"xxx\"); 这次就不阻塞，正常执行了。 原子类J.U.C 包中的原子类可以分为五类： 基本类型：AtomicBoolean、AtomicInteger、AtomicLong 引用类型：AtomicReference、AtomicStampedRerence、AtomicMarkableReference 数组：AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray 对象的属性：AtomicIntegerFieldUpdater、AtomicLongFieldUpdater、AtomicReferenceFieldUpdater JDK1.8新增：DoubleAccumulator、LongAccumulator、DoubleAdder、LongAdder AtomicBoolean、AtomicInteger、AtomicLong和AtomicReference的实例各自提供对相应类型单个变量的原子方式访问和更新功能。例如AtomicBoolean提供对 int 类型单个变量的原子方式访问和更新功能。每个类也为该类型提供适当的实用工具方法。例如，类AtomicLong和AtomicInteger提供了原子增量方法，可以用于生成序列号。 AtomicStampedRerence维护带有整数“标志”的对象引用，可以用原子方式对其进行更新。AtomicMarkableReference维护带有标记位的对象引用，可以原子方式对其进行更新。 AtomicIntegerArray、AtomicLongArray和AtomicReferenceArray类进一步扩展了原子操作，对这些类型的数组提供了支持。例如AtomicIntegerArray是可以用原子方式更新其元素的 int 数组。 AtomicReferenceFieldUpdater、AtomicIntegerFieldUpdater和AtomicLongFieldUpdater是基于反射的实用工具，可以提供对关联字段类型的访问。例如AtomicIntegerFieldUpdater可以对指定类的指定volatile int字段进行原子更新。 DoubleAccumulator、LongAccumulator、DoubleAdder、LongAdder是 JDK1.8 新增的部分，是对AtomicLong等类的改进。比如LongAccumulator与LongAdder在高并发环境下比AtomicLong更高效。 参考文章： 深入理解AbstractQueuedSynchronizer(AQS) AQS源码分析 JAVA进阶之AQS 并发编程——详解 AQS CLH 锁 Java多线程（五）之BlockingQueue深入分析 Java并发编程札记-(三)JUC原子类-01概述","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"AQS原理解析","slug":"AQS原理解析","date":"2020-03-22T16:21:47.000Z","updated":"2020-05-20T11:46:03.434Z","comments":true,"path":"2020/03/23/AQS原理解析/","link":"","permalink":"http://beritra.github.com/2020/03/23/AQS%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/","excerpt":"AbstractQueuedSynchronizer，可以叫做抽象队列同步器，可以说是 J.U.C 并发包里大多数工具的基石，并发包里的几个工具类，还有包括之前已经分析过得ReentrantLock统统是基于该同步器所设立的框架。 就让我们看看这个东西到底有什么精巧的设计。","text":"AbstractQueuedSynchronizer，可以叫做抽象队列同步器，可以说是 J.U.C 并发包里大多数工具的基石，并发包里的几个工具类，还有包括之前已经分析过得ReentrantLock统统是基于该同步器所设立的框架。 就让我们看看这个东西到底有什么精巧的设计。 AQS 实现了对同步状态的管理，以及对阻塞线程进行排队、等待通知等等一些底层的实现处理。AQS 的核心也包括了这些方面：同步队列，独占式锁的获取和释放，共享锁的获取和释放以及可中断锁，超时等待锁获取这些特性的实现，这些实际上是AQS提供出来的模板方法。AQS 是一个抽象类，当我们继承 AQS 去实现自己的同步器时，要做的仅仅是根据自己同步器需要满足的性质实现线程获取和释放资源的方式（修改同步状态变量的方式）即可，至于具体线程等待队列的维护（如获取资源失败入队、唤醒出队、以及线程在队列中行为的管理等），AQS 在其顶层已经帮我们实现好了，AQS 的这种设计使用的正是模板方法模式。 AQS 支持两种模式： 独占模式（exclusive mode）：同一时刻只允许一个线程访问共享资源，如ReentrantLock等 公平模式：获取锁失败的线程需要按照顺序排列，前面的先拿到锁 非公平模式： 当线程需要获取锁时，会尝试直接获取锁 共享模式（shared mode）：同一时刻允多个线程访问共享资源 AQS 使用了 CLH 内部队列，也叫 CLH 锁。这个 CLH 听起来很厉害的样子，实际上是三位创作者的名字简称：Craig, Landin, and Hagersten。CLH 锁是基于链表的可扩展、高性能、公平的自旋锁，申请线程只在本地变量上自旋，它不断轮询前驱的状态，如果发现前驱释放了锁就结束自旋。 AQS 维护了一个内部类，包括以下内容： 1static final class Node &#123;2 static final Node SHARED = new Node();3 static final Node EXCLUSIVE = null;4 static final int CANCELLED = 1;5 static final int SIGNAL = -1;6 static final int CONDITION = -2;7 static final int PROPAGATE = -3;8 volatile int waitStatus;910 volatile Node prev;11 volatile Node next;12 volatile Thread thread;13 14 Node nextWaiter;1516 final boolean isShared() &#123;17 return nextWaiter == SHARED;18 &#125;19 20 final Node predecessor() &#123;21 Node p = prev;22 if (p == null)23 throw new NullPointerException();24 else25 return p;26 &#125;2728 Node() &#123; // Used to establish initial head or SHARED marker29 &#125;30 31 /** Constructor used by addWaiter. */32 Node(Node nextWaiter) &#123;33 this.nextWaiter = nextWaiter;34 THREAD.set(this, Thread.currentThread());35 &#125;3637 /** Constructor used by addConditionWaiter. */38 Node(int waitStatus) &#123;39 WAITSTATUS.set(this, waitStatus);40 THREAD.set(this, Thread.currentThread());41 &#125;42 43 /** CASes waitStatus field. */44 final boolean compareAndSetWaitStatus(int expect, int update) &#123;45 return WAITSTATUS.compareAndSet(this, expect, update);46 &#125;4748 /** CASes next field. */49 final boolean compareAndSetNext(Node expect, Node update) &#123;50 return NEXT.compareAndSet(this, expect, update);51 &#125;5253 final void setPrevRelaxed(Node p) &#123;54 PREV.set(this, p);55 &#125;5657 // VarHandle mechanics58 private static final VarHandle NEXT;59 private static final VarHandle PREV;60 private static final VarHandle THREAD;61 private static final VarHandle WAITSTATUS;62 static &#123;63 try &#123;64 MethodHandles.Lookup l = MethodHandles.lookup();65 NEXT = l.findVarHandle(Node.class, \"next\", Node.class);66 PREV = l.findVarHandle(Node.class, \"prev\", Node.class);67 THREAD = l.findVarHandle(Node.class, \"thread\", Thread.class);68 WAITSTATUS = l.findVarHandle(Node.class, \"waitStatus\", int.class);69 &#125; catch (ReflectiveOperationException e) &#123;70 throw new ExceptionInInitializerError(e);71 &#125;72 &#125;73&#125; 可以看出，该内部类是一个双向链表，保存前后节点，然后每个节点存储了当前的状态waitStatus、当前线程thread，还可以通过SHARED和EXCLUSIVE两个变量定义为共享模式或者独占模式，通过下面的方式： 1// 标识当前节点在共享模式2static final Node SHARED = new Node();3// 标识当前节点在独占模式4static final Node EXCLUSIVE = null; 然后定义了四个常量： 1CANCELLED，值为1，表示当前的线程被取消；2SIGNAL，值为-1，表示当前节点的后继节点包含的线程需要运行，也就是unpark；3CONDITION，值为-2，表示当前节点在等待condition，也就是在condition队列中；4PROPAGATE，值为-3，表示当前场景下后续的acquireShared能够得以执行； 5默认值为0，表示当前节点在sync队列中，等待着获取锁。67waitStatus 表当前节点的状态值，取值为上面的四个常量。 独占模式首先我们来分析互斥模式，互斥模式作为最常用的模式使用范围很广，比如ReentrantLock，加锁和释放锁就是使用互斥模式来实现的。 独占模式中核心加锁方法是acquire(): 1public final void acquire(int arg) &#123;2 if (!tryAcquire(arg) &amp;&amp;3 acquireQueued(addWaiter(Node.EXCLUSIVE), arg))4 selfInterrupt();5&#125; 其中tryAcquire()方法是没有具体实现的，需要继承者自己实现。tryAcquire()方法返回成功或者失败，如果失败之后先执行addWaiter()添加一个独占式的节点: 1/**2 * Creates and enqueues node for current thread and given mode.3 *4 * @param mode Node.EXCLUSIVE for exclusive, Node.SHARED for shared5 * @return the new node6 */7private Node addWaiter(Node mode) &#123;8 Node node = new Node(mode);//创建一个节点，此处mode是独占式的。9 10 for (;;) &#123;//注意这里是循环11 Node oldTail = tail;12 if (oldTail != null) &#123;13 node.setPrevRelaxed(oldTail);// 如果 tail 节点不是 null，就将新节点(node)的前节点设置为 tail 节点，并且将新节点(node)设置成 tail 节点。14 if (compareAndSetTail(oldTail, node)) &#123;//CAS 将 tail 更新为新节点(node)15 oldTail.next = node;//把原 tail 的 next 设为 node。至此，完成了把新节点 node 插入到原来尾节点的后面，并设置成新的尾节点。16 return node;17 &#125;18 &#125; else &#123;19 initializeSyncQueue();//还没有初始化，就调用 initializeSyncQueue() 方法20 &#125;21 &#125;22&#125; initializeSyncQueue()方法也很简单，就是初始化头结点和尾节点： 1/**2 * Initializes head and tail fields on first contention.3 */4private final void initializeSyncQueue() &#123;5 Node h;6 if (HEAD.compareAndSet(this, null, (h = new Node())))7 tail = h;8&#125; 至此，我们添加了一个新的节点到原来的队列，并且把新加入的节点设置成了尾节点。然后看acquireQueue()方法： 1/**2 * Acquires in exclusive uninterruptible mode for thread already in3 * queue. Used by condition wait methods as well as acquire.4 *5 * @param node the node6 * @param arg the acquire argument7 * @return &#123;@code true&#125; if interrupted while waiting8 */9final boolean acquireQueued(final Node node, int arg) &#123;10 boolean interrupted = false;11 try &#123;12 for (;;) &#123;13 final Node p = node.predecessor();14 if (p == head &amp;&amp; tryAcquire(arg)) &#123;15 setHead(node);16 p.next = null; // help GC17 return interrupted;18 &#125;19 if (shouldParkAfterFailedAcquire(p, node))20 interrupted |= parkAndCheckInterrupt();21 &#125;22 &#125; catch (Throwable t) &#123;23 cancelAcquire(node);24 if (interrupted)25 selfInterrupt();26 throw t;27 &#125;28&#125; shouldParkAfterFailedAcquire()（注意该方法是在循环里面） 这个方法最终会返回true或者false，从这个方法的名称可以看出，该方法的作用是在当前线程获取资源失败后是否挂起当前线程，显然： 返回true，说明前驱节点的waitStatus==-1，是正常情况，那么当前线程需要被挂起，等待以后被唤醒。当前节点是被前驱节点唤醒，就等着前驱节点拿到锁，然后释放锁的时候通知当前线程 返回false，说明当前线程不需要被挂起，因为不符合挂起的条件。 1/**2 * Checks and updates status for a node that failed to acquire.3 * Returns true if thread should block. This is the main signal4 * control in all acquire loops. Requires that pred == node.prev.5 *6 * @param pred node's predecessor holding status7 * @param node the node8 * @return &#123;@code true&#125; if thread should block9 */10private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123;11 int ws = pred.waitStatus;//ws是代表前节点的状态12 if (ws == Node.SIGNAL)//前节点状态是等待唤醒状态，那么当前线程需要被挂起，等待以后被唤醒。13 /*14 * This node has already set status asking a release15 * to signal it, so it can safely park.16 */17 return true;18 if (ws &gt; 0) &#123;//前节点状态是 CANCEL，代表可以忽略，我们删除掉这个节点，再看更前面的一个。19 /*20 * Predecessor was cancelled. Skip over predecessors and21 * indicate retry.22 */23 do &#123;24 node.prev = pred = pred.prev;25 &#125; while (pred.waitStatus &gt; 0);26 pred.next = node;27 &#125; else &#123;//前节点状态是0 或者 PROPAGATE，把状态改成 SIGNAL，但是不挂起。28 /*29 * waitStatus must be 0 or PROPAGATE. Indicate that we30 * need a signal, but don't park yet. Caller will need to31 * retry to make sure it cannot acquire before parking.32 */33 compareAndSetWaitStatus(pred, ws, Node.SIGNAL);34 &#125;35 return false;36&#125; 继续往下看，如果shouldParkAfterFailedAcquire(p, node)返回true，接下来就会执行下面这段代码： 1interrupted |= parkAndCheckInterrupt(); 其实这段代码等价于： 1interrupted = interrupted | parkAndCheckInterrupt(); 接着就是parkAndCheckInterrup方法，用来挂起当前的线程，返回中断标志。代码如下： 1/**2 * Convenience method to park and then check if interrupted.3 *4 * @return &#123;@code true&#125; if interrupted5 */6private final boolean parkAndCheckInterrupt() &#123;7 LockSupport.park(this);8 return Thread.interrupted();9&#125; 注意入队与挂起线程操作不响应中断，只是返回线程中断标志，这一点从上面的代码就可以看出来。 在acquireQueued方法中，for循环是在try语句块里面的，所以这块代码会出现异常，下面有catch语句块。在 JDK8 中，没有catch语句块，有一个finally语句块，这是两个版本之间的差异。这里捕获的异常是tryAcquire抛出的，因为tryAcquire需要继承的类自定义实现，有可能抛出异常。catch异常之后，执行以下方法： 1cancelAcquire(node);//取消加锁，恢复状态2if (interrupted)3 selfInterrupt();4throw t; cancelAcquire方法的源码如下： 1/**2 * Cancels an ongoing attempt to acquire.3 *4 * @param node the node5 */6private void cancelAcquire(Node node) &#123;7 // Ignore if node doesn't exist8 if (node == null)9 return;1011 node.thread = null;// node节点内的线程置为空1213 // Skip cancelled predecessors14 Node pred = node.prev; // pred 是前驱节点15 while (pred.waitStatus &gt; 0)// 找到 pred 结点前面最近的一个状态不为 CANCELLED 的结点16 node.prev = pred = pred.prev;1718 // predNext is the apparent node to unsplice. CASes below will19 // fail if not, in which case, we lost race vs another cancel20 // or signal, so no further action is necessary, although with21 // a possibility that a cancelled node may transiently remain22 // reachable.23 Node predNext = pred.next;2425 // Can use unconditional write instead of CAS here.26 // After this atomic step, other Nodes can skip past us.27 // Before, we are free of interference from other threads.28 node.waitStatus = Node.CANCELLED;//当前节点的状态改成 CANCELLED2930 // If we are the tail, remove ourselves.31 if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123;//如果当前节点是尾节点，则利用 CAS 设置尾结点为 pred 结点32 pred.compareAndSetNext(predNext, null);33 &#125; else &#123;34 // If successor needs signal, try to set pred's next-link35 // so it will get one. Otherwise wake it up to propagate.36 int ws;37 //如果 pred 结点不为头结点38 //并且(pred 结点的状态为 SIGNAL 或者 (ws 小于 0 并且 CAS 设置等待状态为 SIGNAL 成功))39 //并且 pred 结点内的线程不为空40 if (pred != head &amp;&amp;41 ((ws = pred.waitStatus) == Node.SIGNAL || 42 (ws &lt;= 0 &amp;&amp; pred.compareAndSetWaitStatus(ws, Node.SIGNAL))) &amp;&amp;43 pred.thread != null) &#123;44 Node next = node.next;45 if (next != null &amp;&amp; next.waitStatus &lt;= 0)//后继节点不为空 并且后继节点的等待状态小于等于046 pred.compareAndSetNext(predNext, next);//把当前节点的后节点设置成本节点的后节点，也就是说把本节点剔除出去。47 &#125; else &#123;48 unparkSuccessor(node);// 释放节点的后继节点49 &#125;5051 node.next = node; // help GC52 &#125;53&#125; cancleAcquire方法执行完成之后，node 节点就取消了加锁，恢复了队列原有的信号状态，然后从队列列删除了 node 节点。 最后一步，如果interrupted为true，就把当前线程挂起。 简化一点，以上所有做的就是以下几个工作： 尝试获取锁 获取不到锁的话，加入队列并将队列中的前元素的状态改为SIGNAL。 如果出错，就恢复状态，抛出异常。 没有出错就按照需求判断是否需要中断，需要的话中断当前线程。 上面研究了独占模式下 AQS 的原理，再让我们一起看看共享模式做了哪些工作。 共享模式共享模式的获取和释放锁的方法也很容易找到： 1public final void acquireShared(int arg) &#123;2 if (tryAcquireShared(arg) &lt; 0)3 doAcquireShared(arg);4&#125;5public final boolean releaseShared(int arg) &#123;6 if (tryReleaseShared(arg)) &#123;7 doReleaseShared();8 return true;9 &#125;10 return false;11&#125; 需要注意到，获取锁除了基本的方法之外，还有两个增强的方法，这两个方法被用在Semaphore、CountDownLatch和ReentrantReadWriteLock中： 1public final void acquireSharedInterruptibly(int arg)//在acquireShared 方法基础上增加了能响应中断的功能；2 throws InterruptedException &#123;3 if (Thread.interrupted())4 throw new InterruptedException();5 if (tryAcquireShared(arg) &lt; 0)6 doAcquireSharedInterruptibly(arg);7&#125;8public final boolean tryAcquireSharedNanos(int arg, long nanosTimeout)//在acquireSharedInterruptibly基础上增加了超时等待的功能9 throws InterruptedException &#123;10 if (Thread.interrupted())11 throw new InterruptedException();12 return tryAcquireShared(arg) &gt;= 0 ||13 doAcquireSharedNanos(arg, nanosTimeout);14&#125; 还是从获取锁开始看，tryAcquireShared方法跟tryAcquire类似，需要继承者手动实现，返回 0 代表当前线程能够执行，但之后的将会进入等待队列中；返回正数直接执行，之后的线程可能也可以直接执行。 我们还是先看实际主要逻辑所在的doAcquireShared方法： 1private void doAcquireShared(int arg) &#123;2 final Node node = addWaiter(Node.SHARED);//创建一个分享模式的节点，CAS 循环加到队尾，node 就是新加到队尾的那个节点。3 boolean interrupted = false;4 try &#123;5 for (;;) &#123;6 final Node p = node.predecessor();7 if (p == head) &#123;//前节点是 head，证明当前节点是队列里的第一个。8 int r = tryAcquireShared(arg);9 if (r &gt;= 0) &#123;//获取锁成功10 setHeadAndPropagate(node, r);11 p.next = null; // help GC12 return;13 &#125;14 &#125;15 if (shouldParkAfterFailedAcquire(p, node))16 interrupted |= parkAndCheckInterrupt();17 &#125;18 &#125; catch (Throwable t) &#123;19 cancelAcquire(node);20 throw t;21 &#125; finally &#123;22 if (interrupted)23 selfInterrupt();24 &#125;25&#125; 这里与上面独占的部分也很相似，只有一个setHeadAndPropagate方法是新的，主要就是把当前节点设置成head节点，然后依次唤醒后续节点。 1/**2 * Sets head of queue, and checks if successor may be waiting3 * in shared mode, if so propagating if either propagate &gt; 0 or4 * PROPAGATE status was set.5 *6 * @param node the node7 * @param propagate the return value from a tryAcquireShared8 */9private void setHeadAndPropagate(Node node, int propagate) &#123;10 Node h = head; // Record old head for check below11 setHead(node);12 /*13 * Try to signal next queued node if:14 * Propagation was indicated by caller,15 * or was recorded (as h.waitStatus either before16 * or after setHead) by a previous operation17 * (note: this uses sign-check of waitStatus because18 * PROPAGATE status may transition to SIGNAL.)19 * and20 * The next node is waiting in shared mode,21 * or we don't know, because it appears null22 *23 * The conservatism in both of these checks may cause24 * unnecessary wake-ups, but only when there are multiple25 * racing acquires/releases, so most need signals now or soon26 * anyway.27 */28 if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 ||29 (h = head) == null || h.waitStatus &lt; 0) &#123;30 Node s = node.next;31 if (s == null || s.isShared())32 doReleaseShared();33 &#125;34&#125; 下面这一大长串判断的逻辑是这样：首先propagate &gt; 0代表当前线程已经获取到了资源，并且需要唤醒后面阻塞的节点；h.waitStatus &lt; 0 代表旧的头节点后面的节点可以被唤醒；(h = head) == null || h.waitStatus &lt; 0 这个操作是说新的头节点后面的节点可以被唤醒，总结来说： propagate &gt; 0代表当前线程已经获取到了资源，并且需要唤醒后面阻塞的节点。 无论新旧头节点，只要其waitStatus &lt; 0，那么其后面的节点可以被唤醒。 如果上面if返回true，接着获取当前节点的后继节点，这里又会有一个判断，如果后继节点是共享模式或者现在还看不到后继的状态，则都继续唤醒后继节点中的线程。上面if返回true，接着执行doReleaseShared方法，代码如下： 1/**2 * Release action for shared mode -- signals successor and ensures3 * propagation. (Note: For exclusive mode, release just amounts4 * to calling unparkSuccessor of head if it needs signal.)5 */6private void doReleaseShared() &#123;7 /*8 * Ensure that a release propagates, even if there are other9 * in-progress acquires/releases. This proceeds in the usual10 * way of trying to unparkSuccessor of head if it needs11 * signal. But if it does not, status is set to PROPAGATE to12 * ensure that upon release, propagation continues.13 * Additionally, we must loop in case a new node is added14 * while we are doing this. Also, unlike other uses of15 * unparkSuccessor, we need to know if CAS to reset status16 * fails, if so rechecking.17 */18 for (;;) &#123;19 Node h = head;20 if (h != null &amp;&amp; h != tail) &#123;21 int ws = h.waitStatus;22 if (ws == Node.SIGNAL) &#123;//如果状态是等待信号23 if (!h.compareAndSetWaitStatus(Node.SIGNAL, 0))//cas 操作失败的话就循环继续24 continue; // loop to recheck cases25 unparkSuccessor(h);// 唤醒后继节点26 &#125;27 // 如果后继节点还未设置前驱节点的waitStatus为SIGNAL，代表目前无需唤醒或者不存在。28 // 那么就将头节点的waitStatus设置为PROPAGATE，代表在下次acquireShared时无条件地传播29 else if (ws == 0 &amp;&amp;30 !h.compareAndSetWaitStatus(0, Node.PROPAGATE))31 continue; // loop on failed CAS32 &#125;33 if (h == head) // loop if head changed34 break;35 &#125;36&#125; 再回头看doAcquireSharedInterruptibly和doAcquireSharedNanos方法，提供了可以中断和可以超时的获取锁方式： 1/**2 * Acquires in shared interruptible mode.3 * @param arg the acquire argument4 */5private void doAcquireSharedInterruptibly(int arg)6 throws InterruptedException &#123;7 final Node node = addWaiter(Node.SHARED);8 try &#123;9 for (;;) &#123;10 final Node p = node.predecessor();11 if (p == head) &#123;12 int r = tryAcquireShared(arg);13 if (r &gt;= 0) &#123;14 setHeadAndPropagate(node, r);15 p.next = null; // help GC16 return;17 &#125;18 &#125;19 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;20 parkAndCheckInterrupt())21 throw new InterruptedException();22 &#125;23 &#125; catch (Throwable t) &#123;24 cancelAcquire(node);25 throw t;26 &#125;27&#125; 可中断获取锁的逻辑跟前面acquire很像，唯一的区别是当parkAndCheckInterrupt返回true时即线程阻塞时该线程被中断，代码抛出被中断异常。 通过调用lock.tryLock(timeout,TimeUnit)方式达到超时等待获取锁的效果，该方法会在三种情况下才会返回： 在超时时间内，当前线程成功获取了锁； 当前线程在超时时间内被中断； 超时时间结束，仍未获得锁返回false。 具体实现如下： 1/**2 * Acquires in shared timed mode.3 *4 * @param arg the acquire argument5 * @param nanosTimeout max wait time6 * @return &#123;@code true&#125; if acquired7 */8private boolean doAcquireSharedNanos(int arg, long nanosTimeout)9 throws InterruptedException &#123;10 if (nanosTimeout &lt;= 0L)11 return false;12 final long deadline = System.nanoTime() + nanosTimeout;13 final Node node = addWaiter(Node.SHARED);14 try &#123;15 for (;;) &#123;16 final Node p = node.predecessor();17 if (p == head) &#123;18 int r = tryAcquireShared(arg);19 if (r &gt;= 0) &#123;20 setHeadAndPropagate(node, r);21 p.next = null; // help GC22 return true;23 &#125;24 &#125;25 nanosTimeout = deadline - System.nanoTime();26 if (nanosTimeout &lt;= 0L) &#123;27 cancelAcquire(node);28 return false;29 &#125;30 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp;31 nanosTimeout &gt; SPIN_FOR_TIMEOUT_THRESHOLD)32 LockSupport.parkNanos(this, nanosTimeout);33 if (Thread.interrupted())34 throw new InterruptedException();35 &#125;36 &#125; catch (Throwable t) &#123;37 cancelAcquire(node);38 throw t;39 &#125;40&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java中的各种集合类","slug":"Java中的各种集合类","date":"2020-03-12T17:00:00.000Z","updated":"2022-02-20T11:27:13.896Z","comments":true,"path":"2020/03/13/Java中的各种集合类/","link":"","permalink":"http://beritra.github.com/2020/03/13/Java%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E9%9B%86%E5%90%88%E7%B1%BB/","excerpt":"Java中的集合主要分为以下集合类：Map、List、Set、Queue和concurrent包里面供多线程环境下使用的以上几种集合类。","text":"Java中的集合主要分为以下集合类：Map、List、Set、Queue和concurrent包里面供多线程环境下使用的以上几种集合类。 Mapjava.util包中提供的常见Map类包括以下几种。这里乱入了个ConcurrentHashMap，放到下面和其他concurrent包的集合一起讲。 HashMap HashMap 是老生常谈的集合了，学习 HashMap 主要关注点是哈希算法、rehash、数据存储、扩容方式、性能区别和结合ConcurrentHashMap 了解为什么线程不安全，后者怎么解决线程安全问题。 哈希算法先看一下JDK中 hashCode 的生成方式，JDK 1.8 以后都是如下方式： 1static final int hash(Object key) &#123;2 int h;3 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);4&#125; 这里可以看到，key 不是 null 的情况下，都是取key.hashCode()之后无符号右移16位，然后取异或。这里的key.hashCode()是 native 方法，直接在 JVM 中返回 int 型散列值。 无符号右移&gt;&gt;&gt; ：不管正负标志位为0还是1，将该数的二进制码整体右移，左边部分总是以0填充，右边部分舍弃。 位与：第一个操作数的的第n位于第二个操作数的第n位如果都是1，那么结果的第n为也为1，否则为0 为什么要这么做？ 理论上散列值是一个 int 型，如果直接拿散列值作为下标访问 HashMap 主数组的话，考虑到 2 进制 32 位带符号的 int 表值范围从-2147483648到2147483648。前后加起来大概 40 亿的映射空间。只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。 但问题是一个 40 亿长度的数组，内存是放不下的。你想，HashMap 扩容之前的数组初始大小才 16。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来访问数组下标。 顺便说一下，这也正好解释了为什么 HashMap 的数组长度要取 2 的整次幂。因为这样（数组长度 -1）正好相当于一个“低位掩码”。“与”操作的结果就是散列值的高位全部归零，只保留低位值，用来做数组下标访问。以初始长度 16 为例，16-1=15。2 进制表示是00000000 00000000 00001111。和某散列值做“与”操作如下，结果就是截取了最低的四位值。即 1 10100101 11000100 001001012&amp; 00000000 00000000 000011113-------------------------------------------------4 00000000 00000000 00000101 &#x2F;&#x2F;高位全部归零，只保留末四位 但这时候问题就来了，这样就算我的散列值分布再松散，要是只取最后几位的话，碰撞也会很严重。更要命的是如果散列本身做得不好，分布上成等差数列的漏洞，恰好使最后几个低位呈现规律性重复，就无比蛋疼。 这时候“扰动函数”的价值就体现出来了，说到这里大家应该猜出来了。看下面这个图: 右位移 16 位，正好是 32bit 的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。 扩容HashMap 两个关键的初始化参数： 1static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 162static final float DEFAULT_LOAD_FACTOR = 0.75f; 前者是初始化容量 16，即新建一个 HashMap 的时候，如果不指定长度，则容量为 16。 后者是加载因子，即当实际长度除以容量高于该因子的时候，进行扩容操作。默认为 0.75，所以 HashMap 空间占用大于 3/4 的时候就开始扩容了。扩容后的容量是原来的两倍。 HashMap 的 resize 不是简单的把长度扩大，而是有下面两个步骤： 扩容：创建一个新的 Entry 空数组，长度是原数组的2倍。 reash：遍历原 Entry 数组，把所有的 Entry 重新 hash 到新数组。为什么要重新 hash 呢？因为长度扩大以后，hash 的规则也随之改变。 让我们回顾一下 hash 公式： index = hashCode(key) &amp; (length - 1) 当原数组长度为 8 时，hash 运算是和111B做与运算；新数组长度为 16，hash 运算是和1111B做与运算。hash 结果显然不同。 那么这里为什么要用 map 容量减去 1 这个数字哪？好处有两个： 分布均匀 速度更快 在 HashMap 的源码中。get和put方法会根据 key 的 hash 值找到这个 entry 在 hash 表数组中的位置，源码如下： 1if ((p &#x3D; tab[i &#x3D; (n - 1) &amp; hash]) &#x3D;&#x3D; null)2 tab[i] &#x3D; newNode(hash, key, value, null); 按照我们理想的状况，hashMap 的存取就是 O(1)，也就是直接根据 hashcode 就可以找到它，每个 bucket 只存储一个节点，链表指向都是null，这样就比较开心了，不要出现一个链表很长的情况。 所以我们希望它能分布的均匀一点，如果让我们设计的话，我们肯定是直接对长度取模：hashcode % length，但 HashMap 的设计者却不是这样写的，它写成了 2 进制运算，因为当容量一定是2^n时，h &amp; (length - 1) == h % length，并且位运算的速度要高于取模。 另外，元素在重新计算 hash 之后，因为 n 变为 2 倍，新的 index 的二进制就是在前面多了一位，比如原来的容量为 8 的时候，元素下标为 5，扩容到 16 之后，根据多的那一位是 0 还是 1，元素下标只需要 +8 或者在原位置就可以了，也就是说 resize 操作也会更快。 存储方式HashMap 实际是一种“数组+链表”数据结构。在 put 操作中，通过内部定义算法寻址找到数组下标，将数据直接放入此数组元素中，若通过算法得到的该数组元素已经有了元素（俗称 hash 冲突，链表结构出现的实际意义也就是为了解决 hash 冲突的问题）。将会把这个数组元素上的链表进行遍历，将新的数据放到链表末尾。 通过哈希算法从寻止上能够高效的找到对应的下标，但是随着数据的增长，哈希冲突碰撞过多。在寻找数据时，先找到链表，然后通过遍历在寻找对应数据，如此将会使得 get 数据效率越来越低。在 JDK 1.8 中，链表元素数量大于等于 8 将会重组该链表结构形成为“红黑树结构”，这种结构使得在 hash 冲突碰撞过多情况下，get效率比链表的效率高很多。 性能在没有哈希冲突的情况下，HashMap 存取元素的时间复杂度是 O(1)，但是这只是理想情况。当冲突不多的时候，重复元素以链表形式存储，时间复杂度是 O(N)，当数据量大的时候，链表转换为红黑树，时间复杂度变成 O(LogN) 线程安全和其他局限HashMap 不是线程安全的，另外如果 HashMap 的 key 是自定义类，需要重写hashCode()方法，并且由于 HashMap 的效率高度依赖hashCode()，需要保证散列分布尽量均匀。 都知道 HashMap 不是线程安全的，那么在哪些环节导致了他线程不安全？ 插入数据的时候 1tab[i] = newNode(hash, key, value, null); 假如 A 线程和 B 线程同时添加元素，然后计算出了相同的哈希值对应了相同的数组位置，因为此时该位置还没数据，然后对同一个数组位置添加，B 的写入操作就会覆盖 A 的写入操作造成 A 的写入操作丢失。 修改数据的时候 跟上面同样，多个线程同时修改数据，可能产生错误。 扩容的时候 线程 1 执行put时，因为元素个数超出threshold而导致 rehash，线程 2 此时执行get，有可能导致这个问题。 因为在 resize 的时候，是计算新的容量和threshold，在创建一个新 hash 表，最后将旧 hash 表中元素 rehash 到新的 hash 表中。如果在这个期间，另一个线程执行读取操作，有可能get到null。 那么 ConcurrentHashMap 如何保证线程安全？这个在另一篇文章中单独叙述。 LinkedHashMapLinkedHashMap 是继承自 HashMap 的，跟 HashMap 最大的区别是，他是基于 Hash 表和链表的实现，并且依靠着双向链表保证了迭代顺序是插入的顺序。 来看看 HashMap 和 LinkedHashMap 的结构图，是不是秒懂了。LinkedHashMap 其实就是可以看成 HashMap 的基础上，多了一个双向链表来维持顺序。 可以用 LinkedHashMap 实现最近访问算法，即最近访问过的元素在最前面， LinkedHashMap 有这么一个构造方法。 public LinkedHashMap(int initialCapacity, float loadFactor,boolean accessOrder) accessOrde为true的时候按照元素最后访问时间排序（LRU算法：最近最久使用），为false则是按照插入顺序排序，默认为false. TreeMapTreeMap 是基于红黑树的实现，具有如下特点： 不允许出现重复的 key； 可以插入null键，null值； 可以对元素进行排序； 无序集合（插入和遍历顺序不一致）； 由于是基于红黑树，TreeMap 在插入、删除、搜索的时候，时间复杂度都是 O(LogN)。红黑树的结构单独另外说明，这里就不赘述。 EnumMapEnumMap 是专门为枚举类型量身定做的 Map 实现。虽然使用其它的 Map 实现（如 HashMap）也能完成枚举类型实例到值得映射，但是使用 EnumMap 会更加高效：它只能接收同一枚举类型的实例作为键值，并且由于枚举类型实例的数量相对固定并且有限，所以 EnumMap 使用数组来存放与枚举类型对应的值。这使得 EnumMap 的效率非常高。EnumMap 在内部使用枚举类型的ordinal()得到当前实例的声明次序，并使用这个次序维护枚举类型实例对应值在数组的位置。 在 key 是枚举类的时候，EnumMap 可以用来代替 HashMap，并且由于是数组实现，性能更好。 HashTableHashtable 与 HashMap 的简单比较 HashTable 基于 Dictionary 类，而 HashMap 是基于 AbstractMap。Dictionary 是任何可将键映射到相应值的类的抽象父类，而 AbstractMap 是基于 Map 接口的实现，它以最大限度地减少实现此接口所需的工作。 HashMap 的 key 和 value 都允许为null，而 Hashtable 的 key 和 value 都不允许为null（出于并发上取不到值的考虑）。HashMap 遇到 key 为null的时候，调用putForNullKey方法进行处理，而对 value 没有处理；Hashtable遇到null，直接返回NullPointerException。 Hashtable 方法是同步，而 HashMap 则不是。我们可以看一下源码，Hashtable 中的几乎所有的 public 的方法都是synchronized的，而有些方法也是在内部通过synchronized代码块来实现。所以有人一般都建议如果是涉及到多线程同步时采用 HashTable，没有涉及就采用 HashMap，但是在 Collections 类中存在一个静态方法：synchronizedMap()，该方法创建了一个线程安全的 Map 对象，并把它作为一个封装的对象来返回。 HashMap的初始容量为 16，Hashtable 初始容量为 11，两者的填充因子默认都是 0.75。 两者计算 hash 的方法不同 Hashtable 计算 hash 是直接使用 key 的 hashcode 对 table 数组的长度直接进行取模 1int hash = key.hashCode();2int index = (hash &amp; 0x7FFFFFFF) % tab.length; HashMap 计算 hash 对 key 的 hashcode 的前后 16 位进行了异或操作，以获得更好的散列值，然后对 table 数组长度取模（实际上是位操作，增加效率） 1 static final int hash(Object key) &#123;2 int h;3 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);4 &#125;5 6static int indexFor(int h, int length) &#123;7 return h &amp; (length-1);8 &#125; IdentityHashMapIdentityHashMap 是一致性哈希表，使用引用相等，而不是equals方法来比较两个对象的相等性。因此，IdentityHashMap 中，如果存在两个键 key1 和 key2，当且仅当key1==key2时，两个键相等，而其他大部分的哈希表，当且仅当k1 == null ? k2 == null : k1.equals(k2)时，两个键才认为是相等的。 IdentityHashMap 使用System.identityHashCode来确定对象的哈希码，该方法返回对象的地址。 看下IdentityHashMap的存储原理图，和 HashMap 不同，HashMap 是通过数组+拉链法存储元素并解决哈希冲突的。IdentityHashMap 将所有的 key 和 value 都存储到Object[]数组 table 中，并且 key 和 value 相邻存储，当出现哈希冲突时，会往下遍历数组，直到找到一个空闲的位置。注意，数组第一个位置存储的是 key，第二个位置存储的是 value。因此奇数位置处存储的是 key，偶数位置处存储的是 value。 IdentityHashMap 同样允许空的键和值，但是不保证 map 中的顺序，尤其是不保证顺序会恒定不变。 WeakHashMap和 HashMap 一样，WeakHashMap 也是一个散列表，它存储的内容也是键值对(key-value)映射，而且键和值都可以是null。不过 WeakHashMap 的键是“弱键”。 当弱引用指向的对象只能通过弱引用（没有强引用或弱引用）访问时，GC会清理掉该对象，之后，引用对象会被放到ReferenceQueue中。在 Entry 的构造函数中可以得知，通过super(key, queue)将 key 保存为弱引用，通过this.value = value将 value 保存为强引用。当 key 中的引用被 gc 掉之后，在下次访问 WeakHashMap（调用expungeStaleEntries函数）时相应的 entry 也会自动被移除。 WeakHashMap 并不是你什么也不干它就能自动释放内部不用的对象的，而是在你访问它的内容的时候释放内部不用的对象。 Listjava.util包中提供的常见List类包括以下几种。 从刚学 Java 的前几天起，大概就会见到这个问题：LinkedList 和 ArrayList 有什么共同点和区别？ 共同点： 二者都是继承自 AbstractList 抽象类，AbstractList 实现了 List 接口中除了size()、get(int location)之外的方法。 二者都是线程不安全的。 区别： ArrayList 是实现了基于动态数组的数据结构，而 LinkedList 是基于链表的数据结构； 数据更新和查找时，ArrayList 可以直接通过数组下标访问，所以效率更高。 数据增加和删除的时候，ArrayList 需要移动其他元素的位置，而 LinkedList 只需要修改一个指针，所以后者效率更高。 VectorVector 是同样继承于AbstractList的一个列表，而它是线程安全的，实现方式是对所有数据操作的方法添加了 synchronized 关键字。其与 ArrayList 的差别如下： 构造函数，ArrayList 比 Vector 稍有深度，Vector 默认数组长度为 10，创建是设置。 扩容方法 grow()，ArrayList 通过位运算进行扩容，而 Vector 则通过增长系数（创建是设置，如果过为空，则增长一倍） Vector 方法调用是线程安全的。 成员变量有所不同 StackStack 栈是 Vector 的一个子类，它实现了一个标准的后进先出的栈。 他的方法很简单，只有empty()、peek()、pop()、push(Object element)、search(object element)这几个。其中 peek 和 pop 的返回值都是堆栈顶部的对象，但是前者只是查看，后者是移除。 Setjava.util包中提供的常见Set类包括以下几种。 HashSet 没什么好说的，其实就是把 HashMap 封装了一层，从 HashSet 的构造方法可以看出，就是维护了一个 HashMap，数据的增删改查也是调用的 HashMap 的方法。 TreeSet 也是一样，其实就是 TreeMap 套了个皮。 EnumSet 就不一样了，跟 EnumMap 其实没有什么关系。EnumSet 是一个 Set 集合的抽象类，其有两个实现类 JumboEnumSet 和 RegularEnumSet，在使用的时候放入的必须是枚举类型，其特点是速度非常快。 EnumSet 的默认子类 RegularEnumSet 和 JumboEnumSet 实现原理都是基于位运算向量，位运算向量的原理就是用一个位表示一个元素的状态（元素的状态只有两种），用一组位表示一个集合的状态，每个位对应一个元素，譬如一个枚举类 DemoEnum 有6个枚举值，则 EnumSet 集合就可以通过一个 byte 字节从右到左（二进制低到高位）来表示，不用的位上用 0 填充，用的位上每个 bit 位代表一个枚举值，1 表示包含该枚举值，0 表示不含该枚举值。因此位向量能表示的枚举值个数与向量长度有关，上面例子中一个 byte 类型最多能表示 8 个枚举值，所以 EnumSet 抽象类的两个实现类 RegularEnumSet 和 JumboEnumSet 分别定义了不同的向量长度。RegularEnumSet 使用 64 位的 long 类型变量作为位向量，而 JumboEnumSet 使用一个 long 类型数组作为向量（数组内存连续），故当我们通过 EnumSet 的工厂方法创建 EnumSet 集合时 EnumSet 会通过判断枚举类的枚举值数量决定使用两个子类的哪一个，如果枚举值个数小于等于 64 就用 RegularEnumSet，大于 64 就用 JumboEnumSet。 简单来说 EnumSet 就是一个高效的枚举类集合。 Queue 队列(Queue)可以当做一种特殊的线性表，遵循先进先出原则。而双向队列(Deque)，是 Queue 的一个子接口，双向队列是指该队列两端的元素既能入队(offer)也能出队(poll),如果将 Deque 限制为只能从一端入队和出队，则可实现栈的数据结构。 PriorityQueue 有一种特殊的队列，叫做优先队列。优先队列的作用是能保证每次取出的元素都是队列中权值最小的（Java的优先队列每次取最小元素，C++的优先队列每次取最大元素）。这里牵涉到了大小关系，元素大小的评判可以通过元素本身的自然顺序（natural ordering），也可以通过构造时传入的比较器（Comparator，类似于C++的仿函数）。 Java 中 PriorityQueue 实现了 Queue 接口，不允许放入null元素；其通过堆实现，具体说是通过完全二叉树（complete binary tree）实现的小顶堆（任意一个非叶子节点的权值，都不大于其左右子节点的权值），也就意味着可以通过数组来作为 PriorityQueue 的底层实现。 最小堆的完全二叉树有一个特性是根节点必定是最小节点，子女节点一定大于其父节点。还有一个特性是叶子节点数量 = 全部非叶子节点数量 +1。 每次增删元素都有可能对树结构进行调整，所以 PriorityQueue 队列不适合进场出队入队的频繁操作，但是他的优先级特性非常适合一些对顺序有要求的数据处理场合。 concurrent包ConcurrentHashMap上面 HashMap 已经说到了 HashMap 在多个线程同时存取或者触发扩容的时候，都有可能出现错误，导致操作被覆盖或者丢失，那么怎么解决这个问题呐？ 第一反应当然是加锁，HashTable 就是这么做的，使用了synchronized关键字。虽然解决了并发访问的安全性问题，但是性能不怎么样。HashTable 中的增删改、甚至equals、toString方法等等都是方法级的锁，所以同时只能一个线程去操作，导致效率问题。 在 JDK 1.7 及之前版本，ConcurrentHashMap 采用的是 Segment 分段锁，即将数据分为一段一段的存储，然后给每一段数据加一把锁。当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 在 JDK 1.8 以后，ConcurrentHashMap 取消了 Segment 分段锁，采用 CAS 和synchronized来保证并发安全。 数据结构与 HashMap 1.8 的结构类似，数组+链表/红黑二叉树(链表长度 &gt;8 时，转换为红黑树)。 通过 JDK 的源码和官方文档看来， 他们认为的弃用分段锁的原因由以下几点： 加入多个分段锁浪费内存空间。 生产环境中， map 在放入时竞争同一个锁的概率非常小，分段锁反而会造成更新等操作的长时间等待。 为了提高 GC 的效率。 在 JDK 11 下对 HashMap 和 ConcurrentHashMap 进行了简单测试，生成 5000 万条随机数然后插入，分别消耗 16348 毫秒和 19194 毫秒。其中包括随机数生成、插入和扩容的时间消耗，可见两者之间性能差距不大。 然后使用 HashTable 在单线程下插入，同样的数据量时间在 17 秒所有，跟 HashMap 差别不大，可以当做是误差范围内。然后使用 20 个线程插入，消耗时间在 15 秒左右，提升并不明显。奇怪的是 ConcurrentHashMap 却使用了 45 秒。然后缩小数据量，在 1000 万以下的时候，ConcurrentHashMap 的插入速度又好于 HashTable 了。这个现象很有意思，有空了详细研究一下产生这个问题的原因。 ConcurrentHashMap 的整体性能要优于 HashTable，但是某些场景不能替代 HashTable，例如强一致性的场景，ConcurrentHashMap 的get、size等方法都没有加锁，ConcurrentHashMap 是弱一致性的。更多关于 ConcurrentHashMap 的原理在另一个文章中单独分析。 ConcurrentSkipListMapConcurrentSkipListMap 提供了一种线程安全的并发访问的排序映射表。内部是 SkipList（跳表）结构实现，在理论上能够在 O(logN) 时间内完成查找、插入、删除操作。 ConcurrentHashMap 与 ConcurrentSkipListMap 性能测试在4线程1.6万数据的条件下，ConcurrentHashMap 存取速度是 ConcurrentSkipListMap 的4倍左右。 但 ConcurrentSkipListMap 有几个 ConcurrentHashMap 不能比拟的优点： 1、ConcurrentSkipListMap 的 key 是有序的。 2、ConcurrentSkipListMap 支持更高的并发。ConcurrentSkipListMap 的存取时间是 log(N)，和线程数几乎无关。也就是说在数据量一定的情况下，并发的线程越多， ConcurrentSkipListMap 越能体现出他的优势。 CopyOnWriteArrayList先讲一下什么是Copy-On-Write，通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。对 CopyOnWrite 容器进行并发的读的时候，不需要加锁，因为当前容器不会添加任何元素。所以 CopyOnWrite 容器也是一种读写分离的思想，延时更新的策略是通过在写的时候针对的是不同的数据容器来实现的，放弃数据实时性达到数据的最终一致性。 1public boolean add(E e) &#123;2 synchronized (lock) &#123;3 Object[] es = getArray();4 int len = es.length;5 es = Arrays.copyOf(es, len + 1);6 es[len] = e;7 setArray(es);8 return true;9 &#125;10&#125;1112public E set(int index, E element) &#123;13 synchronized (lock) &#123;14 Object[] es = getArray();15 E oldValue = elementAt(es, index);1617 if (oldValue != element) &#123;18 es = es.clone();19 es[index] = element;20 setArray(es);21 &#125;22 return oldValue;23 &#125;24&#125; CopyOnWriteArrayList 的实现也不复杂，对有并发风险的操作加了锁。注意这里的内部数组是volatile修饰的，写线程对数组引用的修改对读线程是可见的。由于在写数据的时候，是在新的数组中插入数据的，从而保证读写实在两个不同的数据容器中进行操作。 参考资料： JDK 源码中 HashMap 的 hash 方法原理是什么？ HashMap底层实现原理 【java并发】造成HashMap非线程安全的原因 IdentityHashMap源码详解 WeakHashMap实现原理及源码分析 EnumSet 原理相关 Java源码分析：HashMap 1.8 相对于1.7 到底更新了什么？（这篇非常细致）","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"集合","slug":"集合","permalink":"http://beritra.github.com/tags/%E9%9B%86%E5%90%88/"}]},{"title":"Linux文本处理三剑客grep、awk和sed","slug":"Linux文本处理三剑客grep、awk和sed","date":"2020-03-07T13:15:48.000Z","updated":"2020-06-06T08:35:45.241Z","comments":true,"path":"2020/03/07/Linux文本处理三剑客grep、awk和sed/","link":"","permalink":"http://beritra.github.com/2020/03/07/Linux%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2grep%E3%80%81awk%E5%92%8Csed/","excerpt":"awk、grep、sed 是 Linux 操作文本的三大利器，合称文本三剑客，也是必须掌握的 Linux 命令之一。三者的功能都是处理文本，但侧重点各不相同，其中属 awk 功能最强大，但也最复杂。grep 更适合单纯的查找或匹配文本，sed 更适合编辑匹配到的文本，awk 更适合格式化文本，对文本进行较复杂格式处理。","text":"awk、grep、sed 是 Linux 操作文本的三大利器，合称文本三剑客，也是必须掌握的 Linux 命令之一。三者的功能都是处理文本，但侧重点各不相同，其中属 awk 功能最强大，但也最复杂。grep 更适合单纯的查找或匹配文本，sed 更适合编辑匹配到的文本，awk 更适合格式化文本，对文本进行较复杂格式处理。 grepLinux 系统中 grep 命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。grep 全称是 Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 grep 可用于 shell 脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回 0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回 2。我们利用这些返回值就可进行一些自动化的文本处理工作。 命令的基本格式： 1grep [option] pattern file 即便不熟悉这个命令，应该大多数同学也用过查询进程的命令： 1ps -ef|grep xxxx 这就是 grep 的一个基本用法，从所有进程中搜索某个进程。 grep 常用的参数如下： -A&lt;行数 x&gt;：除了显示符合范本样式的那一列之外，并显示该行之后的 x 行内容。 -B&lt;行数 x&gt;：除了显示符合样式的那一行之外，并显示该行之前的 x 行内容。 -C&lt;行数 x&gt;：除了显示符合样式的那一行之外，并显示该行之前后的 x 行内容。 -c：统计匹配的行数 -e ：实现多个选项间的逻辑or 关系 -E：扩展的正则表达式 -f 文件名：从文件获取 PATTERN 匹配 -F ：相当于fgrep -i –ignore-case #忽略字符大小写的差别。 -n：显示匹配的行号 -o：仅显示匹配到的字符串 -q： 静默模式，不输出任何信息 -s：不显示错误信息。 -v：显示不被 pattern 匹配到的行，相当于[^] 反向匹配 -w ：匹配 整个单词 前三个 A、B、C 参数很容易理解，举个栗子，假设我们有一个文件，文件名是 test，内容是从 1 到 9，每个数字一行： 1➜ grep -A2 7 test273849 -A2 7 的效果就是找到 7 ，然后输出 7 后面两行。 同理，-B2 7和-C2 7就是找到 7 ，然后分别输出 7 前面两行和前后两行： 1➜ grep -B2 7 test25364756➜ grep -C2 7 test758697108119 继续，假设我们有个名叫 test 的文件内容如下： 1➜ cat test2aaaa3bbbbbb4AAAaaa5BBBBASDABBDA grep -c命令的作用就是输出匹配到的行数，比如我们想找包含aaa的有几行，一眼就能看出来有两行，第一行和第三行都包含： 1➜ grep -c aaa test22 grep -e命令是实现多个匹配之间的或关系，比如我们想找包含aaaa或者bbbb的，显然应该返回第一行和第二行： 1➜ grep -e aaaa -e bbbb test2aaaa3bbbbbb grep -F相当于fgrep命令，就是将pattern视为固定字符串。比如搜索&#39;aa*&#39;不带-F和带上，区别如下： 1➜ grep 'aa*' test2aaaa3AAAaaa45➜ grep -F 'aa*' test 可以看到第二次就找不到了，因为搜索的是 aa*这个字符串，而不是正则表达式。 grep -f 文件名的使用方法是把后面这个文件里的内容当做pattern。比如我们有个文件，名字是 grep.txt，然后内容是aa*，使用方法如下： 1➜ grep -f grep.txt test2aaaa3AAAaaa 实际上等同于grep &#39;aa*&#39; test grep -i --ignore-case作用是忽略大小写。 grep -n显示匹配的行号，就是多显示了个行号，不用细说。 grep -o仅显示匹配到的字符串，还是用刚才的aa*距离，之前显示的都是匹配到的字符所在的整行，这个命令是只显示匹配到的字符： 1➜ grep -o 'aa*' test2aaaa3aaa grep -q不打印匹配结果。刚看到这个我疑惑了半天，让你搜索字符串，你不给我结果那有啥用？然后发现还有一条很多教程没说：如果有匹配的内容则立即返回状态值 0。所以一般用在shell脚本中，在 if 判断里面。 grep -s不显示错误信息，不解释。 grep -v显示不被匹配到的行，相当于[^]反向匹配，最常见的还是用在查找线程的命令里，有时候会打印grep线程，可以再加上这么一个去除自己： 1➜ ps -ef|grep Typora2 501 91616 1 0 五11上午 ?? 13:39.32 /Applications/Typora.app/Contents/MacOS/Typora3 501 14814 93748 0 5:33下午 ttys002 0:00.00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn Typora4 5➜ ps -ef|grep Typora|grep -v grep6 501 91616 1 0 五11上午 ?? 13:39.32 /Applications/Typora.app/Contents/MacOS/Typora 可以看到第二次就没有打印grep线程自身 grep -w匹配整个单词，只有完全符合pattern的单次才会匹配到： 1➜ grep aaa test2aaaa3AAAaaa45➜ grep -w aaa test 可以看到第二次结果为空，因为没有aaa这个单词。 关于正则的高级用法就不再深入研究了，改日再统一整理。 sedsed 命令的作用是利用脚本来处理文本文件。使用方法： 1sed [-hnV][-e&lt;script&gt;][-f&lt;script文件&gt;][文本文件] 参数说明： -e&lt;script&gt;或--expression=&lt;script&gt; 以选项中指定的 script 来处理输入的文本文件，这个-e可以省略，直接写表达式。 -f&lt;script文件&gt;或--file=&lt;script文件&gt;以选项中指定的 script 文件来处理输入的文本文件。 -h或--help显示帮助。 -n 或 --quiet 或 --silent 仅显示 script 处理后的结果。 -V 或 --version 显示版本信息。 动作说明： a：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s：取代，通常这个 s 的动作可以搭配正规表示法，例如 1,20s/old/new/g 。 我们先准备一个文件，名为test做测试，内容如下： 1➜ cat test 2HELLO LINUX! 3Linux is a free unix-type opterating system. 4This is a linux testfile! 5Linux test 增加内容使用命令sed -e 3a\\newLine testfile这个命令的意思就是，在第三行后面追加newLine这么一行字符，字符前面要用反斜线作区分。执行完毕之后可以看到结果： 1➜ sed -e 3a\\newline test 2HELLO LINUX! 3Linux is a free unix-type opterating system. 4This is a linux testfile! 5newline6Linux test 但是注意，这个只是将文字处理了，没有写入到文件里，文件里还是之前的内容。 其实 a 前面是可以匹配字符串，比如我们只想在出现 Linux 的行后面追加，就可以：sed -e /Linux/a\\newline test 两个斜线之间的内容是需要匹配的内容。可以看出，只有第二、第四行有Linux，所以结果如下： 1➜ sed -e /Linux/a\\newline test 2HELLO LINUX! 3Linux is a free unix-type opterating system. 4newline5This is a linux testfile! 6Linux test 7newline 这里用双引号把整个表达式括起来也可以，还方便处理带空格的字符。 sed -e /Linux/a\\newline test等效于sed &quot;/Linux/a newline&quot; test 插入内容跟 a 类似，sed 3i\\newline test是在第三行前面插入newline: 1➜ sed 3i\\newline test2HELLO LINUX! 3Linux is a free unix-type opterating system. 4newline5This is a linux testfile! 6Linux test sed /Linux/i\\newline test是在所有匹配到Linux的行前面插入： 1➜ sed /Linux/i\\newline test2HELLO LINUX! 3newline4Linux is a free unix-type opterating system. 5This is a linux testfile! 6newline7Linux test 可以看出插入的用法和增加很相似。 删除删除的字符是d，用法跟前面也很相似，就不赘述，例子如下： 1➜ sed '/Linux/d' test 2HELLO LINUX! 3This is a linux testfile! 可以看到删除了匹配到的两行。 替换替换也是一样，字符是c。举个栗子： 1➜ sed '/Linux/c\\Windows' test 2HELLO LINUX! 3Windows4This is a linux testfile! 5Windows 替换还有个字符是 s，但是用法由不太一样了，最常见的用法：sed &#39;s/old/new/g&#39;其中old代表想要匹配的字符，new是想要替换的字符，比如： 1➜ sed 's/Linux/Windows/g' test2HELLO LINUX! 3Windows is a free unix-type opterating system. 4This is a linux testfile! 5Windows test 这里的/g的意思是一行中的每一次匹配，因为一行中可能匹配到很多次。我们拿一个新的文本文件做例子： 1➜ cat test22aaaaaaaaaaa3bbbbbabbbbb4cccccaacccc 假设我们想把一行中的第三次及以后出现的a变成大写A，那应该这么写： 1➜ sed 's/a/A/3g' test22aaAAAAAAAAA3bbbbbabbbbb4cccccaacccc 可以看出只有第一行的有的改了，因为第二第三行没有这么多a出现。 关于s还有很多用法，还是回到第一个文件，比如可以用/^/和/$/分别代表行首和行尾： 1➜ sed 's/^/###/g' test2###HELLO LINUX! 3###Linux is a free unix-type opterating system. 4###This is a linux testfile! 5###Linux test 6 7➜ sed 's/$/---/g' test8HELLO LINUX! ---9Linux is a free unix-type opterating system. ---10This is a linux testfile! ---11Linux test --- 这个其实就是正则表达式的语法，其他类似语法还有： ^ 表示一行的开头。如：/^#/ 以#开头的匹配。 $ 表示一行的结尾。如：/}$/ 以}结尾的匹配。 \\&lt; 表示词首。 如：`\\ 表示以 abc 为首的詞。 \\&gt; 表示词尾。 如：abc\\&gt; 表示以 abc 結尾的詞。 . 表示任何单个字符。 * 表示某个字符出现了0次或多次。 [ ] 字符集合。 如：[abc] 表示匹配a或b或c，还有 [a-zA-Z] 表示匹配所有的26个字符。如果其中有^表示反，如 [^a] 表示非a的字符 以上的所有用法，还可以在字符前面增加行号或者匹配。什么意思呐？比如你想在第一和第二行后面增加一行内容newline，就是： 1➜ sed '1,2a\\newline' test2HELLO LINUX! 3newline4Linux is a free unix-type opterating system. 5newline6This is a linux testfile! 7Linux test 其他操作同理。不止可以用数字来限定范围，还可以用匹配来限定，只需要用//括起来： 1➜ sed '/LINUX/,/linux/i\\test' test2test3HELLO LINUX! 4test5Linux is a free unix-type opterating system. 6test7This is a linux testfile! 8Linux test 这里的意思是，从匹配到LINUX的那一行，到匹配到linux的那一行，也就是 123 这三行 ，都做插入操作。 多个匹配用-e命令可以执行多次匹配，相当于顺序依次执行两个sed命令： 1➜ sed -e 's/Linux/Windows/g' -e 's/Windows/Mac OS/g' test2HELLO LINUX! 3Mac OS is a free unix-type opterating system. 4This is a linux testfile! 5Mac OS test 这个命令其实就是先把Linux替换成Windows，再把Windows替换成Mac OS。 写入文件上面介绍的所有文件操作都支持在缓存中处理然后打印到控制台，实际上没有对文件修改。想要保存到原文件的话可以用&gt; file或者-i来保存到文件 awkawk 是一个强大的文本分析工具，相对于 grep 的查找，sed 的编辑，awk 在其对数据分析并生成报告时，显得尤为强大。简单来说awk 就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 语法1awk [选项参数] 'script' var=value file(s)2或3awk [选项参数] -f scriptfile var=value file(s) 参数说明： -F fs or –field-separator fs指定输入文件折分隔符，fs是一个字符串或者是一个正则表达式，如-F:。 -v var=value or –asign var=value赋值一个用户定义变量。 -f scripfile or –file scriptfile从脚本文件中读取awk命令。 基本用法最基本的用法是awk 动作 文件名。我们先准备一个文件test： 1➜ cat test 22 this is a test33 Are you like awk4This's a test510 There are orange,apple,mongo 然后输入awk &#39;{print $1,$4}&#39; test就可以看到： 12 a23 like3This's 410 orange,apple,mongo 对比可以很清楚的发现，这行语句的作用是打印每行的第一个和第四个单词。这里如果是$0的话就是把整行都输出出来。 awk -F命令可以指定使用哪个分隔符，默认是空格或者 tab 键： 1➜ awk -F, '&#123;print $2&#125;' test2345apple 可以看出只有最后一行有输出，因为用逗号做分割，之后最后一行被分成了10 There are orange、apple和mongo三项，然后我们要的是第二项。 还可以同时使用多个分隔符： 1➜ awk -F '[ ,]' '&#123;print $1,$2,$5&#125;' test 22 this test33 Are awk4This's a 510 There apple 这个例子便是使用空格和逗号两个分隔符。 匹配项中可以用正则表达式，比如： 1➜ awk '/^This/' test2This's a test 匹配的就是严格以This开头的内容。 还可以取反： 1➜ awk '$0 !~ /is/' test 23 Are you like awk310 There are orange,apple,mongo 这一个的结果就是去掉带有is的行，只显示其余部分。 从文件中读取：awk -f {awk脚本} {文件名}，这个很好理解，就不再做解释。 变量awk中有不少内置的变量，比如$NF代表的是分割后的字段数量，相当于取最后一个。 1➜ awk '&#123;print $NF&#125;' test 2test3awk4test5orange,apple,mongo 可以看出都是每行的最后一项。 其他的内置变量还有： FILENAME：当前文件名 FS：字段分隔符，默认是空格和制表符。 RS：行分隔符，用于分割每一行，默认是换行符。 OFS：输出字段的分隔符，用于打印时分隔字段，默认为空格。 ORS：输出记录的分隔符，用于打印时分隔记录，默认为换行符。 OFMT：数字输出的格式，默认为％.6g。 函数awk还提供了一些内置函数，方便对原始数据的处理。主要如下： toupper()：字符转为大写。 tolower()：字符转为小写。 length()：返回字符串长度。 substr()：返回子字符串。 sin()：正弦。 cos()：余弦。 sqrt()：平方根。 rand()：随机数。 条件awk允许指定输出条件，只输出符合条件的行。输出条件要写在动作的前面： 1awk '条件 动作' 文件名 还是刚才的例子，用逗号分隔之后有好几个空白行，我们加上限制条件，匹配后为空的不显示： 1➜ awk -F, '$2!=\"\" &#123;print $2&#125;' test2apple 可以看到就只剩下apple了。 if 语句awk提供了if结构，用于编写复杂的条件。比如： 1➜ awk '&#123;if ($2 &gt; \"t\") print $1&#125;' test22 这一句的完整含义应该是：把每一行按照空格分割之后，如果第二个单词大于t，就输出第一个单词。这里对字符的大小判断应该是基于字符长度和 unicode 编码。 以上这些只是三剑客的基础用法，包括正则表达式也有很多技巧，更多扩展内容网上也很多了，可以自行搜索，或者翻阅下面的参考文章。 参考文章 SED 简明教程 Linux sed 命令 Linux awk 命令 awk 入门教程","categories":[{"name":"Linux","slug":"Linux","permalink":"http://beritra.github.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://beritra.github.com/tags/Linux/"}]},{"title":"23种设计模式长文总结","slug":"23种设计模式长文总结","date":"2020-03-04T16:59:47.000Z","updated":"2020-05-25T12:43:33.143Z","comments":true,"path":"2020/03/05/23种设计模式长文总结/","link":"","permalink":"http://beritra.github.com/2020/03/05/23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E9%95%BF%E6%96%87%E6%80%BB%E7%BB%93/","excerpt":"一次性将设计模式整理清楚，包括 23 种设计模式的优劣势、使用场景、示例以及 JDK 和常见框架中的使用方法。","text":"一次性将设计模式整理清楚，包括 23 种设计模式的优劣势、使用场景、示例以及 JDK 和常见框架中的使用方法。 简述按照惯例，先上维基百科的解释，什么是设计模式？ 在软件工程中，设计模式（design pattern）是对软件设计中普遍存在（反复出现）的各种问题，所提出的解决方案。这个术语是由埃里希·伽玛（Erich Gamma）等人在1990年代从建筑设计领域引入到计算机科学的。 设计模式并不直接用来完成代码的编写，而是描述在各种不同情况下，要怎么解决问题的一种方案。面向对象设计模式通常以类别或对象来描述其中的关系和相互作用，但不涉及用来完成应用程序的特定类别或对象。设计模式能使不稳定依赖于相对稳定、具体依赖于相对抽象，避免会引起麻烦的紧耦合，以增强软件设计面对并适应变化的能力。 说道设计模式，就不得不提到软件设计七大原则，可以说是设计模式的抽象和设计基础。 单一职责原则 (Single Responsibility Principle)：单一职责原则表示一个模块的组成元素之间的功能相关性。从软件变化的角度来看，就一个类而言，应该仅有一个让它变化的原因；通俗地说，即一个类只负责一项职责。 开放-关闭原则 (Open-Closed Principle)：开放-关闭原则表示软件实体 (类、模块、函数等等) 应该是可以被扩展的，但是不可被修改。 里氏替换原则 (Liskov Substitution Principle)：派生类（子类）对象应当可以在程序中代替其基类（超类）对象。或者说子类对象能够替换父类对象，而程序逻辑不变。 依赖倒转原则 (Dependence Inversion Principle)：高层模块不应该依赖低层模块，二者都应该依赖于其抽象。进一步说，抽象不应该依赖于细节，细节应该依赖于抽象。 接口隔离原则 (Interface Segregation Principle)：一个类不应该依赖他不需要的接口，使用多个专门的接口比使用单一的总接口总要好。 迪米特法则（Law Of Demeter）：又叫最少知识原则（Least Knowledge Principle或简写为LKP），一个对象应当对其它对象有尽可能少的了解。通俗来说就是，只与直接的朋友通信。 组合/聚合复用原则 (Composite/Aggregate Reuse Principle)：就是在一个新的对象里面使用一些已有的对象，使之成为新对象的一部分；新对象通过向这些对象的委派达到复用已有功能的目的。 按照类型，设计模式应该可以分为 3 个大类，一共 23 种。 创建型模式：单例模式、抽象工厂模式、原型模式、建造者模式、工厂模式。 结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。 行为型模式：模板方法模式、命令模式、访问者模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、责任链模式。 23 种设计模式一、单例模式定义：采取方法，使整个软件系统中，对某个类只存在一个对象实例。并且该类只提供一个取得其对象实例的方法（静态方法）。 使用场景：频繁创建和销毁的对象、创建对象时消耗资源非常多的对象、工具类对象、频繁访问的数据库或文件对象。 单例模式有八种方法： 恶汉式（静态常量） 1public class Singleton1 &#123;2 private Singleton1() &#123;3 &#125;45 private static final Singleton1 instance = new Singleton1();67 public Singleton1 getInstance() &#123;8 return instance;9 &#125;10&#125; 优缺点：线程安全，在类装载的时候就实例化，有可能造成内存浪费。 恶汉式（静态代码块） 1public class Singleton2 &#123;2 private Singleton2() &#123;3 &#125;45 private static Singleton2 instance;67 static &#123;8 instance = new Singleton2();9 &#125;1011 public Singleton2 getInstance() &#123;12 return instance;13 &#125;14&#125; 优缺点：跟上面一样。 懒汉式（线程不安全） 1public class Singleton3 &#123;2 private static Singleton3 instance;34 private Singleton3() &#123;5 &#125;67 public static Singleton3 getInstance() &#123;8 if (instance == null) &#123;9 instance = new Singleton3();10 &#125;11 return instance;12 &#125;13&#125; 优缺点：懒加载，省内存。但是线程不安全。 懒汉式（同步方法，线程安全） 就是getInstance()方法上锁，解决线程安全问题。但是因为要等锁，效率太低。 懒汉式（同步代码块+双重检查，线程安全） 1public class Singleton4 &#123;2 private static volatile Singleton4 instance;34 private Singleton4() &#123;5 &#125;67 public static Singleton4 getInstance() &#123;8 if (instance == null) &#123;9 synchronized (Singleton4.class) &#123;10 if (instance == null) &#123;11 instance = new Singleton4();12 &#125;13 &#125;1415 &#125;16 return instance;17 &#125;18&#125; 只在初始化的代码块加锁，并且检查两次，相比第四种优化效率。 静态内部类 1public class Singleton5 &#123;23 private Singleton5() &#123;4 &#125;56 public Singleton5 getInstance() &#123;7 return Singleton.instance;8 &#125;910 private static class Singleton &#123;11 private static final Singleton5 instance = new Singleton5();12 &#125;13&#125; 使用不能在外部直接改变内部类的特性，保证了线程安全和单例。并且由于内部类在调用的时候才会初始化，保证了懒加载，没有资源浪费。 枚举 1public enum Singleton6 &#123;2 INSTANCE;3&#125; 单例模式简洁高效，线程安全还能抗反射。但是不是懒加载，而且上面的形式不太容易理解。一般实际使用可能是下面这种，一个接口的单例实现： 1public interface SomeInterface &#123;//一个接口2 void doSomething();3&#125;45public enum Singleton7 implements SomeInterface &#123;//一个枚举类来实现接口6 INSTANCE;78 @Override9 public void doSomething() &#123;10 System.out.println(\"做点啥\");11 &#125;12&#125;1314public class TestClass &#123;//测试的时候这样就可以直接获取单例的实体类15 public static void main(String[] args) &#123;16 Singleton7.INSTANCE.doSomething();17 &#125;18&#125; 二、原型模式定义：用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。 优点： 1、性能提高。 2、能避免构造函数的约束。 缺点： 1、配备克隆方法需要对类的功能进行通盘考虑，这对于全新的类不是很难，但对于已有的类不一定很容易，特别当一个类引用不支持串行化的间接对象，或者引用含有循环结构的时候。 2、必须实现 Cloneable 接口。 原型模式包含以下主要角色： 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 1//具体原型类2class Realizetype implements Cloneable3&#123;4 Realizetype()5 &#123;6 System.out.println(\"具体原型创建成功！\");7 &#125;8 public Object clone() throws CloneNotSupportedException9 &#123;10 System.out.println(\"具体原型复制成功！\");11 return (Realizetype)super.clone();12 &#125;13&#125;14//原型模式的测试类15public class PrototypeTest16&#123;17 public static void main(String[] args)throws CloneNotSupportedException18 &#123;19 Realizetype obj1=new Realizetype();20 Realizetype obj2=(Realizetype)obj1.clone();21 System.out.println(\"obj1==obj2?\"+(obj1==obj2));22 &#125;23&#125; 原型模式通常适用于以下场景。 对象之间相同或相似，即只是个别的几个属性不同的时候。 对象的创建过程比较麻烦，但复制比较简单的时候。 三、简单工厂模式和工厂方法模式简单工厂模式(Simple Factory Pattern)：又称为静态工厂方法(Static Factory Method)模式，它属于类创建型模式。在简单工厂模式中，可以根据参数的不同返回不同类的实例。简单工厂模式专门定义一个类来负责创建其他类的实例，被创建的实例通常都具有共同的父类。 1public class SimpleFactory &#123;2 public static Product create(String type) &#123;3 Product product;4 switch (type) &#123;5 case \"a\":6 System.out.println(\"生产了产品A\");7 product = new ProductA();8 break;9 case \"b\":10 System.out.println(\"生产了产品B\");11 product = new ProductB();12 break;13 default:14 System.out.println(\"没有这种产品\");15 product = null;16 break;17 &#125;18 return product;19 &#125;20&#125;2122interface Product &#123;23&#125;2425class ProductA implements Product &#123;26&#125;2728class ProductB implements Product &#123;29&#125; 使用的时候，只需要传入相应的参数就可以生产对应的产品： 1SimpleFactory.create(\"a\"); 优点： 将创建实例的工作与使用实例的工作分开，使用者不必关心类对象如何创建，实现了解耦； 把初始化实例时的工作放到工厂里进行，使代码更容易维护。 更符合面向对象的原则 &amp; 面向接口编程，而不是面向实现编程。 缺点： 工厂类集中了所有实例（产品）的创建逻辑，一旦这个工厂不能正常工作，整个系统都会受到影响； 违背“开放 - 关闭原则”，一旦添加新产品就不得不修改工厂类的逻辑，这样就会造成工厂逻辑过于复杂。 简单工厂模式由于使用了静态工厂方法，静态方法不能被继承和重写，会造成工厂角色无法形成基于继承的等级结构。 使用场景： 工厂类负责创建的对象比较少：由于创建的对象较少，不会造成工厂方法中的业务逻辑太过复杂。 客户端只知道传入工厂类的参数，对于如何创建对象不关心：客户端既不需要关心创建细节，甚至连类名都不需要记住，只需要知道类型所对应的参数。 工厂方法是针对每一种产品提供一个工厂类，通过不同的工厂实例来创建不同的产品实例。主要解决了上面简单工厂模式违背了“开放 - 关闭原则”的问题。 1public interface Factory &#123;2 Product create();3&#125;45class FactoryA implements Factory &#123;6 @Override7 public Product create() &#123;8 return new ProductA();9 &#125;10&#125;1112class FactoryB implements Factory &#123;13 @Override14 public Product create() &#123;15 return new ProductB();16 &#125;17&#125;1819interface Product &#123;20 void use();21&#125;2223class ProductA implements Product &#123;24 @Override25 public void use() &#123;26 System.out.println(\"使用产品A\");27 &#125;28&#125;2930class ProductB implements Product &#123;31 @Override32 public void use() &#123;33 System.out.println(\"使用产品B\");34 &#125;35&#125; 使用的时候，只要创建对应的工厂就可以生产对应产品并使用了。 1Factory factory = new FactoryA();2factory.create().use(); 优点： 更符合开-闭原则，新增一种产品时，只需要增加相应的具体产品类和相应的工厂子类即可 符合单一职责原则，每个具体工厂类只负责创建对应的产品 缺点 添加新产品时，除了增加新产品类外，还要提供与之对应的具体工厂类，系统类的个数将成对增加，在一定程度上增加了系统的复杂度；同时，有更多的类需要编译和运行，会给系统带来一些额外的开销 由于考虑到系统的可扩展性，需要引入抽象层，在客户端代码中均使用抽象层进行定义，增加了系统的抽象性和理解难度，且在实现时可能需要用到DOM、反射等技术，增加了系统的实现难度 虽然保证了工厂方法内的对修改关闭，但对于使用工厂方法的类，如果要更换另外一种产品，仍然需要修改实例化的具体工厂类 一个具体工厂只能创建一种具体产品 四、抽象工厂模式为了解决上面工厂方法模式里“一个具体工厂只能创建一类产品”的问题，我们又使用了一种新的设计模式：抽象工厂模式。抽象工厂模式允许使用抽象的接口来创建一组相关产品，而不需要知道或关心实际生产出的具体产品是什么，这样就可以从具体产品中被解耦。 1public abstract class AbstractFactory &#123;2 public abstract AbstractProduct createProductA();3 public abstract AbstractProduct createProductB();4&#125;56class FactoryA extends AbstractFactory &#123;7 @Override8 public AbstractProduct createProductA() &#123;9 return new ProductA();10 &#125;1112 @Override13 public AbstractProduct createProductB() &#123;14 return new ProductB();15 &#125;16&#125;1718class FactoryB extends AbstractFactory &#123;19 @Override20 public AbstractProduct createProductA() &#123;21 return new ProductA();22 &#125;2324 @Override25 public AbstractProduct createProductB() &#123;26 return new ProductB();27 &#125;28&#125;2930abstract class AbstractProduct &#123;31 public abstract void use();32&#125;3334abstract class AbstractProductA extends AbstractProduct &#123;35&#125;3637abstract class AbstractProductB extends AbstractProduct &#123;38&#125;3940class ProductA extends AbstractProductA &#123;41 @Override42 public void use() &#123;43 System.out.println(\"使用了产品A\");44 &#125;45&#125;4647class ProductB extends AbstractProductB &#123;48 @Override49 public void use() &#123;50 System.out.println(\"使用了产品B\");51 &#125;52&#125; 使用的时候，两个工厂就可以同时生产两款产品了。 1new FactoryA().createProductA().use();2new FactoryA().createProductB().use();3new FactoryB().createProductA().use();4new FactoryB().createProductB().use(); 优点： 降低耦合。抽象工厂模式将具体产品的创建延迟到具体工厂的子类中，这样将对象的创建封装起来，可以减少客户端与具体产品类之间的依赖，从而使系统耦合度低，这样更有利于后期的维护和扩展； 更符合开-闭原则。新增一种产品类时，只需要增加相应的具体产品类和相应的工厂子类即可 缺点： 抽象工厂模式很难支持新种类产品的变化。这是因为抽象工厂接口中已经确定了可以被创建的产品集合，如果需要添加新产品，此时就必须去修改抽象工厂的接口，这样就涉及到抽象工厂类的以及所有子类的改变，这样也就违背了“开放-封闭”原则。 五、建造者模式定义：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为建造者模式。它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 使用范围： 当创建复杂对象的算法应该独立于该对象的组成部分以及它们的装配方式时。 当构造过程必须允许被构造的对象有不同表示时。 在这样的设计模式中，有以下几个角色： Builder：为创建一个产品对象的各个部件指定抽象接口。 ConcreteBuilder：实现 Builder 的接口以构造和装配该产品的各个部件，定义并明确它所创建的表示，并提供一个检索产品的接口。 Director：构造一个使用 Builder 接口的对象，指导构建过程。 Product：表示被构造的复杂对象。ConcreteBuilder 创建该产品的内部表示并定义它的装配过程，包含定义组成部件的类，包括将这些部件装配成最终产品的接口。 1public interface PersonBuilder &#123;2 void buildHead();3 void buildBody();4 void buildFoot();5 Person buildPerson();6&#125;78public class ManBuilder implements PersonBuilder &#123;9 Person person;10 public ManBuilder() &#123;11 person = new Man();12 &#125;13 public void buildbody() &#123;14 person.setBody(\"建造男人的身体\");15 &#125;16 public void buildFoot() &#123;17 person.setFoot(\"建造男人的脚\");18 &#125;19 public void buildHead() &#123;20 person.setHead(\"建造男人的头\");21 &#125;22 public Person buildPerson() &#123;23 return person;24 &#125;25&#125;2627public class PersonDirector &#123;28 public Person constructPerson(PersonBuilder pb) &#123;29 pb.buildHead();30 pb.buildBody();31 pb.buildFoot();32 return pb.buildPerson();33 &#125;34&#125;3536public class Person &#123;37 private String head;38 private String body;39 private String foot;40 41 public String getHead() &#123;42 return head;43 &#125;44 public void setHead(String head) &#123;45 this.head = head;46 &#125;47 public String getBody() &#123;48 return body;49 &#125;50 public void setBody(String body) &#123;51 this.body = body;52 &#125;53 public String getFoot() &#123;54 return foot;55 &#125;56 public void setFoot(String foot) &#123;57 this.foot = foot;58 &#125;59&#125; 使用： 1PersonDirector pd = new PersonDirector();2Person manPerson = pd.constructPerson(new ManBuilder()); 建造者（Builder）模式在应用过程中可以根据需要改变，如果创建的产品种类只有一种，只需要一个具体建造者，这时可以省略掉抽象建造者，甚至可以省略掉指挥者角色。 六、享元模式定义：运用共享技术来有効地支持大量细粒度对象的复用。它通过共享已经存在的又橡来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 优点：相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 缺点： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长 享元模式中存在以下两种状态： 内部状态，即不会随着环境的改变而改变的可共享部分； 外部状态，指随环境改变而改变的不可以共享的部分。享元模式的实现要领就是区分应用中的这两种状态，并将外部状态外部化。下面来分析其基本结构和实现方法。 享元模式的主要角色有如下。 抽象享元角色（Flyweight）:是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。 具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。 非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。 享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 享元模式看起来跟单例模式有些相似，但是也有区别：单例模式是类级别的，一个类只能有一个对象实例；享元模式是对象级别的，可以有多个对象实例，多个变量引用同一个对象实例；享元模式可以再次创建对象 也可以取缓存对象，单例模式则是严格控制单个进程中只有一个实例对象。 1public abstract class FlyWeight &#123;2 public abstract void use();3&#125;45class ConcreteFlyweight extends FlyWeight &#123;6 private String string;78 ConcreteFlyweight(String str) &#123;9 string = str;10 &#125;1112 @Override13 public void use() &#123;14 System.out.println(\"使用了享元模式\" + string);15 &#125;16&#125;1718class FlyweightFactory &#123;19 private Hashtable&lt;String, FlyWeight&gt; flyweights = new Hashtable&lt;&gt;();2021 public FlyweightFactory() &#123;22 &#125;2324 public FlyWeight getFlyWeight(String name) &#123;25 FlyWeight flyweight = flyweights.get(name);26 if (flyweight == null) &#123;27 flyweight = new ConcreteFlyweight(name);28 flyweights.put(name, flyweight);29 &#125;30 return flyweight;31 &#125;32&#125; 使用的时候： 1new FlyweightFactory().getFlyWeight(\"test\").use(); 七、代理模式定义：由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 优点： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度； 缺点是： 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 代理模式的主要角色如下： 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 1public class Proxy implements Subject &#123;2 private RealSubject realSubject;3 @Override4 public void request() &#123;5 if(realSubject==null)&#123;6 realSubject = new RealSubject();7 &#125;8 realSubject.request();9 &#125;10&#125;1112interface Subject&#123;13 void request();14&#125;15class RealSubject implements Subject&#123;16 @Override17 public void request() &#123;18 System.out.println(\"真实的请求\");19 &#125;20&#125; 应用场景： 远程代理，这种方式通常是为了隐藏目标对象存在于不同地址空间的事实，方便客户端访问。例如，用户申请某些网盘空间时，会在用户的文件系统中建立一个虚拟的硬盘，用户访问虚拟硬盘时实际访问的是网盘空间。 虚拟代理，这种方式通常用于要创建的目标对象开销很大时。例如，下载一幅很大的图像需要很长时间，因某种计算比较复杂而短时间无法完成，这时可以先用小比例的虚拟代理替换真实的对象，消除用户对服务器慢的感觉。 安全代理，这种方式通常用于控制不同种类客户对真实对象的访问权限。 智能指引，主要用于调用目标对象时，代理附加一些额外的处理功能。例如，增加计算真实对象的引用次数的功能，这样当该对象没有被引用时，就可以自动释放它。 延迟加载，指为了提高系统的性能，延迟对目标的加载。例如，Hibernate 中就存在属性的延迟加载和关联表的延时加载。 代理模式中，代理类中包含了对真实主题的引用，这种方式存在两个缺点。 真实主题与代理主题一一对应，增加真实主题也要增加代理。 设计代理以前真实主题必须事先存在，不太灵活。采用动态代理模式可以解决以上问题，如 Spring AOP。 八、适配器模式定义：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 优点： 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 缺点是：对类适配器来说，更换适配器的实现过程比较复杂。 适配器模式（Adapter）包含以下主要角色： 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 类结构模式： 1public class Adapter extends Adaptee implements target &#123;23 @Override4 public void request() &#123;5 specificRequest();6 &#125;7&#125;89class Adaptee &#123;10 public void specificRequest() &#123;11 System.out.println(\"适配者中的业务代码被调用！\");12 &#125;13&#125;1415interface target &#123;16 void request();17&#125; 对象结构模式： 1class ObjectAdapter implements Target2&#123;3 private Adaptee adaptee;4 public ObjectAdapter(Adaptee adaptee)5 &#123;6 this.adaptee=adaptee;7 &#125;8 public void request()9 &#123;10 adaptee.specificRequest();11 &#125;12&#125; 适配器模式（Adapter）通常适用于以下场景： 以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。 使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。 扩展：适配器模式（Adapter）可扩展为双向适配器模式，双向适配器类既可以把适配者接口转换成目标接口，也可以把目标接口转换成适配者接口。 九、桥接模式定义：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 优点是： 由于抽象与实现分离，所以扩展能力强； 其实现细节对客户透明。 缺点：由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，这增加了系统的理解与设计难度。 桥接（Bridge）模式包含以下主要角色。 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。 光看这些概念上的东西确实很难理解，桥接模式也是比较复杂的设计模式了，主要是为了应对多个维度上的变化。举个栗子。假设我们需要一辆车，车按照用途可以分为小汽车、货车和大巴车，按照颜色可以分为黑色和白色和灰色。如果遵循单一职责原则，每一种车每一个颜色都要一个类，如果再有新的颜色，就要分别在三种车里面增加，就太麻烦了。于是可以用这种方式： 1abstract class Vehicle &#123;2 private Color color;3 public Vehicle(Color color) &#123;4 this.color = color;5 &#125;6 void info() &#123;7 this.color.getColor();8 &#125;9&#125;1011class Bus extends Vehicle &#123;12 public Bus(Color color) &#123;13 super(color);14 &#125;15&#125;1617class Car extends Vehicle &#123;18 public Car(Color color) &#123;19 super(color);20 &#125;21&#125;2223class Truck extends Vehicle &#123;24 public Truck(Color color) &#123;25 super(color);26 &#125;27&#125;282930interface Color &#123;31 void getColor();32&#125;3334class Black implements Color &#123;35 @Override36 public void getColor() &#123;37 System.out.println(\"这是黑色的车\");38 &#125;39&#125;4041class White implements Color &#123;42 @Override43 public void getColor() &#123;44 System.out.println(\"这是白色的车\");45 &#125;46&#125;4748class Grey implements Color &#123;49 @Override50 public void getColor() &#123;51 System.out.println(\"这是灰色的车\");52 &#125;53&#125; 只需要传入对应的颜色，就可以选择需要的车： 1Vehicle vehicle = new Bus(new White());2vehicle.info(); 如果这时候需要新的属性怎么办？比如新增一个分类，按照能源类型分类，将车分为电动车和汽油车，只需要新增一个接口，扩展Vehicle的构造器就可以了。 1interface Energy &#123;2 void getEnergy();3&#125;45class Electric implements Energy &#123;6 @Override7 public void getEnergy() &#123;8 System.out.println(\"这时电能的车\");9 &#125;10&#125;1112abstract class Vehicle &#123;13 private Color color;14 private Energy energy;1516 public Vehicle(Color color) &#123;17 this.color = color;18 &#125;1920 public Vehicle(Color color, Energy energy) &#123;21 this.color = color;22 this.energy = energy;23 &#125;2425 void info() &#123;26 this.color.getColor();27 this.energy.getEnergy();28 &#125;29&#125; 桥接模式通常适用于以下场景： 当一个类存在多个独立变化的维度，且这多个维度都需要进行扩展时。 当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。 当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。 十、装饰模式定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。 优点：装饰类和被装饰类可以独立发展，不会相互耦合，装饰模式是继承的一个替代模式，装饰模式可以动态扩展一个实现类的功能。 缺点：多层装饰比较复杂。 这个直接上代码比较好理解。假如我们实现了画圆的功能： 1interface Shape &#123;2 void draw();3&#125;45class Circle implements Shape &#123;6 @Override7 public void draw() &#123;8 System.out.println(\"画圆\");9 &#125;10&#125; 这时候画圆很简单，只需要new Circle().draw();就可以了。但是我们想扩展新的功能，因为开闭原则又不能直接修改draw()这个方法，那应该怎么办？答案是加个装饰器： 1abstract class ShapeDecorator implements Shape &#123;2 Shape decoratedShape;34 public ShapeDecorator(Shape shape) &#123;5 this.decoratedShape = shape;6 &#125;78 @Override9 public void draw() &#123;10 decoratedShape.draw();11 &#125;12&#125;1314class RedBorderDecorator extends ShapeDecorator &#123;15 public RedBorderDecorator(Shape shape) &#123;16 super(shape);17 &#125;1819 @Override20 public void draw() &#123;21 super.draw();22 setRedBorder();23 &#125;2425 void setRedBorder() &#123;26 System.out.println(\"加上了红色边框\");27 &#125;28&#125; 只需要new RedBorderDecorator(new Circle()).draw();就可以画带着红边框的圆形啦。 使用场景： 1、扩展一个类的功能。 2、动态增加功能，动态撤销。 十一、外观模式定义：是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体的细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 外观（Facade）模式是“迪米特法则”的典型应用，它有以下主要优点。 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。 缺点如: 不能很好地限制客户使用子系统类。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则”。 1public class Facade &#123;2 private SubSystem01 obj1 = new SubSystem01();3 private SubSystem02 obj2 = new SubSystem02();4 private SubSystem03 obj3 = new SubSystem03();56 public void method() &#123;7 obj1.method1();8 obj2.method2();9 obj3.method3();10 &#125;11&#125;1213class SubSystem01 &#123;14 public void method1() &#123;15 System.out.println(\"子系统01的method1()被调用！\");16 &#125;17&#125;1819class SubSystem02 &#123;20 public void method2() &#123;21 System.out.println(\"子系统02的method2()被调用！\");22 &#125;23&#125;2425class SubSystem03 &#123;26 public void method3() &#123;27 System.out.println(\"子系统03的method3()被调用！\");28 &#125;29&#125; 执行方法 1new Facade().method(); 可以看出子系统的实现细节被屏蔽，外部程序不需要关系具体实现细节。 十二、组合模式定义：有时又叫作部分-整体模式，它是一种将对象组合成树状的层次结构的模式，用来表示“部分-整体”的关系，使用户对单个对象和组合对象具有一致的访问性。 优点： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 缺点： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 组合模式包含以下主要角色。 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于实现抽象构件角色中声明的公共接口。 树枝构件（Composite）角色：是组合中的分支节点对象，它有子节点。它实现了抽象构件角色中声明的接口，它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法。 1public interface Component2&#123;3 public void add(Component c);4 public void remove(Component c);5 public Component getChild(int i);6 public void operation();7&#125;8//树叶构件9class Leaf implements Component10&#123;11 private String name;12 public Leaf(String name)13 &#123;14 this.name=name;15 &#125;16 public void add(Component c)&#123; &#125;17 public void remove(Component c)&#123; &#125;18 public Component getChild(int i)19 &#123;20 return null;21 &#125;22 public void operation()23 &#123;24 System.out.println(\"树叶\"+name+\"：被访问！\");25 &#125;26&#125;27//树枝构件28class Composite implements Component29&#123;30 private ArrayList&lt;Component&gt; children=new ArrayList&lt;Component&gt;();31 public void add(Component c)32 &#123;33 children.add(c);34 &#125;35 public void remove(Component c)36 &#123;37 children.remove(c);38 &#125;39 public Component getChild(int i)40 &#123;41 return children.get(i);42 &#125;43 public void operation()44 &#123;45 for(Object obj:children)46 &#123;47 ((Component)obj).operation();48 &#125;49 &#125;50&#125; 可以看出，组合模式就是将多个对象用容器组合一个树形结构的对象。 使用场景： 在需要表示一个对象整体与部分的层次结构的场合。 要求对用户隐藏组合对象与单个对象的不同，用户可以用统一的接口使用组合结构中的所有对象的场合 十三、策略模式定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 优点： 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 缺点： 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类。 举个栗子，假如我们想提供一个算法，但是内部实现有可能更改或者增加。 1public interface Strategy &#123;2 public int doOperation(int num1, int num2);3&#125;45class OperationAdd implements Strategy &#123;6 @Override7 public int doOperation(int num1, int num2) &#123;8 return num1 + num2;9 &#125;10&#125;1112class OperationMultiply implements Strategy &#123;13 @Override14 public int doOperation(int num1, int num2) &#123;15 return num1 * num2;16 &#125;17&#125;1819class Context &#123;20 private Strategy strategy;2122 public Context(Strategy strategy) &#123;23 this.strategy = strategy;24 &#125;2526 public int executeStrategy(int num1, int num2) &#123;27 return strategy.doOperation(num1, num2);28 &#125;29&#125; 这样的话，就可以调用的时候选择不同的算法实现： 1Context context = new Context(new OperationAdd());2System.out.println(\"10 + 5 = \" + context.executeStrategy(10, 5));3context = new Context(new OperationMultiply());4System.out.println(\"10 * 5 = \" + context.executeStrategy(10, 5)); 扩展：在一个使用策略模式的系统中，当存在的策略很多时，客户端管理所有策略算法将变得很复杂，如果在环境类中使用策略工厂模式来管理这些策略类将大大减少客户端的工作复杂度。 十四、责任链模式定义：为了避免请求发送者与多个请求处理者耦合在一起，将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 优点： 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 缺点： 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 职责链模式主要包含以下角色： 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。 现在假设我们要设计一个日志系统，日志等级从低到高分为debug、info、error： 1abstract class AbstractLogger &#123;2 public static int DEBUG = 1;3 public static int INFO = 2;4 public static int ERROR = 3;5 protected int level;67 //责任链中的下一个元素8 protected AbstractLogger nextLogger;910 public void setNextLogger(AbstractLogger nextLogger) &#123;11 this.nextLogger = nextLogger;12 &#125;1314 public void logMessage(int level, String message) &#123;15 if (this.level &lt;= level) &#123;16 write(message);17 &#125;18 if (nextLogger != null) &#123;19 nextLogger.logMessage(level, message);20 &#125;21 &#125;2223 abstract protected void write(String message);24&#125;2526class ConsoleLogger extends AbstractLogger &#123;27 public ConsoleLogger() &#123;28 this.level = DEBUG;29 setNextLogger(new FileLogger());30 &#125;3132 @Override33 protected void write(String message) &#123;34 System.out.println(\"Standard Console::Logger: \" + message);35 &#125;36&#125;3738class FileLogger extends AbstractLogger &#123;39 public FileLogger() &#123;40 this.level = INFO;41 setNextLogger(new ErrorLogger());42 &#125;4344 @Override45 protected void write(String message) &#123;46 System.out.println(\"File::Logger: \" + message);47 &#125;48&#125;4950class ErrorLogger extends AbstractLogger &#123;51 public ErrorLogger() &#123;52 this.level = ERROR;53 &#125;5455 @Override56 protected void write(String message) &#123;57 System.out.println(\"Error Console::Logger: \" + message);58 &#125;59&#125; 使用的时候，会顺着责任链传下去，判断使用对应日志等级方法的进行处理： 1AbstractLogger loggerChain = new ConsoleLogger();2loggerChain.logMessage(AbstractLogger.ERROR, \"This is an information.\"); 应用场景： 有多个对象可以处理一个请求，哪个对象处理该请求由运行时刻自动确定。 可动态指定一组对象处理请求，或添加新的处理者。 在不明确指定请求处理者的情况下，向多个处理者中的一个提交请求。 扩展，责任链可以分为两种情况： 纯的责任链模式：一个请求必须被某一个处理者对象所接收，且一个具体处理者对某个请求的处理只能采用以下两种行为之一：自己处理（承担责任）；把责任推给下家处理。 不纯的责任链模式：允许出现某一个具体处理者对象在承担了请求的一部分责任后又将剩余的责任传给下家的情况，且一个请求可以最终不被任何接收端对象所接收。 十五、模板方法模式定义：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。它是一种类行为型模式。 优点： 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 缺点： 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 模板方法模式包含以下主要角色。 抽象类（Abstract Class）：负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下： 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 基本方法：是整个算法中的一个步骤，包含以下几种类型： 抽象方法：在抽象类中申明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 具体子类（Concrete Class）：实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤 1abstract class AbstractClass &#123;2 public void TemplateMethod() //模板方法3 &#123;4 SpecificMethod();5 abstractMethod1();6 abstractMethod2();7 &#125;89 public void SpecificMethod() //具体方法10 &#123;11 System.out.println(\"抽象类中的具体方法被调用...\");12 &#125;1314 public abstract void abstractMethod1(); //抽象方法11516 public abstract void abstractMethod2(); //抽象方法217&#125;1819//具体子类20class ConcreteClass extends AbstractClass &#123;21 public void abstractMethod1() &#123;22 System.out.println(\"抽象方法1的实现被调用...\");23 &#125;2425 public void abstractMethod2() &#123;26 System.out.println(\"抽象方法2的实现被调用...\");27 &#125;28&#125; 模板方法模式通常适用于以下场景： 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。 十六、命令模式定义：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 优点： 降低系统的耦合度。命令模式能将调用操作的对象与实现该操作的对象解耦。 增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，它满足“开闭原则”，对扩展比较灵活。 可以实现宏命令。命令模式可以与组合模式结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以与备忘录模式结合，实现命令的撤销与恢复。 缺点是：可能产生大量具体命令类。因为计对每一个具体操作都需要设计一个具体命令类，这将增加系统的复杂性。 命令模式包含以下主要角色： 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令角色（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。 1//调用者2class Invoker &#123;3 private Command command;45 public Invoker(Command command) &#123;6 this.command = command;7 &#125;89 public void setCommand(Command command) &#123;10 this.command = command;11 &#125;1213 public void call() &#123;14 System.out.println(\"调用者执行命令command...\");15 command.execute();16 &#125;17&#125;1819//抽象命令20interface Command &#123;21 public abstract void execute();22&#125;2324//具体命令25class ConcreteCommand implements Command &#123;26 private Receiver receiver;2728 ConcreteCommand() &#123;29 receiver = new Receiver();30 &#125;3132 public void execute() &#123;33 receiver.action();34 &#125;35&#125;3637//接收者38class Receiver &#123;39 public void action() &#123;40 System.out.println(\"接收者的action()方法被调用...\");41 &#125;42&#125; 使用的时候： 1Command cmd = new ConcreteCommand();2Invoker ir = new Invoker(cmd);3System.out.println(\"客户访问调用者的call()方法...\");4ir.call(); 命令模式通常适用于以下场景： 当系统需要将请求调用者与请求接收者解耦时，命令模式使得调用者和接收者不直接交互。 当系统需要随机请求命令或经常增加或删除命令时，命令模式比较方便实现这些功能。 当系统需要执行一组操作时，命令模式可以定义宏命令来实现该功能。 当系统需要支持命令的撤销（Undo）操作和恢复（Redo）操作时，可以将命令对象存储起来，采用备忘录模式来实现。 十七、访问者模式定义：将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。 优点： 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 缺点： 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 主要角色： 抽象访问者（Visitor）角色：定义一个访问具体元素的接口，为每个具体元素类对应一个访问操作 visit() ，该操作中的参数类型标识了被访问的具体元素。 具体访问者（ConcreteVisitor）角色：实现抽象访问者角色中声明的各个访问操作，确定访问者访问一个元素时该做什么。 抽象元素（Element）角色：声明一个包含接受操作 accept() 的接口，被接受的访问者对象作为 accept() 方法的参数。 具体元素（ConcreteElement）角色：实现抽象元素角色提供的 accept() 操作，其方法体通常都是 visitor.visit(this) ，另外具体元素中可能还包含本身业务逻辑的相关操作。 对象结构（Object Structure）角色：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。 假设这样一种场景，有多种角色要使用多种物品，其中角色和物品都有可能变动，每个角色使用方式不一样，每种物品提供的功能也不一样，要怎么做设计？我们以电脑为例，假设有笔记本和台式机，使用者分为游戏玩家和程序猿： 1public interface Visitor &#123;2 void visit(Laptop element);3 void visit(Desktop element);4&#125;56//具体访问者游戏玩家7class Gamer implements Visitor &#123;8 public void visit(Laptop element) &#123;9 System.out.println(\"游戏玩家使用笔记本--&gt;\" + element.use());10 &#125;1112 public void visit(Desktop element) &#123;13 System.out.println(\"游戏玩家使用台式机--&gt;\" + element.use());14 &#125;15&#125;1617//具体访问者程序猿18class Programmer implements Visitor &#123;19 public void visit(Laptop element) &#123;20 System.out.println(\"程序猿使用笔记本--&gt;\" + element.use());21 &#125;2223 public void visit(Desktop element) &#123;24 System.out.println(\"程序猿使用台式机--&gt;\" + element.use());25 &#125;26&#125;2728//抽象元素类29interface Computer &#123;30 void accept(Visitor visitor);31&#125;3233//具体元素笔记本类34class Laptop implements Computer &#123;35 public void accept(Visitor visitor) &#123;36 visitor.visit(this);37 &#125;3839 public String use() &#123;40 return \"使用笔记本的操作\";41 &#125;42&#125;4344//具体元素台式机类45class Desktop implements Computer &#123;46 public void accept(Visitor visitor) &#123;47 visitor.visit(this);48 &#125;4950 public String use() &#123;51 return \"使用台式机的操作\";52 &#125;53&#125;5455//对象结构角色56class ObjectStructure &#123;57 private List&lt;Computer&gt; list = new ArrayList&lt;&gt;();5859 public void accept(Visitor visitor) &#123;60 Iterator&lt;Computer&gt; i = list.iterator();61 while (i.hasNext()) &#123;62 i.next().accept(visitor);63 &#125;64 &#125;6566 public void add(Computer element) &#123;67 list.add(element);68 &#125;6970 public void remove(Computer element) &#123;71 list.remove(element);72 &#125;73&#125; 可以看到，无论使用者和物品怎么增加，只需要增加Visitor和Computer两个接口的对应实现类就可以了。 使用方式： 1ObjectStructure os = new ObjectStructure();2os.add(new Desktop());3os.add(new Laptop());4Visitor visitor = new Programmer();5os.accept(visitor);6System.out.println(\"------------------------\");7visitor = new Gamer();8os.accept(visitor); 通常在以下情况可以考虑使用访问者（Visitor）模式： 对象结构相对稳定，但其操作算法经常变化的程序。 对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象的结构。 对象结构包含很多类型的对象，希望对这些对象实施一些依赖于其具体类型的操作。 扩展： 访问者（Visitor）模式是使用频率较高的一种设计模式，它常常同以下两种设计模式联用。 (1)与“迭代器模式”联用。因为访问者模式中的“对象结构”是一个包含元素角色的容器，当访问者遍历容器中的所有元素时，常常要用迭代器。 (2)访问者（Visitor）模式同“组合模式”联用。因为访问者（Visitor）模式中的“元素对象”可能是叶子对象或者是容器对象，如果元素对象包含容器对象，就必须用到组合模式)。 十八、迭代器模式定义：提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 优点： 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 缺点：增加了类的个数，这在一定程度上增加了系统的复杂性。、 迭代器模式主要包含以下角色。 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 迭代器的原理几乎不用解释了，各种JDK中的容器类里就常见。一个简单的实现如下： 1public interface Iterator &#123;2 public boolean hasNext();3 public Object next();4&#125;56interface Container &#123;7 public Iterator getIterator();8&#125;910class NameRepository implements Container &#123;11 public String names[] = &#123;\"Robert\", \"John\", \"Julie\", \"Lora\"&#125;;1213 @Override14 public Iterator getIterator() &#123;15 return new NameIterator();16 &#125;1718 private class NameIterator implements Iterator &#123;19 int index;2021 @Override22 public boolean hasNext() &#123;23 if (index &lt; names.length) &#123;24 return true;25 &#125;26 return false;27 &#125;2829 @Override30 public Object next() &#123;31 if (this.hasNext()) &#123;32 return names[index++];33 &#125;34 return null;35 &#125;36 &#125;37&#125; 使用方式： 1NameRepository namesRepository = new NameRepository();2 3for(Iterator iter = namesRepository.getIterator(); iter.hasNext();)&#123;4 String name = (String)iter.next();5 System.out.println(\"Name : \" + name);6&#125; 应用场景： 当需要为聚合对象提供多种遍历方式时。 当需要为遍历不同的聚合结构提供一个统一的接口时。 当访问一个聚合对象的内容而无须暴露其内部细节的表示时。 十九、观察者模式定义：指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。 优点： 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。 目标与观察者之间建立了一套触发机制。 缺点： 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 观察者模式的主要角色： 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 1public interface Observer &#123;2 void response();3&#125;45//具体观察者16class ConcreteObserver1 implements Observer &#123;7 public void response() &#123;8 System.out.println(\"具体观察者1作出反应！\");9 &#125;10&#125;1112//具体观察者113class ConcreteObserver2 implements Observer &#123;14 public void response() &#123;15 System.out.println(\"具体观察者2作出反应！\");16 &#125;17&#125;//抽象目标1819abstract class Subject &#123;20 protected List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;();2122 //增加观察者方法23 public void add(Observer observer) &#123;24 observers.add(observer);25 &#125;2627 //删除观察者方法28 public void remove(Observer observer) &#123;29 observers.remove(observer);30 &#125;3132 public abstract void notifyObserver(); //通知观察者方法33&#125;3435//具体目标36class ConcreteSubject extends Subject &#123;37 public void notifyObserver() &#123;38 System.out.println(\"具体目标发生改变...\");39 System.out.println(\"--------------\");4041 for (Object obs : observers) &#123;42 ((Observer) obs).response();43 &#125;4445 &#125;46&#125; 使用方式： 1Subject subject = new ConcreteSubject();2Observer obs1 = new ConcreteObserver1();3Observer obs2 = new ConcreteObserver2();4subject.add(obs1);5subject.add(obs2);6subject.notifyObserver(); 也就是非常常见的生产-消费模式，适合以下几种情形： 对象间存在一对多关系，一个对象的状态发生改变会影响其他对象。 当一个抽象模型有两个方面，其中一个方面依赖于另一方面时，可将这二者封装在独立的对象中以使它们可以各自独立地改变和复用 二十、中介者模式定义：定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 优点： 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 缺点：当同事类太多时，中介者的职责将很大，它会变得复杂而庞大，以至于系统难以维护。 中介者模式包含以下主要角色。 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（ConcreteMediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 1public abstract class Mediator &#123;2 public abstract void register(Colleague colleague);34 public abstract void relay(Colleague cl); //转发5&#125;67//具体中介者8class ConcreteMediator extends Mediator &#123;9 private List&lt;Colleague&gt; colleagues = new ArrayList&lt;&gt;();1011 public void register(Colleague colleague) &#123;12 if (!colleagues.contains(colleague)) &#123;13 colleagues.add(colleague);14 colleague.setMedium(this);15 &#125;16 &#125;1718 public void relay(Colleague cl) &#123;19 for (Colleague ob : colleagues) &#123;20 if (!ob.equals(cl)) &#123;21 ob.receive();22 &#125;23 &#125;24 &#125;25&#125;2627//抽象同事类28abstract class Colleague &#123;29 protected Mediator mediator;3031 public void setMedium(Mediator mediator) &#123;32 this.mediator = mediator;33 &#125;3435 public abstract void receive();3637 public abstract void send();38&#125;3940//具体同事类41class ConcreteColleague1 extends Colleague &#123;42 public void receive() &#123;43 System.out.println(\"具体同事类1收到请求。\");44 &#125;4546 public void send() &#123;47 System.out.println(\"具体同事类1发出请求。\");48 mediator.relay(this); //请中介者转发49 &#125;50&#125;5152//具体同事类53class ConcreteColleague2 extends Colleague &#123;54 public void receive() &#123;55 System.out.println(\"具体同事类2收到请求。\");56 &#125;5758 public void send() &#123;59 System.out.println(\"具体同事类2发出请求。\");60 mediator.relay(this); //请中介者转发61 &#125;62&#125; 使用的时候： 1Mediator md = new ConcreteMediator();2Colleague c1, c2;3c1 = new ConcreteColleague1();4c2 = new ConcreteColleague2();5md.register(c1);6md.register(c2);7c1.send();8System.out.println(\"-------------\");9c2.send(); 应用场景： 当对象之间存在复杂的网状结构关系而导致依赖关系混乱且难以复用时。 当想创建一个运行于多个类之间的对象，又不想生成新的子类时。 扩展，实际开发中，通常采用以下两种方法来简化中介者模式，使开发变得更简单： 不定义中介者接口，把具体中介者对象实现成为单例。 同事对象不持有中介者，而是在需要的时f矣直接获取中介者对象并调用。 二十一、备忘录模式定义：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 优点： 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 缺点是：资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 主要角色如下： 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 1public class Memento &#123;2 private String state;34 public Memento(String state) &#123;5 this.state = state;6 &#125;78 public void setState(String state) &#123;9 this.state = state;10 &#125;1112 public String getState() &#123;13 return state;14 &#125;15&#125;1617//发起人18class Originator &#123;19 private String state;2021 public void setState(String state) &#123;22 this.state = state;23 &#125;2425 public String getState() &#123;26 return state;27 &#125;2829 public Memento createMemento() &#123;30 return new Memento(state);31 &#125;3233 public void restoreMemento(Memento m) &#123;34 this.setState(m.getState());35 &#125;36&#125;3738//管理者39class Caretaker &#123;40 private Memento memento;4142 public void setMemento(Memento m) &#123;43 memento = m;44 &#125;4546 public Memento getMemento() &#123;47 return memento;48 &#125;49&#125; 使用方式： 1Originator or=new Originator();2Caretaker cr=new Caretaker();3or.setState(\"S0\");4System.out.println(\"初始状态:\"+or.getState());5cr.setMemento(or.createMemento()); //保存状态 6or.setState(\"S1\");7System.out.println(\"新的状态:\"+or.getState());8or.restoreMemento(cr.getMemento()); //恢复状态9System.out.println(\"恢复状态:\"+or.getState()); 应用场景： 需要保存与恢复数据的场景，如玩游戏时的中间结果的存档功能。 需要提供一个可回滚操作的场景，如 Word、记事本、Photoshop，Eclipse 等软件在编辑时按 Ctrl+Z 组合键，还有数据库中事务操作。 二十二、解释器模式定义：给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 优点： 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 缺点： 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 解释器模式包含以下主要角色。 抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。 终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。 非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。 环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。 客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。 我们以创建一个组合表达式为例： 1public interface Expression &#123;2 public boolean interpret(String context);3&#125;45class TerminalExpression implements Expression &#123;6 private String data;78 public TerminalExpression(String data) &#123;9 this.data = data;10 &#125;1112 @Override13 public boolean interpret(String context) &#123;14 if (context.contains(data)) &#123;15 return true;16 &#125;17 return false;18 &#125;19&#125;2021class OrExpression implements Expression &#123;22 private Expression expr1 = null;23 private Expression expr2 = null;2425 public OrExpression(Expression expr1, Expression expr2) &#123;26 this.expr1 = expr1;27 this.expr2 = expr2;28 &#125;2930 @Override31 public boolean interpret(String context) &#123;32 return expr1.interpret(context) || expr2.interpret(context);33 &#125;34&#125;3536class AndExpression implements Expression &#123;37 private Expression expr1 = null;38 private Expression expr2 = null;3940 public AndExpression(Expression expr1, Expression expr2) &#123;41 this.expr1 = expr1;42 this.expr2 = expr2;43 &#125;4445 @Override46 public boolean interpret(String context) &#123;47 return expr1.interpret(context) &amp;&amp; expr2.interpret(context);48 &#125;49&#125; 使用方式： 1//规则：Robert 和 John 是男性2public static Expression getMaleExpression()&#123;3 Expression robert = new TerminalExpression(\"Robert\");4 Expression john = new TerminalExpression(\"John\");5 return new OrExpression(robert, john); 6&#125;7 8//规则：Julie 是一个已婚的女性9public static Expression getMarriedWomanExpression()&#123;10 Expression julie = new TerminalExpression(\"Julie\");11 Expression married = new TerminalExpression(\"Married\");12 return new AndExpression(julie, married); 13&#125;14 15public static void main(String[] args) &#123;16 Expression isMale = getMaleExpression();17 Expression isMarriedWoman = getMarriedWomanExpression();18 19 System.out.println(\"John is male? \" + isMale.interpret(\"John\"));20 System.out.println(\"Julie is a married women? \" 21 + isMarriedWoman.interpret(\"Married Julie\"));22&#125; 应用场景： 当语言的文法较为简单，且执行效率不是关键问题时。 当问题重复出现，且可以用一种简单的语言来进行表达时。 当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候，如 XML 文档解释。 注意：解释器模式在实际的软件开发中使用比较少，因为它会引起效率、性能以及维护等问题。如果碰到对表达式的解释，在 Java 中可以用 Expression4J 或 Jep 等来设计。 二十三、状态模式定义：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 优点： 状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 缺点： 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 状态模式包含以下主要角色： 环境（Context）角色：也称为上下文，它定义了客户感兴趣的接口，维护一个当前状态，并将与状态相关的操作委托给当前状态对象来处理。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为。 1//抽象状态类2public abstract class State &#123;3 public abstract void Handle(Context context);4&#125;56class Context &#123;7 private State state;89 //定义环境类的初始状态10 public Context() &#123;11 this.state = new ConcreteStateA();12 &#125;1314 //设置新状态15 public void setState(State state) &#123;16 this.state = state;17 &#125;1819 //读取状态20 public State getState() &#123;21 return (state);22 &#125;2324 //对请求做处理25 public void Handle() &#123;26 state.Handle(this);27 &#125;28&#125;2930//具体状态A类31class ConcreteStateA extends State &#123;32 public void Handle(Context context) &#123;33 System.out.println(\"当前状态是 A.\");34 context.setState(new ConcreteStateB());35 &#125;36&#125;3738//具体状态B类39class ConcreteStateB extends State &#123;40 public void Handle(Context context) &#123;41 System.out.println(\"当前状态是 B.\");42 context.setState(new ConcreteStateA());43 &#125;44&#125; 使用方式： 1ShareContext context=new ShareContext(); //创建环境 2context.Handle(); //处理请求3context.Handle();4context.Handle();5context.Handle(); 使用场景： 当一个对象的行为取决于它的状态，并且它必须在运行时根据状态改变它的行为时，就可以考虑使用状态模式。 一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态时。 扩展：在有些情况下，可能有多个环境对象需要共享一组状态，这时需要引入享元模式，将这些具体状态对象放在集合中供程序共享 JDK 出现了哪些设计模式这部分有些乱，没有顺序，基本是从网上找的或者自己想到的，想到哪写到哪里。 Runtime 用到了单例模式。 用到了工厂模式的：Integer.valueOf()、Class.forName()、反射包里的Array.newInstance()等等。 虚拟工厂：java.sql.DriverManager#getConnection()、java.sql.Statement#executeQuery() 建造者模式：StringBuilder#append()、System.arraycopy 原型模式：java.lang.Object中返回一个浅拷贝对象，java.lang.Cloneable 桥接模式：AWT、JDBC 组合模式：各种容器类：List、Map、Set 装饰器模式：BufferedInputStream、java.util.Collections#checked[List|Map|Set|SortedSet|SortedMap]() 适配器模式：Arrays.asList()、InputStreamReader 享元模式：Integer.valueOf() 外观模式：java.lang.Class、javax.faces.webapp.FacesServlet 代理模式：java.lang.reflect.Proxy、RMI 责任链模式：java.util.logging.Logger#log()、javax.servlet.Filter#doFilter() 命令模式：Runnable 解释器模式：java.util.Pattern、java.text.Format 迭代器器模式：java.util.Iterator 中介者模式：Timer、java.util.concurrent.Executor#execute() 备忘录模式：java.util.Date、java.io.Serializable 访问者：javax.lang.model.element.Element和javax.lang.model.element.ElementVisitor 模板模式：java.util.Collections#sort()、java.util.AbstractList#indexOf() 策略模式：java.util.Comparator#compare()、HttpServlet 状态模式：javax.faces.lifecycle.LifeCycle#execute()、java.util.Iterator 观察者模式：java.util.EventListener、javax.servlet.http.HttpSessionAttributeListener 常见框架和类库中怎么用的先以 Spring 为例，后续慢慢补充，这里仅作为一个索引，具体详细怎么用的后续再开文章补充吧。 工厂方法：BeanFactory、ApplicationContext 单例模式：singleton作用域的 bean 代理模式：Spring AOP 就是基于动态代理的。如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib 。 模板方法：jdbcTemplate、hibernateTemplate等等 观察者模式：ApplicationEventPublisher和ApplicationListener 适配器模式：AdvisorAdapter、HandlerAdapter 策略模式：SimpleInstantiationStrategy 参考文章： 抽象工厂模式（Abstract Factory）- 最易懂的设计模式解析 java设计模式之建造者模式 设计模式概述 设计模式 JDK里的设计模式 面试官:“谈谈Spring中都用到了那些设计模式?”","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"设计模式","slug":"设计模式","permalink":"http://beritra.github.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"HTTPS为什么安全？","slug":"HTTPS为什么安全？","date":"2020-03-01T15:52:45.000Z","updated":"2022-02-20T11:27:13.892Z","comments":true,"path":"2020/03/01/HTTPS为什么安全？/","link":"","permalink":"http://beritra.github.com/2020/03/01/HTTPS%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%89%E5%85%A8%EF%BC%9F/","excerpt":"知道 HTTPS，也用过 HTTPS，但是一直不知道原理，这次探究一下。","text":"知道 HTTPS，也用过 HTTPS，但是一直不知道原理，这次探究一下。 惯例，上 Wiki，什么是 HTTPS? 超文本传输安全协议（英语：HyperText Transfer Protocol Secure，缩写：HTTPS；常称为HTTP over TLS、HTTP over SSL或HTTP Secure）是一种通过计算机网络进行安全通信的传输协议。HTTPS经由HTTP进行通信，但利用SSL/TLS来加密数据包。HTTPS开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。这个协议由网景公司（Netscape）在1994年首次提出，随后扩展到互联网上。 可以看出，Https 之所以安全，是经过了加密，那么新的问题又来了，SSL 和 TLS 分别是什么？是怎么加密的？ SSL 和 TLSSSL：（Secure Socket Layer，安全套接字层），位于可靠的面向连接的网络层协议和应用层协议之间的一种协议层。SSL 通过互相认证、使用数字签名确保完整性、使用加密确保私密性，以实现客户端和服务器之间的安全通讯。该协议由两层组成：SSL 记录协议和 SSL 握手协议。 TLS：(Transport Layer Security，传输层安全协议)，用于两个应用程序之间提供保密性和数据完整性。该协议由两层组成：TLS 记录协议和 TLS 握手协议。 SSL 和 TLS 是一种能够在服务器，machines 和通过网络运行的应用程序（列如，客户端连接到 web 服务器）之间提供身份认证和数据加密的加密协议。SSL 是 TLS 的前世。多年来，新版本的发布用来解决漏洞，提供更强大支持，更安全的密码套件和算法。 SSL 最初是由 Netscape 开发的，早在 1995 年以 SSL 2.0 的方式发布（1.0从未对公众发布）。在一些漏洞被发现之后，版本 2.0 在 1996 年很快被 3.0 所取代。注意：版本 2.0 和 3.0 有时会写成 SSLv2 和 SSLv3。 TLS以SSL 3.0为基础于1999年作为SSL的新版本推出。 应该选 SSL 还是 TLS？SSL 2.0 和 SSL 3.0 已经被 IEFT 组织废弃（分别在2011年，2015年）多年来，在被废弃的 SSL 协议中一直存在漏洞并被发现 (e.g. POODLE, DROWN)。大多数现代浏览器遇到使用废弃协议的 web 服务时，会降低用户体验（红线穿过挂锁标志或者https表示警告）来表现。因为这些原因，你应该在服务端禁止使用 SSL 协议，仅仅保留 TLS 协议开启。 为什么要用 TLS/SSL？要想规避风险，要先知道风险点在哪。我们要知道 HTTPS 想要解决哪些问题，具体通过什么手段解决的。 常规的 HTTP 通信，有以下的问题。 窃听风险（eavesdropping）：第三方可以获知通信内容。 篡改风险（tampering）：第三方可以修改通信内容。 冒充风险（pretending）：第三方可以冒充他人身份参与通信。 因此，SSL/TLS 协议就是为了解决这三大风险而设计的，希望达到： 所有信息都是加密传播，第三方无法窃听。 具有校验机制，一旦被篡改，通信双方会立刻发现。 配备身份证书，防止身份被冒充。 运行过程SSL/TLS 协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。 公开密钥密码学（英语：Public-key cryptography）也称非对称式密码学（英语：Asymmetric cryptography）是密码学的一种算法，它需要两个密钥，一个是公开密钥，另一个是私有密钥；公钥用作加密，私钥则用作解密。使用其中一个密钥把明文加密后所得的密文，只能用相对应的另一个密钥才能解密得到原本的明文；连最初用来加密的密钥也不能用作解密。由于加密和解密需要两个不同的密钥，故被称为非对称加密；不同于加密和解密都使用同一个密钥的对称加密。其中一个密钥可以公开，称为公钥，可任意向外发布；不公开的密钥为私钥，必须由用户自行严格秘密保管，绝不透过任何途径向任何人提供，也不会透露给被信任的要通信的另一方。 简单来说，非对称加密就是两个密码，公钥交给对方，私钥自己藏好，把想要传输的信息用私钥加密，然后交给对方，对方用公钥解密。这里仍然有两个问题： 如何保证公钥不被篡改？解决方法：将公钥放在数字证书中。只要证书是可信的，公钥就是可信的。 那么怎么验证数字证书可不可信呐？答案是 CA 来给你鉴定。 CA（Certificate Authority）是证书颁发机构，即颁发数字证书的机构，是负责发放和管理数字证书的权威机构。业界比较大的 CA 机构会被客户端软件广泛承认。比如一个浏览器承认某个 CA 的信用，就把这家机构的根证书（root certificate）内置在客户端当中，作为信任链的源头，后续的其他数字证书只要通过根证书的认证，就会认定为是可信的。根证书如何来判断是否信任某个数字证书？就得从数字证书的结构说起。 数字证书的认证有过部署 HTTPS 服务经历的同学都知道，将自己的网站改为 HTTPS 需要做的第一步就是申请一个数字证书放在 web 服务器下面。数字证书主要包括以下内容： 证书颁发机构的名称 证书持有者公钥 证书签名用到的Hash算法 有效期等等其他信息 这些内容都是明文，然后将明文通过摘要算法获取 hash，通过证书颁发机构的私钥进行非对称加密，得到签名信息，这个步骤叫做数字签名。明文 + hash + 数字签名共同组成了这个数字证书。 现在客户端拿到了这个数字证书，使用证书里说明的 hash 算法对明文进行摘要运算，得到的 hash 值应该和证书里是一样的。然后客户端会查询这个证书的颁发机构名称，从自己内置的信任列表里找该机构的证书，这里有两种情况： 找到了，说明是自己信任的 CA 颁发的，从该机构的数字证书中可以找到证书的公钥，用这个公钥解密之前拿到的数字证书，如果能成功解密，证明证书可信。会添加到信任列表。 没找到，也不一定说明证书是假的，因为数字证书是一条信任链，A 信任 B，B 信任 C，就算内置了 A ，也不能直接用 A 验证 C。所幸 C 的证书内容里会包含 B 的名称和地址，浏览器会自动去找 B 的证书，然后再看看 B 是否可信，直到达到了根节点。或者到了根节点仍然不可信，整个信任链就没有建立，所有证书被浏览器判断为不可信。 如此以来，数字证书的认证就完成了，有两个结果，浏览器信任证书和不信任证书。前者一般会有标示，比如 Chrome 浏览器会在地址栏有个小锁。后者就会提示连接有风险。 这一步完成之后，客户端只是拿到了一个准确、可信的服务端的公钥。而实际上的数据传输还没开始。 客户端拿到公钥之后（或之前，我也不确定），也会把自己的公钥发送过去，双方会运行 Diffie Hellman 算法，简称 DH 算法。DH 算法的作用是这样：通过二者的公钥推导出一个个一样的密码出来。通俗地说：双方会协商一个master-key，这个master-key 不会在网络上传输、交换，它们独立计算出来的，其值是相同的，只有它们自己双方知道。 然后以master-key 推导出 session-key，用于双方 SSL 数据流的加密/解密，采用对称加密，保证数据不被偷窥，加密算法一般用 AES。 同时以 master-key 推导出 hash-key，用于数据完整性检查（Integrity Check Verification）的加密密钥，HASH 算法一般有：MD5、SHA。 为什么要搞这么麻烦呐？因为非对称加密算法很消耗资源，所以只有前置的互相认证的步骤用非对称加密，后续的步骤都用更省资源的对称加密。 到这里，HTTPS 的大致流程就完成了，当然实际上的步骤会更加复杂，客户端和服务端的连接还会有随机数，为了减少资源消耗还会有一些缓存机制。 再回顾一下整个简化的流程： 客户端请求服务端的数字证书。 客户端根据内置的根证书校验服务端发来的证书。 如果证书可信，客户端把自己的公钥发给服务端，然后用双方的公钥推导出一个会话密钥。 后续的请求内容都用会话密钥进行加密和解密。 扩展截了个知乎的证书，可以看到是三级的信任链，知乎的证书签发机构是GeoTrust RSA CA 2018，而 GeoTrust 的签发机构是DigiCert Global Root CA，就到根证书了。 另外注意风险： 把公钥交给对方的时候要注意安全，公钥有可能被劫持。 数字证书的认证步骤也不是绝对安全，有“中间人攻击”。当然这个也需要一定条件，就不细说了。 参考文章： TLS与SSL之间关系 SSL/TLS协议运行机制的概述 详解https是如何确保安全的？ 密钥交换算法","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://beritra.github.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"https","slug":"https","permalink":"http://beritra.github.com/tags/https/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://beritra.github.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"关于Redis应该知道的","slug":"关于Redis应该知道的","date":"2020-02-25T17:23:14.000Z","updated":"2020-05-27T16:12:44.604Z","comments":true,"path":"2020/02/26/关于Redis应该知道的/","link":"","permalink":"http://beritra.github.com/2020/02/26/%E5%85%B3%E4%BA%8ERedis%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84/","excerpt":"NoSQL 几乎是必备的知识，其中 Redis 大概是适用范围最广的非关系型数据库，应该也是我个人接触比较早，了解比较多的，因此整理下 Redis 的相关知识点。先从比较基础的内容整理开始，后续更多复杂和深入的内容考虑慢慢补充或者再开专门的文章来整理。","text":"NoSQL 几乎是必备的知识，其中 Redis 大概是适用范围最广的非关系型数据库，应该也是我个人接触比较早，了解比较多的，因此整理下 Redis 的相关知识点。先从比较基础的内容整理开始，后续更多复杂和深入的内容考虑慢慢补充或者再开专门的文章来整理。 简述直接抄来的一些关于 Redis 的简介： Redis 是一个高性能的 key-value 数据库。 Redis 有以下优势： 性能极高 – 我在本机上用自带的 redis-benchmark 测试，读写能超过 15 万次每秒 。 丰富的数据类型 – Redis 支持二进制的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis 的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC 指令包起来。 丰富的特性 – Redis 还支持 publish/subscribe, 通知, key 过期等等特性。 应用Redis 能用来做什么？互联网公司一般怎么使用它？ 除了常见的缓存（cache），Redis 能做的还有很多，比如： 分布式锁：基于 Redis 的分布式锁实现 排序：sort 命令 去重：小数据量可以直接用 Redis 的 Set 来去重，数据量非常大而且不要是百分百的精准可以考虑用布隆过滤器。 计数器、限速器：基于lua 脚本和incr() 使用 bitmap 实现用户上线次数统计 数据结构Redis 有 5 种常用的基础数据结构，分别为:string (字符串)、list (列表)、set (集合)、hash (哈 希) 和 zset (有序集合)。熟练掌握这 5 种基本数据结构的使用是 Redis 知识最基础也最重要的部分，它也是在 Redis 面试题中问到最多的内容。除此之外，Redis 官网上还提到了三种数据类型：Bit arrays 、HyperLogLogs 和 Streams。 key官网上这么介绍 Redis 的 key： Redis 密钥是二进制安全的，这意味着您可以使用任何二进制序列作为密钥，从“ foo”之类的字符串到 JPEG 文件的内容。空字符串也是有效的键。有关密钥的其他一些规则： 太长的密钥不是一个好主意。例如，一个 1024 字节的密钥是一个坏主意，不仅是内存方面的问题，而且因为在数据集中查找密钥可能需要进行一些代价高昂的密钥比较。即使当手头的任务是匹配一个大值的存在时，对它进行散列（例如使用SHA1）也是一个更好的主意，尤其是从内存和带宽的角度来看。 非常短的键通常不是一个好主意。如果您可以改写“ user:1000:followers”，那么将“ u1000flw”作为密钥写的毫无意义。与键对象本身和值对象使用的空间相比，后者更具可读性，并且添加的空间较小。虽然短键显然会消耗更少的内存，但您的工作是找到合适的平衡。 尝试坚持使用架构。例如，“ object-type:id”是一个好主意，例如“ user:1000”。点或破折号通常用于多字字段，例如“ comment:123:reply.to”或“ comment:123:reply-to”中。 允许的最大密钥大小为512 MB。 key 是所有数据类型通用的键，关于 key 的操作有如下几个： DEL key 删除给定的一个或多个 key 。不存在的 key 会被忽略。 DUMP key 序列化给定 key ，并返回被序列化的值，使用 RESTORE 命令可以将这个值反序列化为 Redis 键。 EXISTS key 检查键是否存在。若 key 存在，返回1，否则返回0。 EXPIRE key seconds 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。注意： 可以对一个已经带有生存时间的 key 执行 EXPIRE 命令，新指定的生存时间会取代旧的生存时间。 使用 PERSIST 命令可以在不删除 key 的情况下，移除 key 的生存时间，让 key 重新成为一个『持久的』(persistent) key 。 其他的操作不会影响key的生存时间。 在 Redis 2.4 版本中，过期时间的延迟在 1 秒钟之内 —— 也即是，就算 key 已经过期，但它还是可能在过期之后一秒钟之内被访问到，而在新的 Redis 2.6 版本中，延迟被降低到 1 毫秒之内。 EXPIREAT key timestamp EXPIREAT 的作用和 EXPIRE 类似，都用于为 key 设置生存时间。不同在于 EXPIREAT 命令接受的时间参数是 UNIX 时间戳(unix timestamp)。 PEXPIRE key milliseconds 这个命令和 EXPIRE 命令的作用类似，但是它以毫秒为单位设置 key 的生存时间，而不像 EXPIRE 命令那样，以秒为单位。 PEXPIREAT key milliseconds-timestamp 这个命令和 EXPIREAT 命令类似，但它以毫秒为单位设置 key 的过期 unix 时间戳，而不是像 EXPIREAT 那样，以秒为单位。 TTL key 以秒为单位，返回给定 key 的剩余生存时间(TTL, time to live)。 PTTL key 这个命令类似于 TTL 命令，但它以毫秒为单位返回 key 的剩余生存时间，而不是像 TTL 命令那样，以秒为单位。 KEYS pattern 查找所有符合给定模式 pattern 的 key 。 MIGRATE host port key destination-db timeout [COPY] [REPLACE] 将 key 原子性地从当前实例传送到目标实例的指定数据库上，一旦传送成功， key 保证会出现在目标实例上，而当前实例上的 key 会被删除。这个命令是一个原子操作，它在执行的时候会阻塞进行迁移的两个实例，直到以下任意结果发生：迁移成功，迁移失败，等到超时。 MOVE key db 将当前数据库的 key 移动到给定的数据库 db 当中。如果当前数据库(源数据库)和给定数据库(目标数据库)有相同名字的给定 key ，或者 key 不存在于当前数据库，那么 MOVE 没有任何效果。 OBJECT subcommand [arguments [arguments] OBJECT 命令允许从内部察看给定 key 的 Redis 对象。 PERSIST key 移除给定 key 的生存时间，将这个 key 从『易失的』(带生存时间 key )转换成『持久的』(一个不带生存时间、永不过期的 key )。 RANDOMKEY 从当前数据库中随机返回(不删除)一个 key 。 RENAME key newkey 将 key 改名为 newkey 。当 key 和 newkey 相同，或者 key 不存在时，返回一个错误。当 newkey 已经存在时， RENAME 命令将覆盖旧值。 RENAMENX key newkey 当且仅当 newkey 不存在时，将 key 改名为 newkey 。当 key 不存在时，返回一个错误。 RESTORE key ttl serialized-value 反序列化给定的序列化值，并将它和给定的 key 关联。参数 ttl 以毫秒为单位为 key 设置生存时间；如果 ttl 为 0 ，那么不设置生存时间。 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination] 返回或保存给定列表、集合、有序集合 key 中经过排序的元素。排序默认以数字作为对象，值被解释为双精度浮点数，然后进行比较。因为 SORT 命令默认排序对象为数字， 当需要对字符串进行排序时， 需要显式地在 SORT 命令之后添加 ALPHA 修饰符。 TYPE key 返回 key 所储存的值的类型。 string (字符串)字符串 string 是 Redis 最简单的数据结构。Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。 字符串结构使用非常广泛，一个常见的用途就是缓存用户信息。我们将用户信息结构体 使用 JSON 序列化成字符串，然后将序列化后的字符串塞进 Redis 来缓存。 Redis 的字符串是动态字符串，是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，内部为当前字符串实际分配的空间 capacity 一般要高于实际字符串长度 len。当字符串长度小于 1M 时， 扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。需要注意的是，字符串最大长度为 512M。 Redis 中字符串对象的编码可以是 int，raw 或者 embstr 中的某一种，分别如下： int 编码：保存long 型的64位有符号整数，系统会缓存包含了 1~1000 数字的对象，相同对象会指向同一个地址。 embstr 编码：保存长度小于44字节的字符串，和字符串对象的其他元信息保存在一段连续的内存地址中。 raw 编码：保存长度大于44字节的字符串，和字符串对象的其他元信息分开保存。 Hash（哈希）Hash 是一个键值(key =&gt; value)对集合。Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 常用命令：hget,hset,hgetall 等。 Hash 的底层数据结构有两种表示：ziplist 和 hashtable。当哈希对象可以同时满足以下两个条件时， 哈希对象使用 ziplist 编码： 哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节； 哈希对象保存的键值对数量小于 512 个； hash 对象使用ziplist 保存时，程序会将保存了键的ziplist节点推入到列表的表尾，然后再将保存了值的ziplist节点推入列表的表尾。使用这种方式保存时，并不需要申请多余的内存空间，而且每个Key都要存储一些关联的系统信息（如过期时间、LRU等），因此和String类型的Key/Value相比，Hash类型极大的减少了Key的数量(大部分的Key都以Hash字段的形式表示并存储了)，从而进一步优化了存储空间的使用效率 hashtable 编码的哈希对象使用字典作为底层实现时， 哈希对象中的每个键值对都使用一个字典键值对来保存： 字典的每个键都是一个字符串对象， 对象中保存了键值对的键； 字典的每个值都是一个字符串对象， 对象中保存了键值对的值。 List（列表）List 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 实现方式：Redis中的列表对象在版本3.2之前，列表底层的编码是 ziplist 和 linkedlist 实现的，但是在版本 3.2 之后，重新引入了一个 quicklist 的数据结构，列表的底层都由 quicklist 实现。在早期的设计中， 当列表对象中元素的长度比较小或者数量比较少的时候，采用 ziplist 来存储，当列表对象中元素的长度比较大或者数量比较多的时候，则会转而使用双向列表 linkedlist 来存储。 这两种存储方式的优缺点 双向链表 linkedlist 便于在表的两端进行 push 和 pop 操作，在插入节点上复杂度很低，但是它的内存开销比较大。首先，它在每个节点上除了要保存数据之外，还要额外保存两个指针；其次，双向链表的各个节点是单独的内存块，地址不连续，节点多了容易产生内存碎片。 ziplist 存储在一段连续的内存上，所以存储效率很高。但是，它不利于修改操作，插入和删除操作需要频繁的申请和释放内存。特别是当 ziplist 长度很长的时候，一次 realloc 可能会导致大批量的数据拷贝。 Set（集合）Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 Sorted Set（有序集合）Redis 有序集合和集合一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个 double 类型的分数。Redis 正是通过分数来为集合中的成员进行从小到大的排序。有序集合的成员是唯一的,但分数(score)却可以重复。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 2^32 - 1 (4294967295, 每个集合可存储40多亿个成员)。 Bitmaps（位图） and HyperLogLogs（？这个怎么翻译）Redis 还支持位图和 HyperLogLogs，它们实际上是基于 String 基本类型的数据类型，但是具有自己的语义。 可以参考官网中introduction to Redis data types以获取有关这些类型的信息。 其他功能Transaction（事务）MULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务的基础。 事务可以一次执行多个命令， 并且带有以下两个重要的保证： 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 EXEC 命令负责触发并执行事务中的所有命令： 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。 当使用 AOF 方式做持久化的时候， Redis 会使用单个 write(2) 命令将事务写入到磁盘中。 然而，如果 Redis 服务器因为某些原因被管理员杀死，或者遇上某种硬件故障，那么可能只有部分事务命令会被成功写入到磁盘中。 如果 Redis 在重新启动时发现 AOF 文件出了这样的问题，那么它会退出，并汇报一个错误。 使用 redis-check-aof 程序可以修复这一问题：它会移除 AOF 文件中不完整事务的信息，确保服务器可以顺利启动。 从 2.2 版本开始，Redis 还可以通过乐观锁（optimistic lock）实现 CAS （check-and-set）操作。 Replication（复制）Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(master server)的精确复制品。 以下是关于 Redis 复制功能的几个重要方面： Redis 使用异步复制。 从 Redis 2.8 开始， 从服务器会以每秒一次的频率向主服务器报告复制流（replication stream）的处理进度。 一个主服务器可以有多个从服务器。 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个图状结构。 复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次同步， 主服务器也可以继续处理命令请求。 复制功能也不会阻塞从服务器： 只要在 redis.conf 文件中进行了相应的设置， 即使从服务器正在进行初次同步， 服务器也可以使用旧版本的数据集来处理命令查询。 不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。 你还可以配置从服务器， 让它在与主服务器之间的连接断开时， 向客户端发送一个错误。 复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 [SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC | DESC] [ALPHA] [STORE destination]](http://redisdoc.com/database/sort.html#sort) 命令可以交给附属节点去运行。 可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。 pub/sub（发布与订阅）SUBSCRIBE、UNSUBSCRIBE 和 PUBLISH三个命令实现了发布与订阅信息泛型（Publish/Subscribe messaging paradigm）， 在这个实现中， 发送者（发送信息的客户端）不是将信息直接发送给特定的接收者（接收信息的客户端）， 而是将信息发送给频道（channel）， 然后由频道将信息转发给所有对这个频道感兴趣的订阅者。 发送者无须知道任何关于订阅者的信息， 而订阅者也无须知道是那个客户端给它发送信息， 它只要关注自己感兴趣的频道即可。 对发布者和订阅者进行解构（decoupling）， 可以极大地提高系统的扩展性（scalability）， 并得到一个更动态的网络拓扑（network topology）。 比如说， 要订阅频道 foo 和 bar ， 客户端可以使用频道名字作为参数来调用 SUBSCRIBE命令： 1redis&gt; SUBSCRIBE foo bar 当有客户端发送信息到这些频道时， Redis 会将传入的信息推送到所有订阅这些频道的客户端里面。 正在订阅频道的客户端不应该发送除 SUBSCRIBE、UNSUBSCRIBE之外的其他命令。 其中， SUBSCRIBE 可以用于订阅更多频道， 而 UNSUBSCRIBE 则可以用于退订已订阅的一个或多个频道。 SUBSCRIBE的执行结果会以信息的形式返回， 客户端可以通过分析所接收信息的第一个元素， 从而判断所收到的内容是一条真正的信息， 还是 SUBSCRIBE或UNSUBSCRIBE 命令的操作结果。 信息的第一个元素标识了信息的类型： subscribe ： 表示当前客户端成功地订阅了信息第二个元素所指示的频道。 而信息的第三个元素则记录了目前客户端已订阅频道的总数。 unsubscribe ： 表示当前客户端成功地退订了信息第二个元素所指示的频道。 信息的第三个元素记录了客户端目前仍在订阅的频道数量。 当客户端订阅的频道数量降为 0 时， 客户端不再订阅任何频道， 它可以像往常一样， 执行任何 Redis 命令。 message ： 表示这条信息是由某个客户端执行 PUBLISH channel message 命令所发送的， 真正的信息。 信息的第二个元素是信息来源的频道， 而第三个元素则是信息的内容。 编程示例Pieter Noordhuis 提供了一个使用 EventMachine 和 Redis 编写的 高性能多用户网页聊天软件 ， 这个软件很好地展示了发布与订阅功能的用法。 Sentinel（哨兵）Redis 的 Sentinel 系统用于管理多个 Redis 服务器（instance）， 该系统执行以下三个任务： 监控（Monitoring）： Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）： 当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）： 当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。 Redis Sentinel 是一个分布式系统， 你可以在一个架构中运行多个 Sentinel 进程（progress）， 这些进程使用流言协议（gossip protocols)来接收关于主服务器是否下线的信息， 并使用投票协议（agreement protocols）来决定是否执行自动故障迁移， 以及选择哪个从服务器作为新的主服务器。 虽然 Redis Sentinel 释出为一个单独的可执行文件 redis-sentinel ， 但实际上它只是一个运行在特殊模式下的 Redis 服务器， 你可以在启动一个普通 Redis 服务器时通过给定 --sentinel 选项来启动 Redis Sentinel 。 持久化Redis 提供了多种不同级别的持久化方式： RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。 了解 RDB 持久化和 AOF 持久化之间的异同是非常重要的， 以下几个小节将详细地介绍这这两种持久化功能， 并对它们的相同和不同之处进行说明。 RDB 的优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次 RDB 文件，并且在每个月的每一天，也备份一个 RDB 文件。 这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心，或者亚马逊 S3 中。 RDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB 的缺点 如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOF 的优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF 的缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH source destination timeout 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 RDB 和 AOF ，应该用哪一个？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 集群集群部分的内容官方文档介绍的比较详细了，这里直接大段抄自官方文档的中文翻译，Redis集群规范和Redis集群教程章节。 什么是 Redis 集群？Redis 集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis 实现， 集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset）。 Redis 集群中不存在中心（central）节点或者代理（proxy）节点， 集群的其中一个主要设计目标是达到线性可扩展性（linear scalability）。 Redis 集群为了保证一致性（consistency）而牺牲了一部分容错性： 系统会在保证对网络断线（net split）和节点失效（node failure）具有有限（limited）抵抗力的前提下， 尽可能地保持数据的一致性。 Redis 集群是一个可以在多个 Redis 节点之间进行数据共享的设施（installation）。 Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 Redis 集群提供了以下两个好处： 将数据自动切分（split）到多个节点的能力。 当集群中的一部分节点失效或者无法进行通讯时， 仍然可以继续处理命令请求的能力。 Redis 集群数据共享Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分哈希槽。 举个例子， 一个集群可以有三个哈希槽， 其中： 节点 A 负责处理 0 号至 5500 号哈希槽。 节点 B 负责处理 5501 号至 11000 号哈希槽。 节点 C 负责处理 11001 号至 16384 号哈希槽。 这种将哈希槽分布到不同节点的做法使得用户可以很容易地向集群中添加或者删除节点。 比如说： 如果用户将新节点 D 添加到集群中， 那么集群只需要将节点 A 、B 、 C 中的某些槽移动到节点 D 就可以了。 与此类似， 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。 因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 Redis 集群中的主从复制为了使得集群在一部分节点下线或者无法与集群的大多数（majority）节点进行通讯的情况下， 仍然可以正常运作， Redis 集群对节点使用了主从复制功能： 集群中的每个节点都有 1 个至 N 个复制品（replica）， 其中一个复制品为主节点（master）， 而其余的 N-1 个复制品为从节点（slave）。 在之前列举的节点 A 、B 、C 的例子中， 如果节点 B 下线了， 那么集群将无法正常运行， 因为集群找不到节点来处理 5501 号至 11000 号的哈希槽。 另一方面， 假如在创建集群的时候（或者至少在节点 B 下线之前）， 我们为主节点 B 添加了从节点 B1 ， 那么当主节点 B 下线的时候， 集群就会将 B1 设置为新的主节点， 并让它代替下线的主节点 B ， 继续处理 5501 号至 11000 号的哈希槽， 这样集群就不会因为主节点 B 的下线而无法正常运作了。 不过如果节点 B 和 B1 都下线的话， Redis 集群还是会停止运作。 Redis 集群的一致性保证（guarantee）Redis 集群不保证数据的强一致性（strong consistency）： 在特定条件下， Redis 集群可能会丢失已经被执行过的写命令。 使用异步复制（asynchronous replication）是 Redis 集群可能会丢失写命令的其中一个原因。 考虑以下这个写命令的例子： 客户端向主节点 B 发送一条写命令。 主节点 B 执行写命令，并向客户端返回命令回复。 主节点 B 将刚刚执行的写命令复制给它的从节点 B1 、 B2 和 B3 。 如你所见， 主节点对命令的复制工作发生在返回命令回复之后， 因为如果每次处理命令请求都需要等待复制操作完成的话， 那么主节点处理命令请求的速度将极大地降低 —— 我们必须在性能和一致性之间做出权衡。 Note 如果真的有必要的话， Redis 集群可能会在将来提供同步地（synchronou）执行写命令的方法。 Redis 集群另外一种可能会丢失命令的情况是， 集群出现网络分裂（network partition）， 并且一个客户端与至少包括一个主节点在内的少数（minority）实例被孤立。 举个例子， 假设集群包含 A 、 B 、 C 、 A1 、 B1 、 C1 六个节点， 其中 A 、B 、C 为主节点， 而 A1 、B1 、C1 分别为三个主节点的从节点， 另外还有一个客户端 Z1 。 假设集群中发生网络分裂， 那么集群可能会分裂为两方， 大多数（majority）的一方包含节点 A 、C 、A1 、B1 和 C1 ， 而少数（minority）的一方则包含节点 B 和客户端 Z1 。 在网络分裂期间， 主节点 B 仍然会接受 Z1 发送的写命令： 如果网络分裂出现的时间很短， 那么集群会继续正常运行； 但是， 如果网络分裂出现的时间足够长， 使得大多数一方将从节点 B1 设置为新的主节点， 并使用 B1 来代替原来的主节点 B ， 那么 Z1 发送给主节点 B 的写命令将丢失。 注意， 在网络分裂出现期间， 客户端 Z1 可以向主节点 B 发送写命令的最大时间是有限制的， 这一时间限制称为节点超时时间（node timeout）， 是 Redis 集群的一个重要的配置选项： 对于大多数一方来说， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么集群会将这个主节点视为下线， 并使用从节点来代替这个主节点继续工作。 对于少数一方， 如果一个主节点未能在节点超时时间所设定的时限内重新联系上集群， 那么它将停止处理写命令， 并向客户端报告错误。 常见问题1、为什么 Redis 这么快？ 数据存于内存 用了多路复用I/O 核心存取过程使用单线程 2、key 过期清除（超时剔除）策略 惰性过期（类比懒加载，这是懒过期）：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果 3、 Redis 的内存淘汰策略Redis 的内存淘汰策略是指在 Redis 的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 4、如何防止缓存穿透？ 数据命中不高，变化实时性比较高的情况下，可以缓存空对象。 数据命中不高，相对固定实时性低的时候，用 BloomFilter 布隆过滤器。 5、为什么 Redis 不支持回滚（roll back）如果你有使用关系式数据库的经验， 那么 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”这种做法可能会让你觉得有点奇怪。 以下是这种做法的优点： Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR key 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR key ， 回滚是没有办法处理这些情况的。 鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。 RedLockRedis 官方站这篇文章 Distributed locks with Redis提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性： 安全特性：互斥访问，即永远只有一个客户端能拿到锁。 避免死锁：最终客户端都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的客户端挂掉了或者出现了网络分区。 容错性：只要大部分 Redis 节点存活就可以正常提供服务。 故障切换(failover)实现方式的局限性通过 Redis 为某个资源加锁的最简单方式就是在一个 Redis 实例中使用过期特性(expire)创建一个 key, 如果获得锁的客户端没有释放锁，那么在一定时间内这个 Key 将会自动删除，避免死锁。 这种做法在表面上看起来可行，但分布式锁作为架构中的一个组件,为了避免Redis宕机引起锁服务不可用, 我们需要为 Redis 实例(master)增加热备(slave)，如果 master 不可用则将 slave 提升为 master。 这种主从的配置方式存在一定的安全风险，由于 Redis 的主从复制是异步进行的，可能会发生多个客户端同时持有一个锁的现象。比如以下情况： Client A 获得在 master 节点获得了锁 在 master 将 key 备份到 slave 节点之前，master宕机 slave 被提升为 master Client B 在新的 master 节点处获得了锁，Client A 也持有这个锁 RedLock 算法介绍下面例子中的分布式环境包含 N 个 Redis Master 节点，这些节点相互独立，无需备份。这些节点尽可能相互隔离的部署在不同的物理机或虚拟机上（故障隔离）。 节点数量暂定为 5 个（在需要投票的集群中，5 个节点的配置是比较合理的最小配置方式）。获得锁和释放锁的方式仍然采用之前介绍的方法。 一个 Client 想要获得一个锁需要以下几个操作： 得到本地时间 client 使用相同的 key 和随机数,按照顺序在每个 master 实例中尝试获得锁。在获得锁的过程中，为每一个锁操作设置一个快速失败时间（如果想要获得一个 10 秒的锁， 那么每一个锁操作的失败时间设为 5 - 50ms）。 这样可以避免客户端与一个已经故障的 master 通信占用太长时间，通过快速失败的方式尽快的与集群中的其他节点完成锁操作。 客户端计算出与 master 获得锁操作过程中消耗的时间，当且仅当 client 获得锁消耗的时间小于锁的存活时间，并且在一半以上的 master 节点中获得锁。才认为 client 成功的获得了锁。 如果已经获得了锁，client 执行任务的时间窗口是锁的存活时间减去获得锁消耗的时间。 如果 client 获得锁的数量不足一半以上，或获得锁的时间超时，那么认为获得锁失败。客户端需要尝试在所有的 master 节点中释放锁，即使在第二步中没有成功获得该 master 节点中的锁，仍要进行释放操作。 RedLock 性能及崩溃恢复的相关解决方法 如果 redis 没有持久化功能，在 client A 获取锁成功后，所有 redis 重启，client B 能够再次获取到锁，这样违法了锁的排他互斥性; 如果启动 AOF 永久化存储，事情会好些， 举例：当我们重启 redis 后，由于 redis 过期机制是按照 unix 时间戳走的，所以在重启后，然后会按照规定的时间过期，不影响业务。但是由于 AOF 同步到磁盘的方式默认是每秒一次，如果在一秒内断电，会导致数据丢失，立即重启会造成锁互斥性失效。但如果同步磁盘方式使用 Always(每一个写命令都同步到硬盘)造成性能急剧下降。所以在锁完全有效性和性能方面要有所取舍。 有效解决既保证锁完全有效性及性能高效及即使断电情况的方法是 redis 同步到磁盘方式保持默认的每秒，在 redis 无论因为什么原因停掉后要等待 TTL 时间后再重启（学名:延迟重启），缺点是 在TTL时间内服务相当于暂停状态。 总结： TTL 时长 要大于正常业务执行的时间+获取所有 redis 服务消耗时间+时钟漂移 获取 redis 所有服务消耗时间要远小于 TTL 时间，并且获取成功的锁个数要在总数的一半以上：N/2+1 尝试获取每个 redis 实例锁时的时间要远小于 TTL 时间 尝试获取所有锁失败后重新尝试一定要有一定次数限制 在 redis 崩溃后（无论一个还是所有），要延迟 TTL 时间重启 redis 在实现多 redis 节点时要结合单节点分布式锁算法共同实现 结合JavaJedis：是老牌的 Redis 的 Java 实现客户端，提供了比较全面的 Redis 命令的支持， Redisson：实现了分布式和可扩展的 Java 数据结构。 Lettuce：高级 Redis 客户端，用于线程安全同步，异步和响应使用，支持集群，Sentinel，管道和编码器。 优点： Jedis：比较全面的提供了Redis的操作特性 Redisson：促使使用者对 Redis 的关注分离，提供很多分布式相关操作服务，例如，分布式锁，分布式集合，可通过 Redis 支持延迟队列 Lettuce：基于 Netty 框架的事件驱动的通信层，其方法调用是异步的。Lettuce 的 API 是线程安全的，所以可以操作单个Lettuce 连接来完成各种操作 Jedis 直接连接 redis server，如果在多线程环境下是非线程安全的，这个时候只有使用连接池，为每个 Jedis 实例增加物理连接 。 lettuce 的连接是基于 Netty 的，连接实例（StatefulRedisConnection）可以在多个线程间并发访问，StatefulRedisConnection是线程安全的，所以一个连接实例可以满足多线程环境下的并发访问，当然这也是可伸缩的设计，一个连接实例不够的情况也可以按需增加连接实例。 Redisson 实现了分布式和可扩展的 Java 数据结构，和 Jedis 相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等 Redis 特性。Redisson 的宗旨是促进使用者对 Redis 的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 总结：优先使用 Lettuce，如果需要分布式锁，分布式集合等分布式的高级特性，添加 Redisson 结合使用，因为 Redisson 本身对字符串的操作支持很差。 参考文章 redis相关原理及面试官由浅到深必问的 Redis 选择hash还是string 存储数据？ Redis 持久化 持久化（persistence） 【原创】那些年用过的Redis集群架构（含面试解析） 通过 Redlock 实现分布式锁","categories":[{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"redis","slug":"redis","permalink":"http://beritra.github.com/tags/redis/"},{"name":"NoSQL","slug":"NoSQL","permalink":"http://beritra.github.com/tags/NoSQL/"}]},{"title":"Java多线程","slug":"Java多线程","date":"2020-02-01T10:00:12.000Z","updated":"2022-02-20T11:27:13.888Z","comments":true,"path":"2020/02/01/Java多线程/","link":"","permalink":"http://beritra.github.com/2020/02/01/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"Java 多线程相关的基础知识。","text":"Java 多线程相关的基础知识。 概念线程和进程 何为线程？ 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 何为进程？ 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 线程和进程有何不同？ 线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 生命周期 一个线程有五个基本状态 新建状态：当线程对象创建后，即进入新建状态，如：Thread t = new MyThread(); 就绪状态：当调用线程对象的start()方法时，线程即进入就绪状态。处于就绪状态的线程只是说明此线程已经做好准备，随时等待 CPU 调度执行，并不是说执行了start()方法就立即执行。 运行状态：当 CPU 开始调度处于就绪状态的线程时，此时线程才得以真正执行，即进入到运行状态。 阻塞状态：处于运行状态中的线程由于某种原因，暂时放弃对CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才有机会再次被CPU调用以进入到运行状态。阻塞状态分为三种： 等待阻塞 同步阻塞 其他阻塞 死亡状态：线程执行完毕或者异常退出，该线程结束生命周期。 重要概念synchronized 关键字synchronized是 Java 中的关键字，是利用锁的机制来实现同步的。 锁机制有如下两种特性： 互斥性：即在同一时间只允许一个线程持有某个对象锁，通过这种特性来实现多线程中的协调机制，这样在同一时间只有一个线程对需同步的代码块(复合操作)进行访问。互斥性我们也往往称为操作的原子性。 可见性：必须确保在锁被释放之前，对共享变量所做的修改，对于随后获得该锁的另一个线程是可见的（即在获得锁时应获得最新共享变量的值），否则另一个线程可能是在本地缓存的某个副本上继续操作从而引起不一致。 synchronized的作用域包括以下几个部分： 对象一个线程访问一个对象中的synchronized(this)同步代码块时，其他试图访问该对象的线程将被阻塞。 1public void run() &#123;2 synchronized (this)&#123;3 try &#123;4 System.out.println(Thread.currentThread().getId() + \" running\");5 Thread.sleep(2000);6 System.out.println(Thread.currentThread().getId() + \" complete\");7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 &#125;11&#125; 同一时刻只有一个线程可以执行run()方法 或者锁一个对象 1public Integer count=0;23public void add()&#123;4 synchronized (count)&#123;5 try &#123;6 System.out.println(Thread.currentThread().getId() + \" running\");7 Thread.sleep(2000);8 count++;9 System.out.println(Thread.currentThread().getId() + \" complete\");10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 &#125;14&#125; 起到同样的作用，count这个对象同一时间只能一个线程访问。 方法1public synchronized void run() &#123;2 try &#123;3 System.out.println(Thread.currentThread().getId() + \" running\");4 Thread.sleep(2000);5 System.out.println(Thread.currentThread().getId() + \" complete\");6 &#125; catch (InterruptedException e) &#123;7 e.printStackTrace();8 &#125;9&#125; 代码块（类）synchronized关键字同样可以同步代码块，但是只有代码块内部的代码被锁，同一个方法里的其他代码仍然可以并行执行。另外需要注意，synchronized其实作用的是对象，所以如果这里锁中是this，并不能保证代码被顺序执行，只能保证同一个对象内的代码。因此要改成锁住类。 1public void synchronizedCodeBlock() &#123;2 System.out.println(Thread.currentThread().getId() + \":同一个方法里没有被锁的部分可以同步执行\");3 synchronized (this.getClass()) &#123;4 try &#123;5 Thread.sleep(100);6 &#125; catch (InterruptedException e) &#123;7 e.printStackTrace();89 &#125;10 System.out.println(Thread.currentThread().getId() + \":被锁住的代码部分必须顺序执行\");11 &#125;12&#125; 静态变量、方法、代码块synchronize关键字如果修饰的是一个静态变量、静态方法或者静态代码块的时候，同步的是这个类的所有实例。 锁静态方法、变量的话，会作用于该类的所有实例。 1public synchronized static void synchronizedStaticFunction() &#123;2 try &#123;3 Thread.sleep(200);4 &#125; catch (InterruptedException e) &#123;5 e.printStackTrace();6 &#125;7 System.out.println(Thread.currentThread().getId() + \":静态方法\");8&#125; Lock 接口Lock不是一个关键字，而是一个接口，使用方式跟synchronized类似 总结来说，Lock和synchronized有以下几点不同： Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 Lock可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 volatile 关键字被volatile修饰的共享变量，就具有了以下两点特性： 保证了不同线程对该变量操作的内存可见性; 禁止指令重排序 JMM主要就是围绕着如何在并发过程中如何处理原子性、可见性和有序性这3个特征来建立的，通过解决这三个问题，可以解除缓存不一致的问题。而volatile跟可见性和有序性都有关。volatile不保证原子性。 一个简单的测试 1public class MyVolatileClass &#123;2 int a = 0;3 4 public void add() &#123;5 a = a+1;6 &#125;7&#125; 开启多个线程执行一万次add()方法之后，可以看到a的结果并不是一万，而是要少于一万。无论int a前面加不加volatile结果都一样。所以volatile不能保证操作的原子性。那么volatile的作用是什么？ 1static volatile boolean stop = false;2public static void main(String[] args) &#123;3 new Thread(() -&gt; &#123;4 while(!stop) &#123;5 &#125;6 System.out.println(\"停止了\");7 &#125;).start();8 try &#123;9 Thread.sleep(1000);10 &#125; catch (InterruptedException e) &#123;11 e.printStackTrace();12 &#125;13 stop = true;14 System.out.println(\"赶快停止吧 \" + stop);15&#125; 如果不带volatile关键字，那么循环很可能不会终止。 锁的种类可中断锁Lock是可中断锁，而synchronized不是可中断锁 线程 A 和 B 都要获取对象 O 的锁定，假设 A 获取了对象 O 锁，B 将等待 A 释放对 O 的锁定，如果使用synchronized，如果 A 不释放，B 将一直等下去，不能被中断。 如果使用ReentrantLock，即使 A 不释放，也可以使 B 在等待了足够长的时间以后，中断等待，而干别的事情。 公平锁/非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁。非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。对于synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过 AQS 来实现线程调度，所以并没有任何办法使其变成公平锁。 乐观锁/悲观锁乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。悲观锁在 Java 中的使用，就是利用各种锁。乐观锁在 Java 中的使用，是无锁编程，常常采用的是 CAS 算法，典型的例子就是原子类，通过 CAS 自旋实现原子操作的更新。 独享锁/共享锁独享锁是指该锁一次只能被一个线程所持有。共享锁是指该锁可被多个线程所持有。对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReentrantReadWriteLock，其读锁是共享锁，其写锁是独享锁。读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。对于synchronized 而言，当然是独享锁。 可重入锁/不可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 读写锁ReentrantLock 属于排他锁，这些锁在同一时刻只允许一个线程进行访问，而读写锁在同一时刻可以允许多个线程访问，但是在写线程访问时，所有的读和其他写线程都被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。 自旋锁阻塞或唤醒一个 Java 线程需要操作系统切换 CPU 状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。 在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃 CPU 的执行时间，看看持有锁的线程是否很快就会释放锁。 而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。 借一张网图来总结： 基本使用方式继承 Thread 类1public class MyThread extends Thread &#123;2 @Override3 public void run() &#123;4 System.out.println(\"MyThread is running\");5 &#125;6&#125; 实现 Runnable 接口1public class MyRunnable implements Runnable &#123;2 @Override3 public void run() &#123;4 System.out.println(\"MyRunnable is running\");5 &#125;6&#125; 实现 Callable 接口1public class MyCallable implements Callable &#123;2 @Override3 public Object call() &#123;4 System.out.println(\"MyCallable is running\");5 return null;6 &#125;7&#125; 三者区别 继承Thread不方便共享变量，由于 Java 的单继承机制，继承了Thread类之后不能再继承别的类。 Runnable和Callable功能比较相似，主要区别有： Runnable是自从 Java 1.1就有了，而Callable是1.5之后才加上去的 Callable规定的方法是call()，Runnable规定的方法是run() Callable的任务执行后可返回值，而Runnable的任务没有返回值(是void) call()方法可以抛出异常，run()方法不可以 运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果。 加入线程池运行，Runnable使用ExecutorService的execute方法，Callable使用submit方法 执行run方法和start方法的区别start()的作用是启动一个新的线程。 通过start()方法来启动的新线程，处于就绪（可运行）状态，并没有运行，一旦得到cpu时间片，就开始执行相应线程的run()方法，这里方法run()称为线程体，它包含了要执行的这个线程的内容，run方法运行结束，此线程随即终止。start()不能被重复调用。用start方法来启动线程，真正实现了多线程运行，即无需等待某个线程的run方法体代码执行完毕就直接继续执行下面的代码。这里无需等待run方法执行完毕，即可继续执行下面的代码，即进行了线程切换。 run()就和普通的成员方法一样，可以被重复调用。 如果直接调用run方法，并不会启动新线程！程序中依然只有主线程这一个线程，其程序执行路径还是只有一条，还是要顺序执行，还是要等待run方法体执行完毕后才可继续执行下面的代码，这样就没有达到多线程的目的。 总结：调用start方法方可启动线程，而run方法只是Thread的一个普通方法调用，还是在主线程里执行。 FutureFuture就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。 在Future接口中声明了5个方法，下面依次解释每个方法的作用： cancel(boolean mayInterruptIfRunning)方法用来取消任务，如果取消任务成功则返回true，如果取消任务失败则返回false。参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务，如果设置true，则表示可以取消正在执行过程中的任务。如果任务已经完成，则无论mayInterruptIfRunning为true还是false，此方法肯定返回false，即如果取消已经完成的任务会返回false；如果任务正在执行，若mayInterruptIfRunning设置为true，则返回true，若mayInterruptIfRunning设置为false，则返回false；如果任务还没有执行，则无论mayInterruptIfRunning为true还是false，肯定返回true。 isCancelled()方法表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true。 isDone()方法表示任务是否已经完成，若任务完成，则返回true； get()方法用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回； get(long timeout, TimeUnit unit)用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null。 也就是说Future提供了三种功能： 判断任务是否完成； 能够中断任务； 能够获取任务执行结果。 FutureTask FutureTask类实现了RunnableFuture接口，我们看一下RunnableFuture接口的实现： 1public interface RunnableFuture extends Runnable, Future &#123;2 void run();3&#125; 可以看出RunnableFuture继承了Runnable接口和Future接口，而FutureTask实现了RunnableFuture接口。所以它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。 FutureTask提供了2个构造器： 1public FutureTask(Callable callable) &#123;&#125;2public FutureTask(Runnable runnable, V result) &#123;&#125; 事实上，FutureTask是Future接口的一个唯一实现类。 线程通信和控制在调用wait(), notify()或notifyAll()的时候，必须先获得锁，且状态变量须由该锁保护，而固有锁对象与固有条件队列对象又是同一个对象。也就是说，要在某个对象上执行wait，notify，先必须锁定该对象，而对应的状态变量也是由该对象锁保护的。 调用一个Object的wait与notify/notifyAll的时候，必须保证调用代码对该Object是同步的，也就是说必须在作用等同于synchronized(obj){......}的内部才能够去调用obj的wait与notify/notifyAll三个方法，否则就会报错： 1java.lang.IllegalMonitorStateException:current thread not owner wait/notify/notifyAll一个简单的等待、唤醒示例： 1public class WaitAndNotify &#123;2 Object lock = new Object();34 public void prevFunc() &#123;5 try &#123;6 Thread.sleep(2000);7 synchronized (lock) &#123;8 lock.notifyAll();9 &#125;1011 &#125; catch (InterruptedException e) &#123;12 e.printStackTrace();13 &#125;14 &#125;1516 public void nextFunc() &#123;17 try &#123;18 synchronized (lock) &#123;19 System.out.println(Thread.currentThread().getId() + \"等待任务执行,线程wait\");20 lock.wait();21 System.out.println(Thread.currentThread().getId() + \"任务执行完毕,线程notify\");22 &#125;2324 &#125; catch (InterruptedException e) &#123;25 e.printStackTrace();26 &#125;2728 &#125;2930 public static void main(String[] args) &#123;31 WaitAndNotify waitAndNotify = new WaitAndNotify();32 new Thread(waitAndNotify::nextFunc).start();33 new Thread(waitAndNotify::nextFunc).start();34 new Thread(waitAndNotify::nextFunc).start();35 new Thread(waitAndNotify::prevFunc).start();36 &#125;37&#125; notify和notifyAll的区别是前者只随机唤醒一个线程，后者唤醒所有。 joinjoin的作用很简单：让一个线程等待另一个线结束之后才能继续运行。 举个栗子，主线程创造一个子线程执行耗时操作，等子线程执行完之后回到主线程继续处理。 1public class ThreadJoin &#123;2 void funcA() &#123;3 try &#123;4 System.out.println(Thread.currentThread().getId() + \"开始耗时操作\");5 Thread.sleep(2000);6 System.out.println(Thread.currentThread().getId() + \"耗时操作完成\");7 &#125; catch (InterruptedException e) &#123;8 e.printStackTrace();9 &#125;10 &#125;1112 public static void main(String[] args) &#123;13 ThreadJoin threadJoin = new ThreadJoin();14 Thread thread = new Thread(threadJoin::funcA);15 thread.start();16 try &#123;17 System.out.println(\"主线程等待结果\");18 thread.join();19 System.out.println(\"主线程结束\");20 &#125; catch (InterruptedException e) &#123;21 e.printStackTrace();22 &#125;23 &#125;24&#125; yieldyield的作用也很简单，就是让出当前时间片，让其他线程使用 CPU，自身从运行状态重新变成就绪状态，然后重新竞争 CPU 的使用权。 平时几乎没用过yield，看到一篇博客上这么说明的： yield 方法可以很好的控制多线程，如执行某项复杂的任务时，如果担心占用资源过多，可以在完成某个重要的工作后使用 yield 方法让掉当前 CPU 的调度权，等下次获取到再继续执行，这样不但能完成自己的重要工作，也能给其他线程一些运行的机会，避免一个线程长时间占有 CPU 资源。 Interrupt线程的interrupt()方法是中断线程，将会设置该线程的中断状态位，即设置为true，中断的结果线程是死亡、还是等待新的任务或是继续运行至下一步，就取决于这个程序本身。线程会不时地检测这个中断标示位，以判断线程是否应该被中断（中断标示值是否为true）。它并不像stop方法那样会中断一个正在运行的线程。 一个最简单的用法，在线程内判断是否被中断，线程外进行中断操作： 1while (!Thread.interrupted()) &#123;2 System.out.println(\"running\");3&#125; 底层实现方式synchronized 的实现方式由于对 c++ 的源码不熟悉，仅从现有的博客中总结synchronized关键字实现方式。 每个对象都拥有自己的监视器，当这个对象由同步块或者这个对象的同步方法调用时，执行方法的线程必须先获取该对象的监视器才能进入同步块和同步方法，如果没有获取到监视器的线程将会被阻塞在同步块和同步方法的入口处，进入到 BLOCKED 状态。 对于同步方法：方法级同步没有通过字节码指令来控制，它实现在方法调用和返回操作之中。当方法调用时，调用指令会检查方法ACC_SYNCHRONIZED访问标志是否被设置，若设置了则执行线程需要持有监视器(Monitor)才能运行方法，当方法完成(无论是否出现异常)时释放监视器。 对于同步代码块：synchronized关键字经过编译后，会在同步块的前后分别形成monitorenter和monitorexit两个字节码指令，每条monitorenter指令都必须执行其对应的monitorexit指令，为了保证方法异常完成时这两条指令依然能正确执行，编译器还会自动产生一个异常处理器，其目的就是用来执行monitorexit指令。 Monitor是线程私有的数据结构，每个线程都有一个可用monitor record列表，同时 还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联(对象头的MarkWord中的LockWord指向monitor的起始地址)，同时monitor中有一个owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。其结构如下: Owner：初始时为 NULL 表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为 NULL EntryQ：关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程 RcThis：表示blocked或waiting在该monitor record上的所有线程的个数 Nest：用来实现重入锁的计数 HashCode：保存从对象头拷贝过来的HashCode值（可能还包含GC age） Candidate：用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值 0 表示没有需要唤醒的线程 1 表示要唤醒一个继任线程来竞争锁。 Lock 实现原理加锁过程以ReentrantLock为例，我们看看他都做了些什么： 1public ReentrantLock() &#123;2 sync = new NonfairSync();3&#125; 在构造方法中，就是初始化了一个静态内部类，叫做非公平同步，继续点进去，可以看到类似这个NonfairSync，还有个FairSync，都是继承于Sync: 1static final class NonfairSync extends Sync2static final class FairSync extends Sync 而Sync由继承自AbstractQueuedSynchronizer，这么长一坨是个什么东西？从名字里可以看出是个抽象的队列同步器。而他由继承自AbstractOwnableSynchronizer(AOS)，整个继承关系如下： FairSync与NonfairSync的区别在于，是不是保证获取锁的公平性，因为默认是NonfairSync，我们就先看这个。 点进AbstractQueuedSynchronizer代码里面，可以看到内部维护了一个双向链表，锁的存储结构归根结底就是两个东西：双向链表+int类型的状态。 可以看到，都是用volatile关键字修饰的，链表的头尾还有transient关键字。 获取锁的时候，又做了什么？我们查看ReentrantLock的lock()方法： 1public void lock() &#123;2 sync.acquire(1);3&#125; 继续追踪，借着注释的帮助： 1/**2 * Acquires in exclusive mode, ignoring interrupts. Implemented3 * by invoking at least once &#123;@link #tryAcquire&#125;,4 * returning on success. Otherwise the thread is queued, possibly5 * repeatedly blocking and unblocking, invoking &#123;@link6 * #tryAcquire&#125; until success. This method can be used7 * to implement method &#123;@link Lock#lock&#125;.8 *9 * @param arg the acquire argument. This value is conveyed to10 * &#123;@link #tryAcquire&#125; but is otherwise uninterpreted and11 * can represent anything you like.12 */13public final void acquire(int arg) &#123;14 if (!tryAcquire(arg) &amp;&amp;15 acquireQueued(addWaiter(Node.EXCLUSIVE), arg))16 selfInterrupt();17&#125; 可以看到这里实际上做了三个动作： 尝试设置state，也就是获取锁，这里的tryAcquire实际上对应了Fair和Nonefair两种情况，我们继续以默认的非公平来看。（公平的情况其实类似，只不过在加锁之前判断了一下当前线程是不是在等待队列的队首，以保证公平性。） 1final boolean nonfairTryAcquire(int acquires) &#123;2 final Thread current = Thread.currentThread();3 int c = getState();4 if (c == 0) &#123;5 if (compareAndSetState(0, acquires)) &#123;6 setExclusiveOwnerThread(current);7 return true;8 &#125;9 &#125;10 else if (current == getExclusiveOwnerThread()) &#123;11 int nextc = c + acquires;12 if (nextc &lt; 0) // overflow13 throw new Error(\"Maximum lock count exceeded\");14 setState(nextc);15 return true;16 &#125;17 return false;18&#125; 很好理解，如果state==0，就是说锁没有被占用，就把state原子操作地设置为 1，也就是占用锁，然后记录当前线程。如果请求占用锁的是当前持有锁的线程，就把state加一。其他情况获取不到锁，返回失败。 这一步如果获取锁成功，就没有后续步骤了，如果失败，就进行第二步。 addWaiter：将当前线程加入上面锁的双向链表（等待队列）中。这里用的是CAS的方式，这一步比较简单就不放源码了。 acquireQueued()： 1/**2 * Acquires in exclusive uninterruptible mode for thread already in3 * queue. Used by condition wait methods as well as acquire.4 *5 * @param node the node6 * @param arg the acquire argument7 * @return &#123;@code true&#125; if interrupted while waiting8 */ 9final boolean acquireQueued(final Node node, int arg) &#123;10 boolean interrupted = false;11 try &#123;12 for (;;) &#123;13 final Node p = node.predecessor();14 if (p == head &amp;&amp; tryAcquire(arg)) &#123;15 setHead(node);16 p.next = null; // help GC17 return interrupted;18 &#125;19 if (shouldParkAfterFailedAcquire(p, node))20 interrupted |= parkAndCheckInterrupt();21 &#125;22 &#125; catch (Throwable t) &#123;23 cancelAcquire(node);24 if (interrupted)25 selfInterrupt();26 throw t;27 &#125;28&#125; 前面我们已经把当前线程加到队列的队尾了，acquireQueued()的作用就是逐步的去执行等待队列的线程，如果当前线程获取到了锁，则返回；否则，当前线程进行休眠，直到唤醒并重新获取锁了才返回。 shouldParkAfterFailedAcquire()方法的作用是判断当前线程是否需要被阻塞，具体的判断规则如下： 规则1：如果前继节点状态为SIGNAL，表明当前节点需要被unpark(唤醒)，此时则返回true。parkAndCheckInterrupt()的作用是阻塞当前线程，并且返回“线程被唤醒之后”的中断状态。 规则2：如果前继节点状态为CANCELLED(ws&gt;0)，说明前继节点已经被取消，则通过先前回溯找到一个有效(非CANCELLED状态)的节点，并返回false。 规则3：如果前继节点状态为非SIGNAL、非CANCELLED，则设置前继的状态为SIGNAL，并返回false。 看代码： 1private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123;2 int ws = pred.waitStatus;// 前继节点的状态3 if (ws == Node.SIGNAL)// 如果前继节点是SIGNAL状态，则意味这当前线程需要被unpark唤醒。此时，返回true。4 /*5 * This node has already set status asking a release6 * to signal it, so it can safely park.7 */8 return true;9 if (ws &gt; 0) &#123;// 如果前继节点是“取消”状态，则设置 “当前节点”的 “当前前继节点” 为 “‘原前继节点’的前继节点”。10 /*11 * Predecessor was cancelled. Skip over predecessors and12 * indicate retry.13 */14 do &#123;15 node.prev = pred = pred.prev;16 &#125; while (pred.waitStatus &gt; 0);17 pred.next = node;18 &#125; else &#123; // 如果前继节点为“0”或者“共享锁”状态，则设置前继节点为SIGNAL状态。19 /*20 * waitStatus must be 0 or PROPAGATE. Indicate that we21 * need a signal, but don't park yet. Caller will need to22 * retry to make sure it cannot acquire before parking.23 */24 pred.compareAndSetWaitStatus(ws, Node.SIGNAL);25 &#125;26 return false;27&#125; 如果“规则1”发生，即“前继节点是SIGNAL”状态，则意味着“当前线程”需要被阻塞。接下来会调用parkAndCheckInterrupt()阻塞当前线程，直到当前先被唤醒才从parkAndCheckInterrupt()中返回。parkAndCheckInterrupt()的作用是阻塞当前线程，并且返回“线程被唤醒之后”的中断状态。 三个步骤都完成之后，if()中的判断终于结束了，进入到方法selfInterrupt()。这个就很简单了，当前线程自己执行一个中断。 再回顾整个过程： 先是通过tryAcquire()尝试获取锁。获取成功的话，直接返回；尝试失败的话，再通过acquireQueued()获取锁。 尝试失败的情况下，会先通过addWaiter()来将当前线程加入到CLH队列末尾；然后调用acquireQueued()，在CLH队列中排序等待获取锁，在此过程中，线程处于休眠状态。直到获取锁了才返回。 如果在休眠等待过程中被中断过，则调用selfInterrupt()来自己产生一个中断。 释放锁过程释放锁过程相对简单： 1public void unlock() &#123;2 sync.release(1);3&#125; 继续追踪： 1public final boolean release(int arg) &#123;2 if (tryRelease(arg)) &#123;3 Node h = head;4 if (h != null &amp;&amp; h.waitStatus != 0)5 unparkSuccessor(h);6 return true;7 &#125;8 return false;9&#125; 可以看到，先尝试释放锁，这里就是判断下锁的state的数字是不是 0，当前线程是不是锁的持有者。释放成功之后，唤醒等待队列的后续节点，因为已经让出锁了，可以让后续的节点进行操作。 可能遇到的问题和解决办法死锁有可能产生死锁的情况： 系统资源的竞争 通常系统中拥有的不可剥夺资源，其数量不足以满足多个线程运行的需要，使得线程在 运行过程中，会因争夺资源而陷入僵局，如磁带机、打印机等。只有对不可剥夺资源的竞争 才可能产生死锁，对可剥夺资源的竞争是不会引起死锁的。 线程推进顺序非法 线程在运行过程中，请求和释放资源的顺序不当，也同样会导致死锁。例如，并发线程 P1、P2 分别保持了资源 R1、R2，而线程 P1 申请资源 R2，线程 P2 申请资源 R1 时，两者都 会因为所需资源被占用而阻塞。 信号量使用不当也会造成死锁。线程间彼此相互等待对方发来的消息，结果也会使得这 些线程间无法继续向前推进。例如，线程A等待线程B发的消息，线程B又在等待线程A 发的消息，可以看出线程A和B不是因为竞争同一资源，而是在等待对方的资源导致死锁。 死锁是由四个必要条件导致的，所以一般来说，只要破坏这四个必要条件中的一个条件，死锁情况就应该不会发生。 如果想要打破互斥条件，我们需要允许进程同时访问某些资源，这种方法受制于实际场景，不太容易实现条件； 打破不可抢占条件，这样需要允许进程强行从占有者那里夺取某些资源，或者简单一点理解，占有资源的进程不能再申请占有其他资源，必须释放手上的资源之后才能发起申请，这个其实也很难找到适用场景； 进程在运行前申请得到所有的资源，否则该进程不能进入准备执行状态。这个方法看似有点用处，但是它的缺点是可能导致资源利用率和进程并发性降低； 避免出现资源申请环路，即对资源事先分类编号，按号分配。这种方式可以有效提高资源的利用率和系统吞吐量，但是增加了系统开销，增大了进程对资源的占用时间。 面试题多个线程交替顺序打印比如两个线程，从零开始，一个打印偶数一个打印奇数。 方法一，轮流休眠唤醒： 1int times = 100 * 10000;2Thread thread1;3Thread thread2;45thread1 = new Thread(() -&gt; &#123;6 for (int i = 0; i &lt; times; i++) &#123;7 final int index1 = 2 * i;8 synchronized (this) &#123;9 print(\"偶数打印：\" + index1);10 if (i == times - 1)11 break;12 try &#123;13 this.notify();14 this.wait();15 &#125; catch (InterruptedException e) &#123;16 e.printStackTrace();17 &#125;18 &#125;19 &#125;20&#125;);2122thread2 = new Thread(() -&gt; &#123;23 for (int i = 0; i &lt; times; i++) &#123;24 final int index2 = 2 * i + 1;25 synchronized (this) &#123;26 print(\"奇数打印：\" + index2);27 if (i == times - 1)28 break;29 try &#123;30 this.notify();31 this.wait(100);32 &#125; catch (InterruptedException e) &#123;33 e.printStackTrace();34 &#125;35 &#125;36 &#125;37&#125;);38thread1.start();39thread2.start();40thread2.join(); 多次测试平均耗时大概 7.7 秒左右。 方法二，利用volitale关键字实现 CAS： 1private static volatile boolean flag = true;23public void func2() &#123;4 int times = 100 * 10000;5 ExecutorService service = Executors.newFixedThreadPool(2);6 service.execute(() -&gt; &#123;7 int i = 0;8 while (i &lt; times) &#123;9 if (flag) &#123;10 print(\"偶数打印：\" + 2 * i++);11 flag = false;12 &#125;13 &#125;14 &#125;);15 service.execute(() -&gt; &#123;16 int i = 0;17 while (i &lt; times) &#123;18 if (!flag) &#123;19 print(\"奇数打印：\" + (1 + 2 * i++));20 flag = true;21 &#125;22 &#125;23 &#125;);24 service.shutdown();25 try &#123;26 service.awaitTermination(Integer.MAX_VALUE, TimeUnit.HOURS);27 &#125; catch (InterruptedException e) &#123;28 e.printStackTrace();29 &#125;30&#125; 多次测试平均时间消耗大概在 3.6 秒，可以看出不用锁会快很多。 方法三，类似方法一，用Lock和Condition轮流休眠和唤醒，效率略高： 1int times = 100 * 10000;2Lock lock = new ReentrantLock();3Condition condition = lock.newCondition();45Thread thread1 = new Thread(() -&gt; &#123;6 int i = 0;7 while (i &lt; times) &#123;8 lock.lock();9 print(\"偶数打印:\" + 2 * i++);10 try &#123;11 condition.signal();12 condition.await();13 &#125; catch (InterruptedException e) &#123;14 e.printStackTrace();15 &#125;16 lock.unlock();17 &#125;18&#125;);19Thread thread2 = new Thread(() -&gt; &#123;20 int i = 0;21 while (i &lt; times) &#123;22 lock.lock();23 print(\"奇数打印:\" + (1 + 2 * i++));24 if (i == times)25 return;26 try &#123;27 condition.signal();28 condition.await();29 &#125; catch (InterruptedException e) &#123;30 e.printStackTrace();31 &#125;32 lock.unlock();33 &#125;34&#125;);35thread1.start();36thread2.start();37try &#123;38 thread2.join();39&#125; catch (InterruptedException e) &#123;40 e.printStackTrace();41&#125; 多次测试执行时间在 6.4 毫秒左右。 线程池Java 通过 Executors 提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 后面还新增了两个： newSingleThreadScheduledExecutor单线程的任务队列。 newWorkStealingPool工作窃取线程池，内部通过 ForkJoinPool 来实现，通过切分任务让所有 CPU 都不闲置。 在阿里巴巴Java开发手册中也明确指出，是『不允许』使用Executors创建线程池。 避免使用 Executors 创建线程池，主要是避免使用其中的默认实现，那么我们可以自己直接调用ThreadPoolExecutor的构造函数来自己创建线程池。在创建的同时，给BlockQueue指定容量就可以了。 应用java.util.concurrent包 数据结构：ConcurrentHashMap, BlockingQueue 系列 线程池：Executor, ExecutorService, ThreadPoolExecutor, ScheduledThreadPoolExecutor, Executors（工厂类）, Callable, Runnable, Future 锁：ReentrantLock, ReentrantReadWriteLock, Condition 线程同步：CountDownLatch, CyclicBarrier 本来想一篇都写写，发现内容实在太多了，这部分后续再单开博客详细论述吧…这篇够长了。 参考链接 Java多线程学习（一）Java多线程入门 java-线程中start和run的区别 Java多线程学习（吐血超详细总结） Java并发编程：Callable、Future和FutureTask Java 之 synchronized 详解 Java并发编程：Lock java中几种锁，分别是什么？ Thread的中断机制(interrupt) Java并发——关键字synchronized解析","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"InnoDB和MyISAM区别和联系","slug":"InnoDB和MyISAM区别和联系","date":"2019-12-24T04:12:12.000Z","updated":"2020-05-20T12:33:15.573Z","comments":true,"path":"2019/12/24/InnoDB和MyISAM区别和联系/","link":"","permalink":"http://beritra.github.com/2019/12/24/InnoDB%E5%92%8CMyISAM%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB/","excerpt":"从别人博客抄来的 MySQL 中 InnoDB 引擎与 MyISAM 引擎的区别与联系。","text":"从别人博客抄来的 MySQL 中 InnoDB 引擎与 MyISAM 引擎的区别与联系。 InnoDB：MySQL默认的事务型引擎，也是最重要和使用最广泛的存储引擎。它被设计成为大量的短期事务，短期事务大部分情况下是正常提交的，很少被回滚。InnoDB 的性能与自动崩溃恢复的特性，使得它在非事务存储需求中也很流行。除非有非常特别的原因需要使用其他的存储引擎，否则应该优先考虑 InnoDB 引擎。 MyISAM：在MySQL 5.1 及之前的版本，MyISAM 是默认引擎。MyISAM 提供的大量的特性，包括全文索引、压缩、空间函数（GIS）等，但 MyISAM 并不支持事务以及行级锁，而且一个毫无疑问的缺陷是崩溃后无法安全恢复。正是由于 MyISAM 引擎的缘故，即使 MySQL 支持事务已经很长时间了，在很多人的概念中 MySQL 还是非事务型数据库。尽管这样，它并不是一无是处的。对于只读的数据，或者表比较小，可以忍受修复操作，则依然可以使用 MyISAM（但请不要默认使用 MyISAM，而是应该默认使用 InnoDB） 1、 存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。分别为：表定义文件、数据文件、索引文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm 文件存储表定义。数据文件的扩展名为 .MYD (MYData)。索引文件的扩展名是 .MYI (MYIndex)。 InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB 表的大小只受限于操作系统文件的大小，一般为 2GB。 2、 存储空间MyISAM： MyISAM支持支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。当表在创建之后并导入数据之后，不会再进行修改操作，可以使用压缩表，极大的减少磁盘的空间占用。 InnoDB： 需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 3、 可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。 InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十 G 的时候就相对痛苦了。 4、 事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行数度比 InnoDB 类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 5、 AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 InnoDB：InnoDB 中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 6、 表锁差异MyISAM： 只支持表级锁，用户在操作表时，select，update，delete，insert 语句都会给表自动加锁，如果加锁以后的表满足 insert 并发的情况下，可以在表的尾部插入新的数据。 InnoDB： 支持事务和行级锁，是 InnoDB 的最大特色。行锁大幅度提高了多用户并发操作的新能。但是 InnoDB 的行锁，只有在查询条件走索引的时候，如果没有走索引，仍然是锁表。 7、 全文索引MySql全文索引 MyISAM：支持 FULLTEXT 类型的全文索引 InnoDB：MySQL 5.6 以前的版本不支持 FULLTEXT 类型的全文索引，但是 innodb 可以使用 sphinx 插件支持全文索引，并且效果更好。 MySQL 5.6 及以后的版本，MyISAM 和 InnoDB 存储引擎均支持全文索引，只有字段的数据类型为 char、varchar、text 及其系列才可以建全文索引。 8、表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个 6 字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 9、表的具体行数MyISAM： 保存有表的总行数，如果select count(*) from table;会直接取出出该值。* InnoDB： 没有保存表的总行数，如果使用select count(*) from table;就会遍历整个表，消耗相当大，但是在加了 where 条件后，MyISAM 和 InnoDB 处理的方式都一样。 10、CRUD操作MyISAM：如果执行大量的 SELECT，MyISAM 是更好的选择。 InnoDB：如果你的数据执行大量的 INSERT 或 UPDATE，出于性能方面的考虑，应该使用 InnoDB 表。 11、 外键MyISAM：不支持 InnoDB：支持 参考文章： Mysql 中 MyISAM 和 InnoDB 的区别有哪些？","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"InnoDB","slug":"InnoDB","permalink":"http://beritra.github.com/tags/InnoDB/"}]},{"title":"ConcurrentHashMap使用和原理","slug":"ConcurrentHashMap使用和原理","date":"2019-12-21T04:12:12.000Z","updated":"2022-02-20T11:27:13.896Z","comments":true,"path":"2019/12/21/ConcurrentHashMap使用和原理/","link":"","permalink":"http://beritra.github.com/2019/12/21/ConcurrentHashMap%E4%BD%BF%E7%94%A8%E5%92%8C%E5%8E%9F%E7%90%86/","excerpt":"ConcurrentHashMap是在 JDK 1.5 时，J.U.C 引入的一个同步集合工具类，顾名思义，这是一个线程安全的 HashMap。不同版本的 ConcurrentHashMap，内部实现机制千差万别，本节所有的讨论基于 JDK 1.8。","text":"ConcurrentHashMap是在 JDK 1.5 时，J.U.C 引入的一个同步集合工具类，顾名思义，这是一个线程安全的 HashMap。不同版本的 ConcurrentHashMap，内部实现机制千差万别，本节所有的讨论基于 JDK 1.8。 基本结构 ConcurrentHashMap内部维护了一个Node类型的数组，也就是table： 1transient volatile Node[] table; 数组的每一个位置table[i]代表了一个桶，当插入键值对时，会根据键的hash值映射到不同的桶位置，table 一共可以包含 4 种不同类型的桶：Node、TreeBin、ForwardingNode、ReservationNode。上图中，不同的桶用不同颜色表示。可以看到，有的桶链接着链表，有的桶链接着树。 TreeBin *所链接的是一颗红黑树，红黑树的结点用 *TreeNode 表示，所以 ConcurrentHashMap 中实际上一共有五种不同类型的 Node 结点。 之所以用 TreeBin 而不是直接用 TreeNode，是因为红黑树的操作比较复杂，包括构建、左旋、右旋、删除，平衡等操作，用一个代理结点 TreeBin 来包含这些复杂操作，其实是一种“职责分离”的思想。另外 TreeBin 中也包含了一些加/解锁的操作。 节点定义Node 节点Node 结点的定义非常简单，也是其它四种类型结点的父类。 默认链接到table[i]——桶上的结点就是 Node 结点。当出现 hash 冲突时，Node 结点会首先以链表的形式链接到 table 上，当结点数量超过一定数目时，链表会转化为红黑树。因为链表查找的平均时间复杂度为O(n)，而红黑树是一种平衡二叉树，其平均时间复杂度为O(logn)。 TreeNode 结点TreeNode就是红黑树的结点，TreeNode 不会直接链接到table[i]——桶上面，而是由TreeBin 链接，TreeBin 会指向红黑树的根结点。 TreeBin 结点TreeBin 相当于 TreeNode 的代理结点。TreeBin 会直接链接到table[i]——桶上面，该结点提供了一系列红黑树相关的操作，以及加锁、解锁操作。 ForwardingNode 结点ForwardingNode 结点仅仅在扩容时才会使用 ReservationNode 结点保留结点，ConcurrentHashMap 中的一些特殊方法会专门用到该类结点。 常量定义1/**2 * 最大容量.3 */4private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;56/**7 * 默认初始容量8 */9private static final int DEFAULT_CAPACITY = 16;1011/**12 * The largest possible (non-power of two) array size.13 * Needed by toArray and related methods.14 */15static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;1617/**18 * 负载因子，为了兼容JDK1.8以前的版本而保留。19 * JDK1.8中的ConcurrentHashMap的负载因子恒定为0.7520 */21private static final float LOAD_FACTOR = 0.75f;2223/**24 * 链表转树的阈值，即链接结点数大于8时， 链表转换为树.25 */26static final int TREEIFY_THRESHOLD = 8;2728/**29 * 树转链表的阈值，即树结点树小于6时，树转换为链表.30 */31static final int UNTREEIFY_THRESHOLD = 6;3233/**34 * 在链表转变成树之前，还会有一次判断：35 * 即只有键值对数量大于MIN_TREEIFY_CAPACITY，才会发生转换。36 * 这是为了避免在Table建立初期，多个键值对恰好被放入了同一个链表中而导致不必要的转化。37 */38static final int MIN_TREEIFY_CAPACITY = 64;3940/**41 * 在树转变成链表之前，还会有一次判断：42 * 即只有键值对数量小于MIN_TRANSFER_STRIDE，才会发生转换.43 */44private static final int MIN_TRANSFER_STRIDE = 16;4546/**47 * 用于在扩容时生成唯一的随机数.48 */49private static int RESIZE_STAMP_BITS = 16;5051/**52 * 可同时进行扩容操作的最大线程数.53 */54private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;5556/**57 * The bit shift for recording size stamp in sizeCtl.58 */59private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;6061static final int MOVED = -1; // 标识ForwardingNode结点（在扩容时才会出现，不存储实际数据）62static final int TREEBIN = -2; // 标识红黑树的根结点63static final int RESERVED = -3; // 标识ReservationNode结点（）64static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash6566/**67 * CPU核心数，扩容时使用68 */69static final int NCPU = Runtime.getRuntime().availableProcessors(); 字段定义1/**2 * Node数组，标识整个Map，首次插入元素时创建，大小总是2的幂次.3 */4transient volatile Node&lt;K, V&gt;[] table;56/**7 * 扩容后的新Node数组，只有在扩容时才非空.8 */9private transient volatile Node&lt;K, V&gt;[] nextTable;1011/**12 * 控制table的初始化和扩容.13 * 0 : 初始默认值14 * -1 : 有线程正在进行table的初始化15 * &gt;0 : table初始化时使用的容量，或初始化/扩容完成后的threshold16 * -(1 + nThreads) : 记录正在执行扩容任务的线程数17 */18private transient volatile int sizeCtl;1920/**21 * 扩容时需要用到的一个下标变量.22 */23private transient volatile int transferIndex;2425/**26 * 计数基值,当没有线程竞争时，计数将加到该变量上。类似于LongAdder的base变量27 */28private transient volatile long baseCount;2930/**31 * 计数数组，出现并发冲突时使用。类似于LongAdder的cells数组32 */33private transient volatile CounterCell[] counterCells;3435/**36 * 自旋标识位，用于CounterCell[]扩容时使用。类似于LongAdder的cellsBusy变量37 */38private transient volatile int cellsBusy;394041// 视图相关字段42private transient KeySetView&lt;K, V&gt; keySet;43private transient ValuesView&lt;K, V&gt; values;44private transient EntrySetView&lt;K, V&gt; entrySet; get 操作get 方法的逻辑很简单，首先根据 key 的 hash 值计算映射到 table 的哪个桶——table[i]。 如果table[i]的 key 和待查找 key 相同，那直接返回； 如果table[i]对应的结点是特殊结点（hash值小于0），则通过find方法查找； 如果table[i]对应的结点是普通链表结点，则按链表方式查找。 第二种情况下，table[i]是不同的节点，处理方式也会不同： Node 结点的查找当槽table[i]被普通 Node 结点占用，说明是链表链接的形式，直接从链表头开始查找。 TreeBin 结点的查找TreeBin 的查找比较特殊，我们知道当槽table[i]被 TreeBin 结点占用时，说明链接的是一棵红黑树。由于红黑树的插入、删除会涉及整个结构的调整，所以通常存在读写并发操作的时候，是需要加锁的。 ConcurrentHashMap 采用了一种类似读写锁的方式：当线程持有写锁（修改红黑树）时，如果读线程需要查找，不会像传统的读写锁那样阻塞等待，而是转而以链表的形式进行查找（TreeBin本身时Node类型的子类，所有拥有 Node 的所有字段） ForwardingNode 结点的查找ForwardingNode 是一种临时结点，在扩容进行中才会出现，所以查找也在扩容的table上进行。 ReservationNode 结点的查找ReservationNode 是保留结点，不保存实际数据，所以直接返回null。 put 操作put 方法内部调用了putVal这个私有方法 putVal的逻辑还是很清晰的，首先根据key计算hash值，然后通过hash值与table容量进行运算，计算得到key所映射的索引——也就是对应到table中桶的位置。 这里需要注意的是计算索引的方式：i = (n - 1) &amp; hash n - 1 == table.length - 1，table.length 的大小必须为2的幂次的原因就在这里。 读者可以自己计算下，当table.length为2的幂次时，(table.length-1)的二进制形式的特点是除最高位外全部是 1，配合这种索引计算方式可以实现 key 在 table 中的均匀分布，减少 hash 冲突——出现 hash 冲突时，结点就需要以链表或红黑树的形式链接到table[i]，这样无论是插入还是查找都需要额外的时间。 putVal方法一共处理四种情况： 1、首次初始化 table 懒加载ConcurrentHashMap在构造的时候并不会初始化 table 数组，首次初始化就在这里通过initTable方法完成。首次加载的时候，会通过sizeCtl变量来控制，只有变量修改成功后才开始初始化，避免了多线程同时初始化的问题。 2、table[i] 对应的桶为空最简单的情况，直接 CAS 操作占用桶table[i]即可。 3、发现 ForwardingNode 结点，说明此时 table 正在扩容，则尝试协助进行数据迁移ForwardingNode 结点是 ConcurrentHashMap 中的五类结点之一，相当于一个占位结点，表示当前 table 正在进行扩容，当前线程可以尝试协助数据迁移。 4、出现 hash 冲突,也就是table[i]桶中已经有了结点ConcurrentHashMap 就是在最后一种情况这里加锁的。 当两个不同key映射到同一个table[i]桶中时，就会出现这种情况： 当table[i]的结点类型为 Node——链表结点时，就会将新结点以“尾插法”的形式插入链表的尾部。 当table[i]的结点类型为 TreeBin——红黑树代理结点时，就会将新结点通过红黑树的插入方式插入。 再然后，涉及将链表转换为红黑树 —— treeifyBin，但实际情况并非立即就会转换，当 table 的容量小于 64 时，出于性能考虑，只是对 table 数组扩容1倍——tryPresize。 addCount 操作在putVal的最后一步，就是调用addCount方法，把计数器增加，但是并发环境下也不是一个简简单单的操作，我们看下这个方法都做了什么。 在看这个方法之前，必须了解ConcurrentHashMap进行数量统计的基本方法，它引入了两个变量baseCount、counterCells。baseCount负责简单计数，直接 CAS 操作，用于在没有出现竞争的情况下统计，操作失败的再用counterCells进行补充，补充方式有点复杂，这里先不展开，改天值得单独开文解读，总的来说思路是跟 LongAdder 类似。最终计数结果是baseCount、counterCells两个变量取和。 判断计数盒子属性是否是空，如果是空，就尝试修改 baseCount 变量，对该变量进行加 X。 如果计数盒子不是空，或者修改 baseCount 变量失败了，则放弃对 baseCount 进行操作。 如果计数盒子是null或者计数盒子的 length 是 0，或者随机取一个位置取于数组长度是null，那么就对刚刚的元素进行 CAS 赋值。 如果赋值失败，或者满足上面的条件，则调用fullAddCount方法重新死循环插入。 fullAddCount的逻辑简单来说是分配一个数组counterCells，长度从 2 开始， 最多是大于等于 cpu 数量的 2 的整数次幂（比如 10 个 cpu 就是 16）。然后对当前线程 hash 取模找到对应位置，进行 cas 累加。由于 cpu 数量就这么多，不可能所有位置都 cas 失败，基本上是空间换时间的思想。 这里如果操作 baseCount 失败了（或者计数盒子不是null），且对计数盒子赋值成功，那么就检查 check 变量，如果该变量小于等于 1。直接结束。否则，计算一下 count 变量。 如果 check 大于等于 0 ，说明需要对是否扩容进行检查。 如果 map 的 size 大于sizeCtl（扩容阈值），且 table 的长度没到最大值，那么就进行扩容。 根据 length 得到一个标识符，然后，判断sizeCtl状态，如果小于 0 ，说明其他线程在初始化或者在扩容。 如果正在扩容，那么就校验一下数据是否变化了。如果检验数据不通过，break。 如果校验数据通过了，那么将sizeCtl加一，表示多了一个线程帮助扩容。然后进行扩容。 如果没有在扩容，但是需要扩容。那么就将sizeCtl更新，赋值为标识符左移 16 位 —— 一个负数。然后加 2。 表示已经有一个线程开始扩容了，然后进行扩容。然后再次更新 count，看看是否还需要扩容。 transfer 扩容操作扩容同样是非常重要的步骤，也是很容易发生线程安全问题的部分，让我们看看ConcurrentHashMap怎么做的： 通过计算 CPU 核心数和 Map 数组的长度得到每个线程（CPU）要帮助处理多少个桶，并且这里每个线程处理都是平均的。默认每个线程处理 16 个桶。因此，如果长度是 16 的时候，扩容的时候只会有一个线程扩容。 初始化临时变量nextTable。将其在原有基础上扩容两倍。 死循环开始转移。多线程并发转移就是在这个死循环中，根据一个finishing变量来判断，该变量为 true 表示扩容结束，否则继续扩容。 进入一个 while 循环，分配数组中一个桶的区间给线程，默认是 16。 出 while 循环，进 if 判断，判断扩容是否结束，如果扩容结束，清空临死变量，更新 table 变量，更新扩容阈值。如果没完成，但已经无法领取区间（没了），该线程退出该方法，并将 sizeCtl 减一，表示扩容的线程少一个了。如果减完这个数以后，sizeCtl 回归了初始状态，表示没有线程再扩容了，该方法所有的线程扩容结束了。（这里主要是判断扩容任务是否结束，如果结束了就让线程退出该方法，并更新相关变量）。然后检查所有的桶，防止遗漏。 如果没有完成任务，且 i 对应的槽位是空，尝试 CAS 插入占位符，让putVal方法的线程感知。 如果i对应的槽位不是空，且有了占位符，那么该线程跳过这个槽位，处理下一个槽位。 如果以上都是不是，说明这个槽位有一个实际的值。开始同步处理这个。 到这里，都还没有对桶内数据进行转移，只是计算了下标和处理区间，然后一些完成状态判断。同时，如果对应下标内没有数据或已经被占位了，就跳过了。 处理每个桶的行为都是同步的，此处加了synchronized关键字。防止putVal的时候向链表插入数据。 如果这个桶是链表，那么就将这个链表根据length取于拆成两份，取于结果是 0 的放在新表的低位，取于结果是 1 放在新表的高位。 如果这个桶是红黑数，那么也拆成 2 份，方式和链表的方式一样，然后，判断拆分过的树的节点数量，如果数量小于等于 6，改造成链表。反之，继续使用红黑树结构。 到这里，就完成了一个桶从旧表转移到新表的过程。 多线程无锁扩容的关键就是通过 CAS 设置 sizeCtl 与 transferIndex 变量，协调多个线程对 table 数组中的 node 进行迁移。 总结由上面种种可以看到，ConcurrentHashMap对比HashMap和HashTable成功处理线程安全的关键有几个： 关键操作加锁。这个说起来简单，但是优化起来很复杂，添加修改删除元素、扩容，HashTable也是这么做的。 善用锁升级，从无锁到偏向锁，再到自旋锁和重量级锁，一步步加码，保证低并发的时候最大效率。 空间换时间，在面对超大并发，自旋锁大量重复消耗的时候，让多个线程同步进行修改，增加效率。 多余线程参与扩容、计数等工作，能不闲着绝不闲着。 ConcurrentHashMap 六千多行代码，五十三个内部类，都是精华代码，Doug Lea 大神果然厉害。 参考文章： 简书博主莫那一鲁道对ConcurrentHashMap进行了非常细致的总结，本文学习和抄袭了大量文字，更详细的内容建议直接去看原文：ConcurrentHashMap 源码阅读小结","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"MySQL中的事务","slug":"MySQL中的事务","date":"2019-12-17T10:11:12.000Z","updated":"2022-02-20T11:27:13.888Z","comments":true,"path":"2019/12/17/MySQL中的事务/","link":"","permalink":"http://beritra.github.com/2019/12/17/MySQL%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1/","excerpt":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。注意，该书内容是基于MySQL 5.6 及更早的版本，具体的实现方式有可能发生了改变，有些内容我做了注释和补充，但是不保证百分百的完整和正确。","text":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。注意，该书内容是基于MySQL 5.6 及更早的版本，具体的实现方式有可能发生了改变，有些内容我做了注释和补充，但是不保证百分百的完整和正确。 事务（Transaction）是数据库区别于文件系统的重要特性之一。事务会把数据库从一种一致性状态转换为另一种一致状态。在数据库提交工作时，可以确保要么所有修改都已经保存了，要么所有修改都不保存。 InnoDB 存储引擎中的事务完全符合 ACID 的特性。ACID 是以下 4 个词的缩写： 原子性（atomicity） 一致性（consistency） 隔离性（isolation） 持久性（durability） 本文主要关注事务的原子性这一概念，并说明怎么正确使用事务及编写正确的事务应用程序，避免在事务方面养成一些不好的习惯。 认识事务概述事务可由一条非常简单的 SQL 语句组成，也可以由一组复杂的 SQL 语句组成。事务是访问并更新数据库中各种数据项的一个程序执行单元。在事务操作中，要么都做修改，要么都不做，这就是事务的目的，也是事务模型区别于文件系统的重要特征之一。 理论上说，事务有严格的定义，必须同时满足四个特性，即 ACID 特性。但是数据库厂商出于各种目的，并没有严格去满足事务的 ACID 标准。对于 InnoDB 存储引擎而言，其默认的事务隔离级别为READ REPEATABLE 完全遵循和满足事务的 ACID 特性。这里具体介绍事务的 AICD 特性，并给出相关概念。 A（Atomicity）原子性。在计算机系统中，每个人都将原子性视为理所当然。例如在 C 语言中调用SQRT函数，要么返回正确的平方根值，要么返回错误的代码，而不会在不可预知的情况下改变任何的数据结构和参数。如果SQRT函数被多个程序同时调用，一个程序的返回值也不会是其他程序需要计算的平方根。 然而在数据的事务中实现调用操作的原子性，就没那么简单了，例如一个 ATM 机取款的流程： 登录 ATM 机平台，验证密码。 从远程银行的数据库中，取得账户信息。 用户在 ATM 机上输入欲提取的金额。 从远程银行的数据库中，更新账户信息。 ATM 机出款。 用户取钱。 整个取款流程应当视作原子操作，即要么都做，要么不做。不能用户钱未从 ATM 机上取得，但是银行卡上的钱已经被扣除了，相信这是任何人都不能接受的一种情况。而通过事务模型，可以保证操作的原子性。 原子性是指整个数据库事务是不可分割的工作单位。只有使事务中所有的数据库操作都执行成功，才算整个事务成功。任何一个 SQL 语句执行失败，已经执行成功的 SQL 语句也必须撤销，数据库状态应该退回到执行事务前的状态。 C（consistency），一致性。一致性是指事务将数据库从一种状态转变为下一种一致的状态。在事务开始之前和结束之后，数据库的完整性约束没有被破坏。事务是一致性的单位，如果事务中某个动作失败了，系统可以自动撤销事务，返回初始化的状态。 I（isolation），隔离性。隔离性还有其他称呼，如并发控制（consurrency control）、可串行化（serializability）、锁（locking）等。事务的隔离性要求每个读写事务的对象对其他事务的操作对象能相互分离，即该事务提交前对其他事务都不可见，通常这使用锁来实现。当前数据库系统中都提供了一种粒度锁（granular lock）的策略，允许事务仅锁住一个实体对象的子集，以此来提高事务之间的并发度。 D（durability），持久性。事务一旦提交，其结果就是永久性的。即使发生宕机等故障，数据库也能将数据恢复。需要注意的是，只能从事务本身的角度来保证结果的永久性。例如，在事务提交后，所有的变化都是永久的。即使当数据库发生崩溃而需要恢复数据时，也能保证恢复后提交的数据都不会丢失。但若不是数据库本身的问题，那么数据有可能丢失。因此保证事务系统的高可靠性（High Reliability），而不是高可用性（High Availability）。对于高可用性的实现，事务本身并不能保证，需要一些系统共同配合来完成。 分类从事务理论的角度来说，可以把事务分为以下几种类型： 扁平事务（Flat Transactions） 带有保存点的扁平事务（Flat Transaction with Savepoints） 链事务（Chained Transactions） 嵌套事务（Nested Transaction） 分布式事务（Distributed Transactions） 扁平事务（Flat Transaction）是事务类型中最简单的一种，但在实际生产环境中，可能是最频繁的事务。在扁平事务中，所有操作都处于同一层次，其由BEGIN WORK开始，由COMMIT WORK或ROLLBACK WORK结束，其间的操作是原子的，要么都执行，要么都回滚。因此扁平事务是应用程序成为原子操作的基本组成模块。 下图显示了扁平事务的三种不同结果： 扁平事务的主要限制是不能提交或者回滚事务中的某一部分，或者分步骤提交。下面给出了一个扁平事务不足以支持的例子。例如用户在旅行网站上进行自己的旅行度假计划。用户摄像从杭州道意大利的佛罗伦萨，这两个城市之间没有直达的班级，需要用户预定并转乘航班，或者需要搭火车等待。用户旅行度假的事务为： 1BEGIN WROK2S1：预定杭州到上海的高铁3S2：上海浦东国际机场坐飞机，预定区米兰的航班4S3：在米兰坐火车前往佛罗伦萨，预定去佛罗伦萨的火车 但是当用户执行到 S3 的时候，发现由于飞机到达米兰的时间台湾，已经没有当天的火车。这时用户希望在米兰当地住一晚，第二天出发去佛罗伦萨。这时如果事务为扁平事务，则需要回滚之前 S1、S2、S3 的三个操作，这个代价就显得有点大。因为当再次进行该事务的时候，S1、S2 的执行计划是不变的。也就是说，如果有支持计划的回滚操作，那么就不需要终止整个事务。因此就出现了带有保存点的扁平事务。 带有保存点的扁平事务（Flat Transaction with Savepoint），除了支持扁平事务支持的操作外，允许在事务执行过程中回滚到同一事务中较早的一个状态。这是因为某霞事务可能在执行过程中出现的错误并不会导致所有的操作都无效，放弃整个事务不合乎要求，开销页太大。保存点（Savepoint）用来通知系统应该记住事务当前的状态，以便当以后发生错误时，事务能回到保存点当时的状态。对于扁平的事务来说，其隐式的设置了一个保存点。而在整个事务中，只有一个保存点，因此，回滚只能回滚到事务开始时的状态。保存点用SAVE WORK函数来建立，通知系统记录当前的处理状态。当出现问题的时候，保存点能用作内部的重启动点，根据应用逻辑，决定是回到最近一个保存点还是其他更早的保存点。下图显示了在事务中使用了保存点： 上图显示了如何在事务中使用保存点。灰色背景部分的操作表示由ROLLBACK WORK而导致的部分回滚，实际并没有执行的操作。当用 BEGIN WORK 开启一个事务时，隐式地包含一个保存点，当事务通过ROLLBACK WORK：2 发出部分回滚命令的时候，事务回滚到保存点 2，接着一次执行，并再次执行到ROLLBACK WORK：7，知道最后的COMMIT WORK操作，这时表示事务结束，除灰色阴影部分的操作外，其他的操作都已经执行，并且提交。 另一点需要注意的是，保存点在事务内部是递增的，这从上图中也可以看出。有人可能会想，返回保存点 2 之后，下一个保存点可以是 3，因为之前的动作都终止了。然而新的保存点编号为 5，这意味着ROLLBACK不影响保存点的计数，并且单调递增的编号能保持事务执行的整个历史过程，包括在执行过程中想法的改变。 此外，当事务通过ROLLBACK WORK：2 命令部分回滚命令时，要记住事务并没有完全被回滚，只是回滚到了保存点 2 而已。这代表当前事务还是活跃的，如果想要完全回滚事务，还需要再执行命令ROLLBACK WORK。 链事务（Chained Transaction）可以视为是保存点模式的一种变种。带有保存点的扁平事务，当事务发生崩溃的时候，所有保存点都将消失，因为其保存点是易失的（volatile），而非持久的（persistent）。这意味着当进行恢复时，事务需要从开始处重新执行，而不能从最近的一个保存点继续执行。 链事务的思想是：在提交一个事务时，释放不需要的数据对象，将必要的数据处理上下文隐式地传给下一个要开始的事务。注意，提交事务操作和开始下一个事务操作将合并为一个原子操作。这意味着下一个事务将看到上一个事务的结果，就好像在一个事务中进行的一样。下图显示了链事务的工作方式： 链事务与带有保存点的扁平事务不同的是，带有保存点的扁平事务能回滚到任意正确的保存点。而链事务中的回滚仅限于当前事务，即只能恢复到最近的一个保存点。对于锁的处理，两者也不相同。链事务在执行COMMIT后即释放了当前事务所持有的锁， 而带有保存点的扁平事务不影响迄今为止所持有的锁。 嵌套事务（Nested Transaction）是一个层次结构框架。由一个顶层事务（top-level transaction）控制着各个层次的事务。顶层事务之下嵌套的事务被称为子事务（subtransaction），其控制着每一个局部的变量。嵌套事务的层次结构如图所示： 下面是 Moss 对嵌套事务的定义： 嵌套事务是由若干事务组成的一颗树，子树既可以是嵌套事务，也可以是扁平事务。 处在叶节点的事务是扁平事务。但是每个子事务从根到叶节点的距离可以是不同的。 处于根节点的事务成为顶层事务，其他事物称为子事务。事务的前驱（predecessor）称为父事务（parent），事务的下一层成为儿子事务（child）。 子事务既可以提交也可以回滚。但是它的提交操作并不会马上生效，除非其父事务已经提交。由此可以推论出，任何子事务都在顶层事务提交后才真正的提交。 树中任何一个事务的回滚都会引起它所有的子事务一同回滚，故子事务仅保留 A、C、I 特性，不具有 D 的特性。 根据 Moss 的理论，实际的工作是交由叶子节点来完成的，即只有叶子节点的事务才能访问数据库、发送消息、获取其他类型的资源。而高层的事务仅负责逻辑控制，决定何时调用相关的子事务。即使一个系统不支持嵌套事务，用户也可以通过保存点技术来模拟嵌套事务，如图所示： 从图中也可以发现，在恢复时采用保存点技术比嵌套查询有更大的灵活性。例如在完成 Tk3 这个事务的时候，可以回滚到保存点 S2 的状态。而在嵌套查询的层次结构中，这是不被允许的。 但是用保存点技术来模拟嵌套事务在锁的持有方面还是与嵌套查询有些区别。当通过保存点技术来模拟嵌套事务时，用户无法选择哪些锁需要被子事务继承，哪些需要被父事务保留。就是说：无论有多少个保存点，所有被锁住的对象都可以被得到和访问。而在查询嵌套中，不同的子事务在数据库对象上持有的锁是不同的。 例如有一个父事务 P1，其持有对象 X 和 Y 的排它锁，现在要开始调用子事务 p11，那么父事务 P1 可以不传递锁，也可以传递所有的锁，也可以只传递一个排他锁。如果子事务 P11 中还有持有对象 X、Y、Z 的排他锁。如果这时又再次调用了一个子事务 P12，那么它可以传递那里已经持有的锁。 然而，如果系统支持在嵌套事务中并行地执行各个事务，在这种情况下，采用保存点的扁平事务来模拟嵌套事务就不切实际了。者从另一个方面反映出，想要实现事务键的并行性，需要真正支持的嵌套事务。 分布式事务（Distributed Transaction）通常是在一个分布式环境下运行的扁平事务，因此需要根据数据所在位置访问网络中的不同节点。 假设一个用户在 ATM 机上进行银行的转账操作，例如持卡人从招商银行的储蓄卡转账 10000 元到工商银行的储蓄卡。在这汇总情况下，可以将 ATM 机视为节点 A，招商银行的后台数据库视为节点 B，工商银行的后台数据库视为 C，这个转账操作可以分解为以下的步骤： 节点 A 发出转账命令。 节点 B 执行储蓄卡中的余额值减去 10000。 节点 C 执行储蓄卡中的余额值加上 10000。 节点 A 通知用户完成或者节点 A 通知用户操作失败。 这里需要使用分布式事务，因为节点 A 不能通过调用一台数据库就完成任务，其需要访问网络中两个节点的数据库，而在每个节点中的数据库执行的事务操作都又是扁平的。对于分布式事务，其同样需要满足 ACID 特性，要么都发生，要么都失效。对于上述例子，如果 2、3 步中任何一个操作失败，都会导致整个分布式事务回滚。 对于 InnoDB 存储引擎来说，支持扁平事务、带有保存点的事务、链事务、分布式事务。对于嵌套事务，其并不原生支持，因此，对于有并行事务需求的用户来说，MySQL 数据库或 InnoDB 存储引擎就显得无能为力了。然而用户仍可以通过带有保存点的事务来模拟串行的嵌套事务。 事务的实现事务的隔离性由锁来实现。原子性、一致性、持久性通过数据库的 redo log 或 undo log 来完成。redo log 又称为重做日志，用来保证事务的原子性和持久性。undo log 用来保证事务的一致性。 有人可能会认为 undo 是 redo 的逆过程，其实不然。redo 和 undo 的作用都可以视为是一种恢复操作，redo 恢复提交事务修改的页操作，而 undo 回滚行记录到某个特定版本。因此两者记录的内容不同，redo 通常是物理日志，记录的是页的屋里操作改动，undo 是逻辑日志，根据每行记录进行记录。 redo重做日志用来实现事务的持久性，即事务 ACID 中的 D。其由两部分组成：一是内存中的重做日志缓冲（redo log buffer），其是易失的；二是重做日志文件（redo log file），其是持久的。 InnoDB 是事务的存储引擎，通过Force Log at Commit机制实现事务的持久性，即当事务提交（COMMIT）的时候，必须先把事务的所有日志写入到重做日志文件进行持久化，等事务的提交操作完成之后才算完成。这里的日志是指重做日志，在 InnoDB 存储引擎中，由两部分组成，即 redo log 和 undo log。redo log 用来保证事务的持久性，undo log 用来帮助事务回滚以及 MVCC 的功能。redo log 基本上都是顺序写的，在数据库运行时不需要对 redo log 的文件进行读取操作。而 undo log 是需要进行随机读写的。 为了确保每次日志都写入重做日志文件，在每次日志缓冲写入重做日志文件之后，InnoDB 存储引擎都需要调用一次fsync操作。由于重做日志文件打开并没有使用O_DIRECT选项，因此重做日志缓冲先写入文件系统缓存。由于fsync的效率取决于磁盘性能，因此磁盘的性能决定了事务提交的性能，也就是数据库的性能。 InnoDB 存储引擎允许用户手动设置非持久性的情况发生，因此提高数据库的性能。当事务提交的时候，日志不写入重做日志文件，而是等待一个时间周期后再执行fsync操作。由于并非强制在事务提交的时候进行一次fync操作，显然可以提高数据库性能。但是在数据库发生宕机的时候，由于部分日志未发刷新到磁盘，因此会丢失最后一段时间的事务。 参数inndob_flush_log_at_trx_commit用来控制重做日志刷新到磁盘的策略，默认值是 1，表示每次都会调用fsync操作。这个参数还可以设置为 0 或者 2。0 表示事务提交时不进行写入重做日志操作，这个操作仅在 master thread 中完成，而在 master thread 中每一秒会进行一次fsync操作。2 表示事务提交时将重做日志写入重做日志文件，但仅写入文件系统的缓存中，不进行 fsync 操作。在这个设置下，当 MySQL 数据库发生宕机而操作系统不发生宕机时，并不会导致事务的丢失。但是当操作系统宕机时，重启数据库后会丢失未从文件系统缓存刷新到重做日志文件那部分事务。 下图是模拟插入 50 万行数据，参数为 1 是每插入一条就显式执行一次 COMMIT 操作，跟其他参数设置之间的数据库效率对比： 虽然用户可以通过设置参数innodb_flush_log_at_trx_commit为 0 或 2 来提高事务提交的性能，但是需要牢记的事，这种设置方法丧失了事务的 ACID 特性。而针对上述存储过程，为了提高事务的提交性能，应该在将 50 万行数据插入表后进行一次 COMMIT 操作，而不是在每插入一条记录之后进行一次 COMMIT 操作。 在 MySQL 数据库中还有一种二进制日志（binlog），其用来进行 POINT-TIME（PIT）的恢复及主从复制（Replication）环境的建立。从表面上看其和重做日志非常相似，都是记录了对于数据库操作的日志。然而，从本质上来看，二者有着非常大的不同。 首先，重做日志是在 InnoDB 存储引擎层产生，而二进制日志是在 MySQL 数据库的上层产生的，并且二进制日志不仅仅对于 InnoDB 引擎，MySQL 数据库中的任何存储引擎对于数据库的更改都会产生二进制日志。 其次，两种日志记录的内容形式不同。MySQL 数据库上层的二进制日志是一种逻辑日志，其记录的是对应的 SQL 语句（注，这里不严谨，新版本的 MySQL 不完全是对应的 SQL）。而 InnoDB 存储引擎的重做日志在事务进行中不断地被写入，这表现为日志并不是随事务提交的顺序进行写入的。 undo重做日志记录的事务的行为，可以很好地通过其对页进行“重做”操作。但是事务有时候还会进行回滚操作，这时就需要 undo。因此在对数据库在进行修改的时候，InnoDB 存储引擎不但会产生 redo，还会产生一定量的 undo。这样如果用户执行的事务或语句由于某种原因失败了，又或者用户用一条ROLLBACK语句请求回滚，就可以利用这些 undo 信息将数据回滚到修改之前的样子。 redo 存放在重做日志文件中，与 redo 不同，undo 存放在数据库内部的一个特殊段（segment）中，这个段称为 undo 段（undo segment）。undo 段位于共享表空间内。可以通过py_innodb_page_info.py工具来查看当前共享表空间中 undo 的数量。 用户通常对 undo 有这样的误解：undo 用于将数据库物理的恢复到执行语句或事务之前的样子——但事实并非如此。undo 是逻辑日志，因此只将数据库逻辑的恢复到原来的样子。所有修改都被逻辑地取消掉了，但是数据结构和页本身在回滚之后可能大不相同。这是因为在多用户并发系统中，可能会有数十、数百甚至数千个并发事务。数据库的主要任务就是协调堆数据记录的并发访问。比如，一个事务在修改当前一个页中某几条记录，同时还有别的事务在对同一个页中另几条记录进行修改。因此，不能将一个页回滚到事务刚开始的样子，因为这样会影响其他事务正在进行的工作。 例如，用户执行了一个 INSERT 10 万条记录的事务，这个事务会导致分配一个新的段，即表空间会增大。在用户执行ROLLBACK时，会将插入的事务进行回滚，但是表空间的大小不会因此收缩。因此，当 InnoDB 存储引擎回滚时，它实际上做的是与先前相反的工作。对于每个 INSERT ，InnoDB 存储引擎会完成一个 DELETE；对于每个 DELETE，InnoDB 存储引擎会执行一个 INSERT；对于每个 UPDATE，InnoDB 引擎会执行一个相反的 UPDATE，将修改前的行放进去。 除了回滚操作，undo 的另一个作用是 MVCC，即在 InnoDB 存储引擎中 MVCC 的实现是通过 undo 来完成。当用户读取一行记录时，若该记录已经被其他事务占用，当前事务可以通过 undo 读取之前的行版本信息，以此实现非锁定读取。 最后也是最重要的一点是，undo log 会产生 redo log，也就是 undo log 的产生会伴随着 redo log 的产生，这是因为 undo log 也需要持久性的保护。 prugedelete 和 update 操作可能不会直接删除原有的数据。假设有如下表 t： 1CREATE TABLE t(2a INT，3b VARCHAR(32)，4PRIMARY KEY(a)，5KEY(n)6)ENGINE&#x3D;Innodb; 对其执行 如下 SQL 语句： 1DELETE FROM t WHERE a&#x3D;1; 表 t 上列 a 有聚集索引，列 b 上有辅助索引。对于上述 delete 操作，仅仅是将主键列等于 1 的记录delete flag设置为 1，记录并没有被删除，即记录还存在于 B+ 树中。其次，对辅助索引上 a 等于 1，b等于 1 的记录同样没有做任何处理，甚至没有产生 undo log。而真正删除这样记录的操作其实被“延时”了，最终在 purge 操作中完成。 purge 用于最终完成 delete 和 update 操作。这样设计是因为 InnoDB 存储引擎支持 MVCC，所以记录不能在事务提交时立即进行处理。这时其他事务可能正在引用这行，故 InnoDB 存储引擎需要保存记录之前的版本。而是否可以删除改条记录通过 purge 来进行判断。若改行记录已经不被其他任何事务引用，那么就可以进行真正的 delete 操作。可见，purge 操作是清理之前的 delete 和 update 操作，故上述操作“最终”完成。而实际执行的操作为 delete 操作，清理之前行记录的版本。 group commit若事务为非只读事务，则每次事务提交时需要进行一次fsync操作，以保证重做日志都已经写入磁盘。当数据库发生宕机时，可以通过重做日志进行恢复。虽然固态硬盘的出现提高了磁盘的性能，然而磁盘的fsync性能是有限的。为了提高磁盘fsync的效率，当前数据库都提供了group commit的功能，即一次fsync可以刷新确保多个事务日志被写入文件。对于InnoDB 存储引擎来说，事务提交时会进行两个阶段的操作： 修改内存中事务对应的信息，并且将日志写入重做日志缓冲。 调用fsync将确保日志都从重做日志缓冲个写入磁盘。 步骤 2 相对于步骤 1 是一个较慢的过程，这是因为存储引擎需要与磁盘打交道。但当有事务进行这个过程时，其他事务可以进行步骤 1 的操作，正在提交的事务完成提交操作后，再次执行步骤 2 时，就可以将多个事务的重做日志通过一次fsync刷新到磁盘，这样就大大减少了磁盘的压力，从而提高了数据库的整体性能。对于写入或更新较为频繁的操作，group commit的效果尤为明显。 事务控制语句在 MySQL 命令行的默认设置下，事务都是自动提交（auto commit）的，即执行 SQL 语句之后就会马上执行 COMMIT 操作。因此要显式的开启一个事务需要使用命令BEGIN、START TRANSACTION，或者执行命令SET AUTOCOMMIT=0，禁用当前会话的自动提交。每个数据库厂商自动提交的设置都不相同，每个 DBA 或开发人员需要非常明白这一点，这对之后的 SQL 编程会有非凡的意义，因此用户不能以之前的经验来判断 MySQL 数据库的运行方式。在具体介绍其含义之前，先看看有哪些事务控制语句： START TRANSACTION|BEGIN：显式地开启一个事务。 COMMIT：提交事务，等价于COMMIT WORK。 ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。等价于ROLLBACK WORK SAVEPOINT identifier：SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个保存点。 ROLLBACK TO[SAVEPOINT] identifier：这个语句与SAVEPOINT命令一起使用。可以把事务回滚到标记点，而不回滚在此标记点之前的任何工作。例如可以发出两条UPDATE语句，后面跟一个SAVEPOINT，然后又是两条DELETE语句。如果执行DELETE语句期间出现了某种异常情况，并且捕获到了这个异常，同时发出了ROLLBACK TO SAVEPOINT命令，事务就会回滚到指定的SAVEPOINT，撤销DELETE完成的所有工作，而UPDATE语句完成的工作不受影响。 SET TRANSACTION：这个语句用来设置事物的隔离级别。InnoDB 存储引擎提供的事务隔离级别有：READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ、SERIALISZABLE。 START TRANSACTION、BEGIN语句都可以在 MySQL 命令行下显式地开启一个事务。但是在存储过程中，MySQL数据库的分析器会自动将BEGIN识别为BEGIN END，因此在存储过程中只能使用START TRANSACTION语句来开启一个事务。 COMMIT和COMMIT WORK语句基本是一致的，都是用来提交事务。不同之处在于COMMIT WORK用来控制事务结束后的行为是CHAIN还是RELEASE的。如果是CHAIN方式，那么事务就变成了链事务。 InnoDB 存储引擎中的事务都是原子的，这说明下述两种情况：构成事务的每条语句都会提交（成为永久），或者所有语句都回滚。这种保护还延伸到单个的语句。一套语句要么完全成功，要么完全回滚（注意，这里说的是语句回滚）。因此一条语句失败并抛出异常时，并不会导致先前已经执行的语句自动回滚。所有的执行都会得到保留，必须由用户自己来决定是否对其进行提交或回滚的操作。 另一个容易犯的错误是ROLLBACK TO SAVEPOINT，虽然有ROLLBACK，但其实并不真正的结束一个事务，因此即使执行了ROLLBACK TO SAVEPOINT，之后也需要显式地运行COMMIT或ROLLBACK命令。 隐式提交的 SQL 语句以下 SQL 语句会产生一个隐式的提交操作，即执行完这些语句后，会有一个隐式的COMMIT操作。 DDL 语句：ALTER DATABASE...UPGRADE DATA DIRECTORY NAME，ALTER EVENT，ALTER PROCEDURE，ALTER TABLE，ALTER VIEW，CREATE DATABASE，CREATE ENENT，CREATE INDEX，CREATE PROCEDURE，CREATE TABLE，CREATE TRIGGER，CREATE VIEW，DROP DATABASE，DROP EVENT，DROP INDEX，DROP PROCEDURE，DROP TABLE，DROP TRIGGER，DROP VIEW，RENAME TABLE，TRAUNCATE TABLE 用来隐式地修改 MySQL 架构的操作：CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD 管理语句：ANALYZE TABLE、CACHE INDEX、CHECK TABLE、LOAD INDEX INTO CACHE、OPTIMEIZE TABLE、REPAIR TABLE、 另外需要注意的是，TRUNCATE TABLE语句是 DDL，因此虽然和对整张表执行DELETE的结果是一样的，但它不能回滚的。 对于事务操作的统计对于 InnoDB 存储引擎是支持事务的，因此 InnoDB 存储引擎的应用需要在考虑每秒请求数（Question Per Second，QPS）的同时，应该关注每秒事务处理的能力（Transaction Per Second，TPS）。 计算 TPS 的方法是（com_commit+com_rollback）/time。但是利用这种方法进行计算的前提是：所有的事务必须都是显式的提交，如果存在隐式地提交和回滚（默认 autocommit=1），不会计算到com_commit和com_rollback中。 事务的隔离级别SQL 标准定义的四个隔离级别： READ UNCOMMITTED READ COMMITTED REPEATABLE READ SERIALIZABLE InnoDB 存储引擎默认支持的隔离级别是 REPEATABLE READ，但与标准 SQL 不同的是，InnoDB 存储引擎在 REPEATABLE READ 事务隔离级别下，使用 Next-Key Lock 锁的算法，因此避免幻读的产生。所以说，InnoDB 存储引擎在默认的 REPEATABLE READ的事务隔离级别下，已经能完全保证事务的隔离性要求，即达到 SQL 标准的 SERIALIZABLE 隔离级别。 隔离级别越低，事务请求的锁越少或保持锁的时间就越短，这也是为什么大多数数据库的默认隔离级别是 READ COMMITTED。 大部分用户质疑 SERIALIZABLE 隔离级别带来的性能问题，但是两者的开销几乎是一样的，甚至 SERIALIZABLE 可能更优！因此在 InnoDB 存储引擎中选择 REPEATABLE READ 的事务隔离级别并不会有任何性能的损失。同样的，即使使用 READ COMMITTED 的隔离级别，用户也不会得到性能的大幅提升。 在 InnoDB 存储引擎中，可以使用以下命令来设置当前会话或全局的事务隔离级别： 1SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL&#123;2READ UNCOMMITTED3READ COMMITTED4REPEATABLE READ5SERIALIZABLE6&#125; 如果想在 MySQL 数据库启动时就设置事务的默认隔离级别，那就需要修改 MySQL 的配置文件，在[mysqld]中添加如下行： 1[mysqld]2transaction-isolation &#x3D; READ-COMMITTED 查看当前会话的事务隔离级别，可以使用： 1SELECT @@transaction_isolation\\G; 查看全局的事务隔离级别，可以使用： 1SELECT @@global.transaction_isolation\\G; 注意，MySQL 8.0 之前查询当前事务隔离级别的语句是select @@tx_isolation，之后是上面用的select @@transaction_isolation 在 SERIALIABLE 的隔离级别下，InnoDB 引擎会对每个 SELECT 语句后自动加上一个LOCK IN SHARE MODE，即为每个读取操作加一个共享锁。因此在这个事务隔离级别下，读占用了锁，对一致性的非锁定读不再予以支持。这时，事务隔离级别 SERIALIZABLE 符合数据库理论上的要求，即事务是 well-formed 的，并且是 two-phrased 的。 分布式事务MySQL 数据库分布式事务InnoDB 存储引擎提供了对 XA 事务的支持，并通过 XA 事务来支持分布式事务的实现。分布式事务指的是允许多个独立的事务资源（transaction resources）参与到一个全局事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都会滚，这对事务原有的 ACID 要求又有了提高。另外，在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE。 XA 事务允许你不同数据库之间的分布式事务，如一台数据库是 MySQL 数据库的，另一台是 Oracle 的，有可能还有一台是 SQL SERVER 数据库的，只要参与到全局事务中的每个节点都支持 XA 事务。 XA 事务由一个或多个资源管理器（Resource MAanagers）、一个事务管理器（Transaction Manager）以及一个应用程序（Application Program）组成。 资源管理器：提供访问事务资源的方法。通常一个数据库就是一个资源管理器。 事务管理器：协调参与全局事务中的各个事务。需要和参与全局事务的所有资源管理器进行通信。 应用程序：定义事务的边界，指定全局事务中的操作。 在 MySQL 数据库的分布式事务中，资源管理器就是 MySQL 数据库，事务管理器为连接 MySQL 服务器的客户端。下图显示了一个分布式事务的模型： 分布式事务使用两段式提交（two-phase commit）的方式。在第一阶段，所有参与全局事务的节点都开始准备（PREPARE），告诉事务管理器他们准备号提交了。在第二阶段，事务管理器告诉资源管理器执行ROLLBACK还是COMMIT。如果任何一个节点显示不能提交，则所有的节点都被告知需要回滚。可见与本地事务不同的是，分布式事务需要多一次 PREPARE 的操作，待收到所有节点的同意信息后，再进行COMMIT或是ROLLBACK操作。 内部 XA 事务之前讨论的分布式事务是外部事务，即资源管理器是 MySQL 本身。在 MySQL 数据库中还存在另一种分布式事务，其在存储引擎与插件之间，又或者在存储引擎与存储引擎之间，称之为内部 XA 事务。】 最常见的内部 XA 事务存在于 binlog 与 InnoDB 存储引擎之间。由于复制的需要，因此目前绝大多数的数据库都开启了 binlog 功能。在事务提交时，先写二进制日志，再写 InnoDB 存储引擎的重做日志。对上述两个操作的要求也是原子的。即二进制日志和重做日志必须同时写入。若 二进制日志先写了，而在写入 InnoDB 存储引擎时发生了宕机，那么 slave 可能会接收到 master 传过去的二进制日志并执行，最终导致了主从不一致的情况。 为了解决这个问题，MySQL 数据库在 binlog 与 InnoDB 存储引擎之间采用 XA 事务。当事务提交的时候，InnoDB 存储引擎回西安做一个 PREPARE 操作，将事务的 xid 写入，接着二进制日志写入，入股偶在 InnoDB 存储引擎提交前，MySQL 数据库宕机了，那么 MySQL 数据库在重启后会先检查准备的 UXID 事务是否已经提交，若没有，则在存储引擎层再进行一次提交操作。如下图所示： 不好的事务习惯在循环中提交有的开发人员习惯再循环中进行事务的提交，比如可能常写的一个存储过程： 这个里面的COMMIT命令其实并不关键。因为 InnoDB 存储引擎默认自动提交，这也是经常被开发人员忽视的问题： 其实无论上面哪个存储过程都存在一个问题，当发生错误的时候，数据库会停留在一个位置的位置。例如，用户需要插入 10000 条记录，但是当插入 5000 条时，发生了错误，这时前 5000 条记录都已经存放在数据库中，那应该怎么处理呐？另外一个问题性能问题，上面两个存储过程都不会比下面的存储过程更快，因为下面的存储过程将所有的INSERT都放在了一个事务中： 第三种方法要快的多！这是因为每次提交都要写一次重做日志，存储过程 load1 和 load2 实际上写了 10000 次重做日志文件，而对存储过程 load3 来说，实际上只写了 1 次。 所以无论从何种角度出发，都不应该在一个循环中反复进行提交操作，无论是显式还是隐式的提交。 使用自动提交使用自动提交不是一个好习惯，因为这会使初级 DBA 容易犯错，另外还可能是一些开发人员产生错误的理解。在编写程序开发时，最好把事务的控制权限交给开发人员，即在程序端进行事务的开始和结束。同时，开发人员必须了解是自动提交可能带来的问题。 使用自动回滚InnoDB 存储引擎支持通过定义一个 HANDLER 来进行自动事务的回滚操作，如在一个存储过程中发生了错误会自动对其进行回滚操作。因此很多开发人员喜欢在应用程序的存储过程中使用自动回滚操作。 在存储过程中使用自动回滚很容易丢失错误信息，所以应该在程序中控制而不是数据库中。在程序中控制事务的好处是用户可以得知发生错误的原因，然后根据发生的原因进一步调试程序。 长事务长事务（Loing-Lived Transaction），顾名思义，就是执行时间较长的事务，比如，对于银行系统中的数据库，每过一个阶段可能需要更新对应账户的利息。如果对应账户的数量非常大，例如对有 1 亿用户的表进行更新操作，可能需要非常长的时间来完成，可能需要一个小时，也可能需要 4、5 个小时，这取决于数据库的硬件配置。然而，由于事务的 ACID 特性，这个操作被封装在一个事务中完成。这就产生了一个问题，在执行过程中，当数据库、操作系统或者硬件发生问题的时候，重新开始事务的代价变得不可接受。数据库需要回滚所有已经发生的变化，而这个过程可能比产生这些变化的时间还要长。因此，对于长事务的问题，有时候可以通过转化为小批量（mini batch）的事务来进行处理。当事务发生错误时，只需要回滚一部分数据，然后接着上次已完成的事务继续进行。 由一个小地方还需要注意，要保证事务在处理工程中，没有其他的事务来更新表中的数据，需要人工加一个共享锁。 MVCC关于 MVCC，感觉博客中好多解释是冲突的，干脆去看官网，以下内容直接粘贴的官网文档： InnoDB多版本InnoDB是一个 多版本的存储引擎：它保留有关已更改行的旧版本的信息，以支持诸如并发和回滚之类的事务功能 。此信息存储在表空间中的数据结构中，该数据结构称为 回滚段（出现在 Oracle 中类似的数据结构之后）。InnoDB 使用回滚段中的信息来执行事务回滚中所需的撤消操作。它还使用该信息来构建行的早期版本，以实现一致的读取。 在内部的实现中，InnoDB向数据库中存储的每一行添加了三个字段。包括： 一个 6 个字节的DB_TRX_ID字段，表示插入或更新该行的最后一个事务的事务标识符。同样，在内部用更新来实现删除，在更新操作的时候，行中的软删除标记位被标记为删除。 还包含一个 7 字节的 DB_ROLL_PTR字段，称为滚动指针。回滚指针指向写入回滚段的撤消日志（undo log）记录。如果行已更新，则撤消日志记录将保存了更新之前的全部信息，方便进行重建。 一个 6 字节的DB_ROW_ID字段包含一个行 ID，该行 ID 随着插入新行而单调增加。如果 InnoDB自动生成聚集索引，该索引包含行 ID 值。否则，该 DB_ROW_ID列不会出现在任何索引中。 回滚段中的撤消日志分为插入撤销和更新撤消日志。插入撤消日志仅在事务回滚时才需要，并且在事务提交后可以立即将其丢弃。更新撤消日志也用于一致的读取中，但是只有在其没有了分配了快照的事务后，才可以被删除。（因为一致性读取可能需要使用更新撤消日志中的信息来构建数据库行的早期版本。） 定期提交您的事务，包括仅用于一致读取的事务。否则， InnoDB将无法丢弃更新撤消日志中的数据，并且回滚段可能会变得太大而填满表空间。 回滚段中的撤消日志记录的物理大小通常小于相应的插入或更新的行。您可以使用此信息来计算回滚段所需的空间。 在InnoDB多版本方案中，当您使用 SQL 语句删除行时，并不会立即将其从数据库中物理删除。InnoDB仅在丢弃那些删除操作的撤销记录时，才物理删除相应的行及其索引记录。此删除操作称为purge，它非常快，通常花费与执行删除操作的SQL语句相同的时间顺序。 如果您以大致相同的速率在表中以较小的批次插入和删除行，则由于所有“ 死 ”行，清除线程可能开始滞后并且表可能会变得越来越大 ，从而使所有内容都受磁盘约束而且非常慢。在这种情况下，请限制新行的操作，并通过调整innodb_max_purge_lag系统变量来向清除线程分配更多资源 。有关更多信息，请参见第15.14节“ InnoDB启动选项和系统变量”。 多版本索引和二级索引InnoDB多版本并发控制（MVCC）对二级索引的处理方式不同于聚簇索引。聚簇索引中的记录将就地更新，其隐藏的系统列指向撤消日志条目，可以从中重建记录的早期版本。与聚簇索引记录不同，辅助索引记录不包含隐藏的系统列，也不会就地更新。 更新二级索引列时，将对旧的二级索引记录进行删除标记，将新记录插入，并最终清除带有删除标记的记录。当二级索引记录被删除标记或二级索引页被较新的事务更新时，InnoDB在聚集索引中查找数据库记录。在聚集索引中，DB_TRX_ID检查记录的记录，如果在启动读取事务后修改了记录，则从撤消日志中检索记录的正确版本。 如果二级索引记录被标记为删除，或二级索引页被更新的事务更新， 则不使用覆盖索引技术。InnoDB在聚集索引中查找记录，而不是从索引结构中返回值。 但是，如果启用了 索引条件下推（ICP）优化，并且WHERE只能使用索引中的字段来评估部分条件，则 MySQL 服务器仍会将WHERE条件的这一部分下推到存储引擎，并使用索引。如果找不到匹配的记录，则避免聚集索引查找。如果找到了匹配的记录，即使在删除标记的记录中，也要在 InnoDB聚簇索引中查找记录。 小结我们了解了 InnoDB 存储引擎管理事务的许多方面。了解了事务如何工作以及如何使用。 事务必须遵循 ACID 特性，即 Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）和 Durability（持久性）。隔离性通过锁来完成；原子性、一致性、隔离性通过 redo 和 undo 来完成。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"事务","slug":"事务","permalink":"http://beritra.github.com/tags/%E4%BA%8B%E5%8A%A1/"}]},{"title":"离线环境下安装高可用FastDFS","slug":"FastDFS安装文档","date":"2019-12-01T04:12:12.000Z","updated":"2019-12-15T15:44:34.048Z","comments":true,"path":"2019/12/01/FastDFS安装文档/","link":"","permalink":"http://beritra.github.com/2019/12/01/FastDFS%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3/","excerpt":"记录一次离线环境下安装高可用 FastDFS 的过程","text":"记录一次离线环境下安装高可用 FastDFS 的过程 FastDFS安装文档需准备2台机器（安装Centos7操作系统），A,B A：安装tracker，storage（group1），nginx B：安装tracker，storage（group2），nginx 默认选择A服务器的tracker作为leader，tracker为从 1.安装libfastcommon包 解压libfastcommon-1.0.40.tar.gz cd libfastcommon-1.0.40 | ./make.sh | ./make.sh install ibfastcommon.so 默认安装到了/usr/lib64/libfastcommon.so，但是FastDFS主程序设置的lib目录是/usr/local/lib，所以此处重新设置软链接： ​ ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.so 2.安装FastDFS 解压fastdfs-5.11 cd fastdfs-5.11 | ./make.sh | ./make.sh install ln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.so cd /etc/fdfs 对三个配置文件进行拷贝,备用 cp client.conf.sample client.conf cp storage.conf.sample storage.conf cp tracker.conf.sample tracker.conf 进入 fastdfs-5.11/conf目录,复制以下文件，http.conf和mime.types为了跟nginx模块整合使用，client.conf是fastdfs的客户端测试文件 cp http.conf /etc/fdfs/ cp mime.types /etc/fdfs/ cp client.conf /etc/fdfs/ 3.安装tracker 新建tracker的目录数据文件和日志文件 mkdir /home/face/FastDFS/tracker 编辑tracker配置文件/etc/fdfs/tracker.conf,主要配置参数如下： port=22122 base_path= /home/face/FastDFS/tracker work_thread=4 #最好和cpu核数保持一致 store_lookup=0 #选择上传文件模式 0代表group轮询 1指定特定group 2选择空间最大的group 通过命令启动tracker: systemctl start fdfs_trackerd 查看/home/face/fastDFS/tracker目录下的data和logs目录，看是否启动成功 通过netstat命令查看端口监听情况：netstat -unltp|grep fdfs 检测22122 4.安装storage 新建storage的目录数据文件和日志文件 mkdir /home/face/FastDFS/storage | mkdir /home/face/FastDFS/storage/data 编辑storage配置文件/etc/fdfs/storage.conf，主要配置参数如下： A机器：group_name=group1 B机器：group_name=group2 port=23000 base_path=/home/face/FastDFS/storage store_path_count=1 store_path0=/home/face/FastDFS/storage/data tracker_server=A机器ip:22122 tracker_server=B机器ip:22122 通过命令启动storage: systemctl start fdfs_storaged 查看/home/face/fastDFS/storage目录下的data和logs目录，看是否启动成功 看storage服务器是否已经登记到 tracker服务器，运行以下命令：/usr/bin/fdfs_monitor /etc/fdfs/storage.conf 5.安装fastdfs-nginx-module上面的4个步骤已经可以上传和下载文件，但是fastdfs自己提供的http服务比较简洁，而且会出现数据同步不及时导致的读取数据不存在的情况，所以需要安装nginx模块来解决这个问题。 解压fastdfs-nginx-module-master包 cd fastdfs-nginx-module/src 复制文件到/etc/fdfs/目录下， cp mod_fastdfs.conf /etc/fdfs/ 进入/etc/fdfs/,编辑mod_fastdfs.conf,主要配置参数如下： base_path=/home/face/FastDFS/storage tracker_server=A机器ip:22122 tracker_server=B机器ip:22122 A机器：group_name=group1 B机器：group_name=group2 url_have_group_name = true store_path_count=1 store_path0=/home/face/FastDFS/storage/data group_count = 2 [group1]group_name=group1storage_server_port=23000store_path_count=1store_path0=/home/face/FastDFS/storage/data[group2]group_name=group2storage_server_port=23000store_path_count=1store_path0=/home/face/FastDFS/storage/data 进入fastdfs-nginx-module/src目录，编辑conf文件，修改如下： 1ngx_addon_name&#x3D;ngx_http_fastdfs_module23if test -n &quot;$&#123;ngx_module_link&#125;&quot;; then4 ngx_module_type&#x3D;HTTP5 ngx_module_name&#x3D;$ngx_addon_name6 ngx_module_incs&#x3D;&quot;&#x2F;usr&#x2F;include&#x2F;fastdfs &#x2F;usr&#x2F;include&#x2F;fastcommon&#x2F;&quot;7 ngx_module_libs&#x3D;&quot;-lfastcommon -lfdfsclient&quot;8 ngx_module_srcs&#x3D;&quot;$ngx_addon_dir&#x2F;ngx_http_fastdfs_module.c&quot;9 ngx_module_deps&#x3D;10 CFLAGS&#x3D;&quot;$CFLAGS -D_FILE_OFFSET_BITS&#x3D;64 -DFDFS_OUTPUT_CHUNK_SIZE&#x3D;&#39;256*1024&#39; -DFDFS_MOD_CONF_FILENAME&#x3D;&#39;\\&quot;&#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf\\&quot;&#39;&quot;11 . auto&#x2F;module12else13 HTTP_MODULES&#x3D;&quot;$HTTP_MODULES ngx_http_fastdfs_module&quot;14 NGX_ADDON_SRCS&#x3D;&quot;$NGX_ADDON_SRCS $ngx_addon_dir&#x2F;ngx_http_fastdfs_module.c&quot;15 CORE_INCS&#x3D;&quot;$CORE_INCS &#x2F;usr&#x2F;include&#x2F;fastdfs &#x2F;usr&#x2F;include&#x2F;fastcommon&#x2F;&quot;16 CORE_LIBS&#x3D;&quot;$CORE_LIBS -lfastcommon -lfdfsclient&quot;17 CFLAGS&#x3D;&quot;$CFLAGS -D_FILE_OFFSET_BITS&#x3D;64 -DFDFS_OUTPUT_CHUNK_SIZE&#x3D;&#39;256*1024&#39; -DFDFS_MOD_CONF_FILENAME&#x3D;&#39;\\&quot;&#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf\\&quot;&#39;&quot;18fi 6.安装nginx 解压nginx-1.14.2.tar.gz cd nginx-1.14.2 ./configure –prefix=/usr/local/nginx –add-module=/usr/src/fastdfs-nginx-module-master/srcmake &amp;&amp; make install 编辑nginx.conf配置文件，配置如下： 1upstream fdfs_group1 &#123;2 server A:8080 weight&#x3D;1 max_fails&#x3D;2 fail_timeout&#x3D;30s;3&#125; 4 5upstream fdfs_group2 &#123;6 server B:8080 weight&#x3D;1 max_fails&#x3D;2 fail_timeout&#x3D;30s;7&#125; 8 9include &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;conf.d&#x2F;*.conf; 创建conf.d文件夹，进入该文件夹，创建tracker.conf和storage.conf 编辑tracker.conf 1server &#123;2 listen 80;3 server_name A;4 5 location ~ &#x2F;group1&#x2F;M00 &#123;6 proxy_next_upstream http_502 http_504 error timeout invalid_header;7 proxy_pass http:&#x2F;&#x2F;fdfs_group1;8 expires 30d;9 &#125;1011 location ~ &#x2F;group2&#x2F;M00 &#123;12 proxy_next_upstream http_502 http_504 error timeout invalid_header;13 proxy_pass http:&#x2F;&#x2F;fdfs_group2;14 expires 30d;15 &#125;16&#125; 编辑stroage.conf A机器： 1server &#123;2 listen 8080;3 server_name A;4 location ~ &#x2F;group1&#x2F;M00 &#123;5 root &#x2F;home&#x2F;face&#x2F;FastDFS&#x2F;storage&#x2F;data;6 index index.html index.htm; 7 ngx_fastdfs_module;8 &#125;9&#125; B机器： 1server &#123;2 listen 8080;3 server_name B;4 location ~ &#x2F;group2&#x2F;M00 &#123;5 root &#x2F;home&#x2F;face&#x2F;FastDFS&#x2F;storage&#x2F;data;6 index index.html index.htm;7 ngx_fastdfs_module;8 &#125;9&#125; 启动nginx：/usr/local/nginx/sbin/nginx 7.测试文件上传，浏览器访问数据 编辑3个文件: echo “1111” &gt;&gt;/opt/1.txt &amp;&amp; echo “2222” &gt;&gt;/opt/2.txt &amp;&amp; echo “3333” &gt;&gt;/opt/3.txt 编辑client.conf文件，配置如下： tracker_server=A机器ip:22122 tracker_server=B机器ip:22122 上传文件: /usr/local/bin/fdfs_test /etc/fdfs/client.conf upload 1.txt /usr/local/bin/fdfs_test /etc/fdfs/client.conf upload 2.txt /usr/local/bin/fdfs_test /etc/fdfs/client.conf upload 3.txt 返回结果中会把上传完成的url 打印出来，复制URL在浏览器打开是否正常","categories":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"FastFDS","slug":"FastFDS","permalink":"http://beritra.github.com/tags/FastFDS/"},{"name":"高可用","slug":"高可用","permalink":"http://beritra.github.com/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"}]},{"title":"MySQL中的锁","slug":"MySQL中的锁","date":"2019-12-01T04:12:12.000Z","updated":"2022-02-20T11:27:13.888Z","comments":true,"path":"2019/12/01/MySQL中的锁/","link":"","permalink":"http://beritra.github.com/2019/12/01/MySQL%E4%B8%AD%E7%9A%84%E9%94%81/","excerpt":"本文主要摘抄自 MySQL 的官方文档和《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。注意，该书内容是基于MySQL 5.6 及更早的版本，具体的实现方式有可能发生了改变，有些内容我做了注释和补充，但是不保证百分百的完整和正确。","text":"本文主要摘抄自 MySQL 的官方文档和《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。注意，该书内容是基于MySQL 5.6 及更早的版本，具体的实现方式有可能发生了改变，有些内容我做了注释和补充，但是不保证百分百的完整和正确。 开发多用户、数据库驱动的应用时，最大的一个难点是：一方面要最大程度地利用数据库的并发访问，另一方面还要确保每个用户能以一致的方式读取和修改数据。为此就有了锁（locking）的机制，同时这也是数据库系统区别于文件系统的一个关键特性。 什么是锁锁机制用于管理对共享资源的并发访问。InnoDB 存储引擎会在行级别上对表数据上锁，这固然不错。不过InnoDB 存储引擎也会在数据库内部其他多个地方使用锁，从而允许对多种不同资源提供并发访问。例如，操作缓冲池 LRU 列表，删除、添加、移动 LRU 列表中的元素，为了保证一致性，必须有锁的介入。 lock 与 latch在数据库中，lock 与 latch 都可以被称为“锁”，但是两者有着截然不同的含义，本文主要关注的是lock。 latch 一般称为闩锁（轻量级的锁），因为其要求锁定的时间必须非常短。若持续的时间长，则应用的性能会非常差。在 InnoDB 存储引擎中，latch 又可以分为 mutex（互斥锁）和 rwlock（读写锁）。其目的是用来保证并发线程操作临界资源的正确性，并且通常没有死锁检测的机制。 lock 的对象是事务，用来锁定的是数据库中的对象，如表、页、行。并且一般 lock 的对象仅在事务 commit 或 rollback 后进行释放（不同事务隔离级别释放的时间可能不同）。此外，lock，正如在大多数数据库中一样，是有死锁机制的。下图显示了 lock 与 latch 的不同： 对于 InnoDB 存储引擎中的 latch，可以通过命令SHOW ENGINE INNODB MUTEX来进行查看。具体的数据结果说明如下： 上述信息是比较底层的，一般仅供开发人员参好。但是用户还是可以通过这些参数调优。 相对于 latch 的查看，lock 的信息就显得直观多了。用户可以通过命令SHOW ENGING INNODB STATUS及infomation_schema架构下的表INNODB_TRX、INNODB_LOCKS、INNODB_LOCK_WAITS来观察锁的信息。 InnoDB 存储引擎中的锁锁的类型InnoDB 存储引擎实现了如下两种标准的行级锁： 共享锁（Shared Lock，S Lock），允许事务读一行数据。 排它锁（Exclusive Lock，X Lock），允许事务删除或更新一行数据。 如果一个事务 T1 已经获得了行 r 的共享锁，那么另外的事务 T2 可以立即获得行 r 的共享锁，因为读取并没有改变行 r 的数据，称这种情况为锁兼容（Lock Compatible）。但若有其他事务 T3 想获得行 r 的排它锁，则其必须等待事务 T1、T2 释放行 r 上的共享锁——这种情况称为锁不兼容。下表显示了共享锁和排他锁的兼容性。 可以看出，X 锁与任何所都不兼容，而 S 锁仅和 S 锁兼容。需要特别注意的是，S 和 X 锁都是行锁，兼容是指对同一记录（row）锁的兼容性情况。 此外，InnoDB 存储引擎支持多粒度（granular）锁定，这种锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB 存储引擎支持一种额外的锁方式，称之为意向锁（Intention Lock）。意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度（fine granularity）上进行加锁。 怼细粒度的对象上锁，那么首先需要对粗粒度的对象上锁。如图所示，如果要对页上的对象 r 上 X 锁，那么分别需要对数据库A、表、页上 IX 锁，最后对记录 r 上 X 锁。若其中任何一部分导致等待，那么操作都需要等粗粒度锁的完成。 InnoDB 引擎支持意向锁设计的比较简练，其意向锁即为表级别的锁。设计目的主要是为了在一个事务中揭示下一行将被请求的锁类型。其支持两种意向锁： 意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁。 意向排他锁（IX Lock），事务想要获得一张表中某几行的排它锁。 由于 InnoDB 存储引擎支持的是行级别的锁，因此意向锁不会阻塞除全表扫描以外的任何请求。表级意向锁和行级锁的兼容性如图所示： 用户可以通过命令SHOW ENGINE INNODB STATUS命令查看当前锁请求的信息 从 InnoDB 1.0 开始，在INFORMATION_SCHEMA架构下添加了表INNODB_TRX、INNODB_LOCKS、INNODB_LOCKS_WAITS。通过这三张表，用户可以更简单地监控当前事务并分析可能存在的锁问题。INNODB的定义如下图所示，一共8个字段（我在MySQL 8.0 版本进行测试，发现已经扩充到了24个字段）： 这个表可以显示当前运行的 InnoDB 事务，并不能判断锁的一些情况。如果需要查看锁，则还需要访问表INNODB_LOCKS，该表的字段如下所示： 再通过表INNODB_LOCKS查看了每张表上锁的情况后，用户就可以判断由此引发的等待状况了。当事务较小时，用户就可以人为地、直观地进行判断了。但是当事务量非常大，其中锁和等待也时常发生，这个时候就不那么容易判断。但是通过表INNODB_LOCK_WAIT，可以很直观地反应当前事务的等待。表INNODB_LOCK_WAITS由四个字段组成，如下图所示。 注意：在 MySQL 8.0 版本中，INFORMATION_SCHEMA 下的 INNODB_LOCKS 和 INNODB_LOCK_WAITS 表已被删除。 用Performance Schema data_locks和 data_lock_waits 表替代。 一致性非锁定读一致性的非锁定读（consistent nonlocking read）是指 InnoDB 存储引擎通过行多版本控制（multi versioning）的方式来读取当前执行时间数据库中行的数据。如果读取的行正在执行 DELETE 或者 UPDATE 操作，这时读取操作不会因此去等待行上锁的释放。相反的，InnoDB 存储引擎会去读取行的一个快照存储。 之所以称其为非锁定读，因为不需要等待访问的行上 X 锁的释放。快照数据是指该行的之前版本的数据，该实现是通过 undo 段来完成，而 undo 用来在事务中回滚数据，因此快照数据本身是没有额外的开销的。此外，读取快照数据是不需要上锁的吗，因为没有事务需要对历史的数据进行修改操作。 可以看出，非锁定读机制极大地提提高了数据库的并发性。在 InnoDB 存储引擎的默认设置下，这是默认的读取方式，即读取不会占用和等待表上的锁。但是在不同事务隔离级别下，读取的方式不同，并不是在每个事务隔离级别下都是采用的非锁定的一致性读。此外，即使都是采用的非锁定的一致性读，但是对于快照数据的定义也各不相同。 在事务隔离级别READ COMMITTED和REPEATABLE READ（InnoDB存储引擎的默认事务隔离级别）下，InnoDB 存储引擎使用非锁定的一致性读。然而，对于快照数据的定义却不相同。在READ COMMITTED事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据。而在REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本。 举个栗子，首先在当前 MySQL 数据库的连接回话 A 中执行以下 SQL 语句： 1mysql&gt; begin;2Query OK, 0 rows affected (0.01 sec)34mysql&gt; select * from parent where id &#x3D;1;5+------+6| id |7+------+8| 1 |9+------+101 row in set (0.00 sec) 在会话 A 显式的开启了一个事务，并读取了表 parent 中 id 为 1 的数据，但是事务并没有结束。与此同时，用户再开启另一个回话 B，这样可以模拟并发的情况，然后对 B 做如下的操作： 1mysql&gt; begin;2Query OK, 0 rows affected (0.00 sec)34mysql&gt; update parent set id&#x3D;3 where id&#x3D;1;5Query OK, 1 row affected (0.00 sec)6Rows matched: 1 Changed: 1 Warnings: 0 在会话 B 中将事务表 parent 中 id 为 1 的记录修改为 id=3 ，但是事务同样没有提交。这样 id=1 的行实际上加了一个 X 锁。这时如果在会话 A 中再次读取 id 为 1 的记录，根据 InnoDB 引擎的特性，即在READ COMMITTED和REPEATETABLE READ的事务隔离级别下会使用非锁定的一致性读。回到之前的会话 A ，接着上次未提交的事务，执行 SQL 语句 select * from parent where id=1的操作，这时不管使用READ COMMITTED还是REPEATETABLE READ的事务隔离级别，显示的数据应该都是： 1mysql&gt; select * from parent where id &#x3D;1;2+------+3| id |4+------+5| 1 |6+------+71 row in set (0.00 sec) 由于当前 id=1 的数据被修改了1次，因此只有一个行版本的记录。接着，在会话 B 中提交上次的事务： 1mysql&gt; commit;2Query OK, 0 rows affected (0.00 sec) 在会话 B 提交事务之后，在会话 A 中再次运行select * from parent where id=1语句，在READ COMMITTED和REPEATETABLE READ的事务隔离级别下得到的结果就不一样了。对于READ COMMITTED的事务隔离级别，它总是能够读取行的最新版本，如果行被锁定了，则读取最新的一个快照（fresh snapshot）。在上述例子中，因为会话 B 已经提交了事务，所以READ COMMITTED事务隔离级别下会得到如下结果： 1mysql&gt; select @@transaction_isolation\\G;2*************************** 1. row ***************************3@@transaction_isolation: REPEATABLE-READ41 row in set (0.00 sec)56mysql&gt; select * from parent where id &#x3D;1;7Empty set (0.00 sec) 而对于REPEATABLE READ的事务隔离级别，总是读取事务开始时的行数据。因此对于REPEATABLE REPEAD事务隔离级别，得到的结果如下： 1mysql&gt; select @@transaction_isolation\\G;2*************************** 1. row ***************************3@@transaction_isolation: REPEATABLE-READ41 row in set (0.00 sec)56mysql&gt; select * from parent where id &#x3D;1;7+------+8| id |9+------+10| 1 |11+------+121 row in set (0.00 sec) 注意，MySQL 8.0 之前查询当前事务隔离级别的语句是select @@tx_isolation，之后是上面用的select @@transaction_isolation 下面将从时间的角度上展现上述演示的示例过程，如下表所示。需要特别注意的是，对于READ COMMITTED的事务隔离级别而言，从数据库理论的角度来看，其违反了事务 ACID 中的 I 的特性，即隔离性。 一致性锁定锁在前一节讲到，默认配置下，事务隔离级别是REPEATABLE READ模式，InnoDB 存储引擎的SELECT操作使用一致性非锁定锁。但是在某些情况下，用户需要显式得堆数据库读取操作进行加锁以保证数据逻辑的一致性。InnoDB 存储引擎对于SELECT语句支持两种一致性的锁定读（locking read）操作： SELECT … FOR UPDATE SELECT … LOCK IN SHARE MODE SELECT ... FOR UPDATE堆读取的行记录加一个 X 锁，其他事务不能对已锁定的行加上任何锁。SELECT ... LOCK IN SHARE MODE对读取的行加一个 S 锁，其他事务可以向被锁定的行加 S 锁，但是如果加 X 锁，就会被阻塞。 以上这两个语句必须在同一个事务中，当事务提交了，锁也就释放了。因此在使用两个SELECT 锁的时候，务必记得带上BEGIN，START TRANSACTION或者SET AUTOCOMMIT=0。 自增长与锁自增长是非常常用的一个属性，在 InnoDB 引擎的内存结构中，对每个含有自增长值的表都有一个自增长计数器（auto-increment counter）。当对含有自增长的计数器的表进行插入操作时，这个计数器会被初始化，执行如下的语句来得到计数器的值： 1SELECT MAX(auto_inc_col) FROM t FOR UPDATE; 插入操作会依据这个自增的计数器的值加 1 赋予自增长列。这个实现方式称作AUTO-INC Locking。这个锁其实是采用一种特殊的表锁机制，为了提高插入的性能，锁不是在一个事务完成后才释放，而是在完成对自增长值插入 SQL 语句后立即释放。 虽然AUTO-INC Locking从一定程度上提高了并发插入的效率，但是还存在一定性能上的问题。首先对于自增长值得咧并发插入性能较差，事务必须等待前一个插入完成。其次，对于INSERT ... SELECT的大数据量插入会影响插入性能。因为另一个事务中的插入会被阻塞。 从 MySQL 5.1.22 开始，InnoDB 存储引擎中提供了一种轻量级互斥锁的自增长实现机制，这种机制大大提高了自增值的插入性能。从该版本开始，InnoDB 存储引擎提供了一个参数innodb_autoinc_lock_mode来控制自增长的模式，该参数的默认值为 1 。我们对自增长的插入进行分类的话，如图所示。 接着来分析参数innodb_autoinc_lock_mode以及各个设置下堆自增的印象，总共有三个有效值可供供设定，即0、1、2，具体说明如下表所示： 在 InnoDB 存储引擎中，自增长的列必须是索引，同时必须是索引的第一个列。如果不是第一个列，则 MySQL 数据库会抛出异常。 外键和锁在 InnoDB 存储引擎中，对于一个外键列，如果没有显式的对这个列加索引，InnoDB 引擎会自动加一个索引，因为这样可以避免表锁。对于外键的插入或更新，首先需要查询父表中的记录，即 SELECT 父表。单是对于父表的 SELECT 操作，不是使用一致性非锁定读的方式，因为这样会发生数据不一致的问题。因此这时使用的是SELECT ... LOCK IN SHARE MODE方式，即主动对父表加一个 S 锁。如果这时候父表已经加上了一个 X 锁，子表上的操作会被阻塞。 锁的算法行锁的 3 种算法InnoDB 存储引擎有三种行锁的算法，分别是： Record Lock：单个行记录上的锁 Gap Lock：间隙锁，锁定一个范围，但不包含记录本身 Next-Key Lock：Gap Lock+Record Lock，锁定一个范围，并且锁定记录本身 Record LockRecord Lock 总是会锁住索引记录，如果 InnoDB 存储引擎表在建立的时候没有设置任何一个索引，那么这是 InnoDB 存储引擎会使用隐式的主键来进行锁定。比如， SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; 会阻止其他任何事务操作t.c1 = 10的行，包括更新、插入和删除操作。 Gap LockGap lock 会锁住两个索引中间的一段间隙（范围），或者索引之前或之后的一段范围。比如， SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE; 会阻止其他事务插入15 到到 t.c1的字段中，无论这个字段中当前有没有记录。因为从 10 到 20 这一段索引已经被锁住了。 这个间隙的范围可以跨越单个索引值，多个索引值，甚至为空。 间隙锁是性能和并发性之间权衡，只在某些事务隔离级别中使用。 对于使用唯一索引来锁定唯一行来锁定行的语句，不需要间隙锁定。（这不包括搜索条件仅包含多列唯一索引的某些列的情况；在这种情况下，会发生间隙锁定。）例如，如果 id 列具有唯一索引，则以下语句仅使用 id 值为 100 的行的索引记录锁，其他会话是否在前面的间隙中插入行都没有关系： 1SELECT * FROM child WHERE id = 100; 如果 id 未建立索引或索引不唯一，则该语句会锁定前面的间隙。 在这里还值得注意的是，可以通过不同的事务将冲突的锁保持在间隙上。例如，事务 A 可以在间隙上保留一个共享的间隙锁（间隙 S 锁），而事务 B 可以在同一间隙上保留排他的间隙锁（间隙 X 锁）。允许冲突的间隙锁的原因是，如果从索引中清除记录，则必须合并由不同事务保留在记录上的间隙锁。 InnoDB 中的间隙锁是“完全禁止的”，这意味着它们的唯一目的是防止其他事务插入间隙。间隙锁可以共存。一个事务进行的间隙锁定不会阻止另一事务对相同的间隙进行间隙锁定。共享和专用间隙锁之间没有区别。它们彼此不冲突，并且执行相同的功能。 间隙锁定可以显式禁用。如果将事务隔离级别更改为 READ COMMITTED，则会发生这种情况。在这种情况下，将禁用间隙锁定来进行搜索和索引扫描，并且间隙锁定仅用于外键约束检查和重复键检查。 使用 READ COMMITTED 隔离级别还有其他影响。 MySQL 评估 WHERE 条件后，将释放不匹配行的记录锁。对于 UPDATE 语句，InnoDB 进行“半一致”读取，以便将最新的提交版本返回给 MySQL，以便 MySQL 可以确定该行是否与 UPDATE 的 WHERE 条件匹配。 Next-Key LockNext-Key Lock 是结合了 Gap Lock 和 Record Lock 的一种锁定算法，在 Next-Key Lock 算法下，InnoDB 对于行的查询都是采用这种锁定算法。例如一个索引有10，11，13和20这四个值，那么该索引可能被 Next-Key Lock 的区间为： 1(-∞，10]2(10，11]3(11，13]4(13，20]5(20，+∞) 采用 Next-Key Lock 的锁定技术称为 Next-Key Locking。其设计的目的是为了解决 Phantom Problem，这将在下一个小节介绍。利用这种技术，锁定的不是单个值，而是一个范围，是谓词锁（predict lock）的一种改进。除了 Next-Key Locking，还有 Previous-Key Locking 技术。 同样上述索引 10，11，13 和 20，如果采用 Previous-Key Locking 技术，可锁定的区间会变成： 1(-∞，10)2[10，11)3[11，13)4[13，20)5[20，+∞) 若事务 T1 已经通过 Next-Key Locking 锁定了如下范围： (10,11]、(11,13] 当插入新的记录 12 的时候，锁定的范围会变成： (10,11]、(11,12]、(12,13] 当查询的索引含有唯一属性的时候，InnoDB 存储引擎会对 Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围，从而提高应用的并发性。 使用下列代码创建测试表 t ： 1create table t(a int primary key);2insert into t select 1;3insert into t select 2;4insert into t select 5; 然后执行下列语句： 表 t 一共只有1、2、5 三个值。在上面的例子中，会话 A 首先对 a=5 进行 X 素哟定。而由于 a 是主键且唯一，因此锁定的仅仅是 5 这个值，而不是（2,5）这个范围，这样在会话 B 中插入值 4 而不会阻塞，可以立即插入并返回。 若是辅助索引，情况会完全不同。根据下列代码创建测试表 z： 1mysql&gt; create table z(a int,b int, primary key(a),key(b));2mysql&gt; insert into z select 1,1;3mysql&gt; insert into z select 3,1;4mysql&gt; insert into z select 5,3;5mysql&gt; insert into z select 7,6;6mysql&gt; insert into z select 10,8; 表 z 的列 b 是辅助索引，若在会话 A 中开启事务，执行下面的 SQL 语句： 1select * from z where b&#x3D;3 for update; 这时 SQL 语句将通过索引列 b 进行查询，因此使用传统的 Next-Key Locking技术进行加锁，并且由于有两个索引，需要对其分别加锁。对于聚集索引，仅对列 a 等于 5 的索引加上 Record Lock。而对于辅助索引，其加上的是 Next-Key Lock，锁定范围是（1,3），需要特别注意的，InnoDB 存储引擎还会对辅助索引下一个键值加上 gap lock，即还有一个辅助索引范围为（3,6）的锁，因此，若在新回话 B 中运行下面的 SQL 语句，都会被阻塞。 1select * from z where a&#x3D;5 lock in share mode;2insert into z select 4,2;3insert into z select 6,5; 原因如下： 第一个 SQL 语句不能执行，因为在会话 A 中执行的 SQL 语句已经对聚集索引中列 a=5 的值加上 X 锁，因此执行会被阻塞。 第二个 SQL 语句，主键插入 4 ，没有问题，但是插入的辅助索引 2 在锁定的范围（1,3）中，所以同样会阻塞 第三个 SQL 语句，插入的主键 6 没有被锁定，5 也不在范围（1,3）之间，但是插入的值 5 在另一个锁定的范围（3,6）中，所以同样需要等待。 上面的例子可以看出，Gap Lock 的作用是为了阻止多个事务将记录插入到同一个范围内，而这会导致 Phantom Problem 问题的产生。例如上面的例子中，会话 A 用户锁定了 b=3 的记录，如果此时没有锁定（3,6），那么用户可以插入 b 列为 3 的记录，这会导致会话 A 中的用户再次执行查询时返回不同的记录，这就导致了 Phantom Problem 问题的产生。 用户可以通过以下两种方式来显式的关闭 Gap Lock： 将事务的隔离级别设置为READ COMMITTED 将参数innodb_locks_unsafe_for_binlog设置为 1 在上述配置下，除了外键约束和唯一性检查仍然需要 Gap Lock，其余情况仅使用 Record Lock 进行锁定。但需要牢记的是，上述设置破坏了事务的隔离性，并且对于 replication，可能会导致主从数据的不一致。因此从性能上看，READ COMMITTED也不会优于默认的事务隔离级别READ REPEATABLE。 需要再次提醒的是，对于唯一键值的锁定，Next-Key Lock 降级为 Record Lock 仅存在于查询所有的唯一索引列。若唯一索引由多个列组成，而查询仅是查找多个唯一索引列中的其中一个，那么查询其实是 range 类型查询，而不是 point 类型查询，故 InnoDB 存储引擎依然使用 Next-Key Lock 进行锁定。 解决 Phantom Problem在默认的事务隔离级别下，即REPEATABLE READ下，InnoDB 存储引擎采用 Next-Key Locking 机制来避免 Phantom Problem（幻想问题）。 Phantom Problem 是指在同一事务下，连续执行两次同样的 SQL 可能导致不同的结果，第二次的 SQL 语句可能会返回之前不存在的行。 还是以上一节创建的表 t 为例，表 t 由 1、2、5 三个值组成，如果事务 T1 执行如下 SQL 语句： 1select * from t where a&gt;2 for update; 注意这时候 T1 并没有进行提交操作，上述应该返回 5 这个结果。如果与此同时，另一个事务 T2 插入了 4 这个值，并且数据库允许该操作，那么事务 T1 再次执行查询 SQL 语句的时候，就会得到结果 4 和 5 。这与第一次查询的结果不同，违反了事务的隔离性，即当前事务可以看到其他事务的结果。 InnoDB 引擎采用了 Next-Key Locking 算法避免出现上面的情况。对于上面的 SQL，实际上锁住的不只是 5 这个单个值，而是对（2,+∞）范围都加了 X 锁，因此对于这个范围内的插入都是不允许的，从而避免 Phantom Problem。 InnoDB 引擎默认的事务隔离级别是REPEATABLE READ，在该隔离级别下，采用 Next-Key Locking 来加锁。而在事务隔离级别READ COMMITTED级别下，仅采用 record Lock，因此在上述示例中，会话 A 需要将事务的隔离级别设置为READ COMMITTED。 此外，用户可以通过 InnoDB 存储引擎的 Next-Key Locking 机制在应用层面实现唯一性检查： 1select * from table where col&#x3D;xxx lock in share mode; 如果用户通过索引查询一个值，并对该行加上一个 S Lock，那么即使查询的值不存在，锁定的也是一个范围，因此若此行没有任何返回，那么新插入的值一定是唯一的。 这里如果在第一步select ... lock in share mode操作的回收，有多个事务并发操作，这种唯一性检查机制是否存在问题呐？答案是不会，因为这时候会导致死锁，只有一个事务操作成功，其他会抛出死锁的错误提示。 锁问题通过锁定机制可以实现事务的隔离性要求，使得事务可以并发的工作。锁提高了并发，但是也有潜在的问题。不过好在事务隔离性的要求，锁只会带来以下三种问题。 脏读在理解脏读（Dirty Read）之前，需要理解脏数据的概念。脏数据是指未提交的数据，如果读到了脏数据，即一个事务可以读到另外一个事务未提交的数据，则显然违反了数据的隔离性。 下面的表格显示了一个脏读的例子： 表 t 还是上面创建的那个，不过不同于上述例子，这里把事务隔离级别改成了READ UNCOMMITTED。因此在会话 A 未提交的前提下，事务 B 两次 SELECT 取得了不同结果，即产生了脏读。 脏读现象在生产环境并不经常发生，由上面例子可以看出来，脏读需要隔离级别为READ UNCOMMITTED，实际上大多数数据库至少都是READ COMMITTED，而 InnoDB 默认的都是READ REPEATABLE。 脏读隔离看似毫无用处，但是一些特殊的情况下还可以将事务的隔离级别放开到READ UNCOMMITTED。例如 replication 环境下的 slave 节点，并且在该 slave 节点上的查询并不需要特别精确的返回值。、 不可重复读不可重复读是指在一个事务内多次读取同一个数据集合。在这个事务还没有结束时，另一个事务也访问该同一个数据集合，并做了一些 DML 操作。因此在第一个事务两次读数据之间，由于第二个事务的修改，那么第一个事务读到的数据可能是不一样的。这种情况成为不可重复读。 不可重复读和脏读的区别：脏读读到的是未提交的数据，不可重复读读到的却是提交过的数据，但是其违反了数据库事务一致性的要求。下面的表格展示了一个例子： 这个例子的前提是，两个事务的隔离级别都调整为READ COMMITTED。会话 B 的事务提交之后，事务 A 进行读取，读到的是 1 和 2 两条记录。 一般情况下，不可重复读是可以接受的，因为读到的是已经提交的数据，本身不会带来太大的问题，因此很多数据库的默认级别设置为READ COMMITTED，允许不可重复读。 在 InnoDB 存储引擎中，使用 Next-Key Lock 算法来避免不可重复读的问题。在 Next-Key Lock 算法下，对于索引的扫描，不仅是锁住了扫描到的索引，还锁着了索引覆盖的范围（gap）。因此在这个范围内的插入都是不被允许的。这样就避免了其他事务在这个范围内插入数据导致不可重复读的问题。 丢失更新丢失更新是另一个锁导致的问题，简单来说就是一个事务的更新操作会被另一个事务的更新操作覆盖，从而导致数据不一致。例如： 事务 T1 将行记录 r 更新为 v1，但是事务 T1 尚未提交。 与此同时，事务 T2 将行记录 r 更新为 v2，事务 T2 未提交。 事务 T1 提交。 事务 T2 提交。 虽然在数据库的任何隔离级别下，都不会导致理论意义上的丢失更新问题，即使是READ UNCOMMITTED的事务隔离级别，对于任何行的 DML 操作，需要对行或者其他粗粒度级别的对象加锁。因此上述步骤中，T2 的更新操作不能顺利执行，会被阻塞。 但是，生产中还会出现类似的另一个逻辑意义上的丢失更新问题，尤其是多用户的计算机系统环境下： 事务 T1 查询一行数据，放到本地内存，并显示给一个终端用户 User1。 与此同时，事务 T2 也查询了数据，将数据展示给另一个用户 User2。 User1 修改了这行记录，更新数据库提交。 User2 也修改了记录，更新数据库提交。 显然，这个过程中 User1 的更新操作“丢失”了。为了避免这种问题，需要让事务操作串行化，而不是并行。即在操作步骤 1 中，对用户读取的记录加上一个排他锁 X 。同样，在步骤 2 中，同样需要加入排它锁 X 。通过这种方式，步骤 2 就必须等待步骤 1 和 3 完成。下面的表格演示了如何避免上述逻辑上丢失更新问题的产生。 阻塞由于不同锁之间的兼容关系，有时候一个事务中的锁需要等待另一个事务中的锁释放它占用的资源，这就是阻塞。 在 InnoDB 存储引擎中，参数innodb_lock_wait_timeout控制等待时间，innodb_rollback_on_timeout用来设定是否在等待超时时堆进行中的事务进行回滚操作。参数innodb_lock_wait_timeout是动态的，可以在数据库运行时动态调整，而innodb_rollback_on_timeout是静态的，不能在运行中修改。 死锁死锁的概念死锁是指两个或两个以上事务在执行过程中，因争夺锁资源而造成的一种相互等待的现象。 解决死锁最简单的方式是超时，当两个事务相互等待的时候，当其中一个等待超过阈值，进行回滚，另一个事务就可以继续进行。超时机制虽然简单，但是仅仅通过超时后事务回滚的方式处理，或者根据 FIFO 的顺序选择回滚对象。但如果超时的事务所占权重比较大，如事务操作更新很多航，占用了较多的 undo log，这时候采用 FIFO 的方式就不合适了。 因此，除了超时机制，当前数据库还普遍采用 wait-for graph（等待图）的方式来进行死锁检测。 wait-for graph 要求数据库保存以下两种信息： 锁的信息链表 事务等待链表 通过上述链表可以构造一张图，如果这张图中存在回路，就代表的存在死锁。 wait-for graph 是一种比较主动的死锁检测机制，InnoDB 一般会选择回滚 undo 量最小的事务。 死锁概率死锁的概率推导过程就不抄了，直接截图放这里： 可以看出死锁发生概率与以下几点有关： 系统中事务数量（n），数量越多死锁概率越大。 每个事务的操作量，数量越多死锁概率越大。 操作数据的集合（R），越小则死锁的概率越大。 死锁的示例下面的表演示了一个死锁的经典情况： 上面的例子中，会话 B 抛出了 1213 这个错误提示，即表示事务发生了死锁。大多数死锁 InnoDB 引擎本身可以侦测到，不需要人为进行干预。 还有另外一种情况，即当前事务持有了待插入记录的下一个记录的 X 锁，但是在等待队列中存在一个 S 锁的请求，则可能发生死锁。举个栗子，先创建一个表： 1CREATE TABLE t( a INT PRIMARY KEY)ENGINE&#x3D;INNODB;2INSERT INTO t VALUES (1),(2),(4),(5); 然后运行下表所示的查询： 可以看到，会话 A 中已经对记录 4 持有了 X 锁，但是会话 A 中插入记录 3 时会导致死锁发生。这个问题的产生是由于会话 B 中请求记录 4 的 S 锁而发生等待，但之前请求的锁对于主键值记录 1、2 都已经成功，若在时间点 5 能插入记录，那么会话 B 在获得记录 4 持有的 S 锁之后，还需要向后获得记录 3 的记录，这样就显得不合理。因此 InnoDB 引擎在这里主动选择了死锁，而回滚的是 undo log 记录大的事务，这与 AB-BA 死锁的处理方式又有所不同。 锁升级锁升级（Lock Escalation）是指将当前锁的粒度降低。举例来说，数据库可以吗一个表的 1000 个行锁升级为一个页锁，或者页锁升级为一个表锁。如果数据库设计中认为锁是稀有资源，想要尽量避免锁的开销，就会频繁出现锁升级现象。 InnoDB 存储引擎不存在锁升级的问题。因为其不是根据每个记录来产生行锁的，而是采用位图。不管一个事务锁住页中的一个记录还是多个记录，开销通常都是一致的。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"锁","slug":"锁","permalink":"http://beritra.github.com/tags/%E9%94%81/"}]},{"title":"MySQL基础知识总结","slug":"MySQL基础知识总结","date":"2019-12-01T04:12:12.000Z","updated":"2022-02-20T11:27:13.892Z","comments":true,"path":"2019/12/01/MySQL基础知识总结/","link":"","permalink":"http://beritra.github.com/2019/12/01/MySQL%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/","excerpt":"总结 MySQL 中的一些基础知识点。","text":"总结 MySQL 中的一些基础知识点。 本文尽量按照MySQL 8.0 版本的规则，但是很多参考书籍和资料还是基于 5.* 版本，可能有疏漏。 SQL基础SQL 全称是 Structure Query Language（结构化查询语言）。主要分为三个类别： DDL（Data Definiton Languages）语句：数据定义语言，这些语句定义的不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的关键字包括create、drop、alter等。 DML（Data Manipulation Language）语句：数据操纵语句、用于添加、删除、更新和查询数据库记录，并检查数据完整性，常见的语句关键字包括insert、delete、update和select等。 DCL（Data Control Language）语句：数据控制语句，用于控制不同的数据段直接的许可和访问级别的语句。这些语句定义的数据库、表、字段、用户的访问权限和安全级别。主要语句关键字包括grant、revoke等。 DDL语句数据库增删改查数据库创建的格式如下： 1CREATE DATABASE db_name2 [[DEFAULT] CHARACTER SET charset_name]3 [[DEFAULT] COLLATE collation_name] 常用的字符集参数为： 1create database test_database default CHARACTER SET utf8mb4 collate utf8mb4_unicode_ci; 建议使用utf8mb4代替utf8，后者在存储比如emoji表情的时候会出错。collate关键字代表的是字符排序规则，会影响结果展示的顺序和order by等排序参数。 删除数据库语句： 1drop database db_name; 要修改字符集参数的话： 1ALTER DATABASE db_name2 [[DEFAULT] CHARACTER SET charset_name]3 [[DEFAULT] COLLATE collation_name] MySQL似乎是没有提供对数据库重命名的方法。 如果想查看一共有多少个数据库，可以用： 1show databases; 会显示包含系统库在内的所有数据库。 表的增删改查创建表的语句比较复杂，直接把官网文档上的完整表述贴了过来： 1CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name2 (create_definition,...)3 [table_options]4 [partition_options]56CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name7 [(create_definition,...)]8 [table_options]9 [partition_options]10 [IGNORE | REPLACE]11 [AS] query_expression1213CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name14 &#123; LIKE old_tbl_name | (LIKE old_tbl_name) &#125;1516create_definition:17 col_name column_definition18 | &#123;INDEX|KEY&#125; [index_name] [index_type] (key_part,...)19 [index_option] ...20 | &#123;FULLTEXT|SPATIAL&#125; [INDEX|KEY] [index_name] (key_part,...)21 [index_option] ...22 | [CONSTRAINT [symbol]] PRIMARY KEY23 [index_type] (key_part,...)24 [index_option] ...25 | [CONSTRAINT [symbol]] UNIQUE [INDEX|KEY]26 [index_name] [index_type] (key_part,...)27 [index_option] ...28 | [CONSTRAINT [symbol]] FOREIGN KEY29 [index_name] (col_name,...)30 reference_definition31 | check_constraint_definition3233column_definition:34 data_type [NOT NULL | NULL] [DEFAULT &#123;literal | (expr)&#125; ]35 [AUTO_INCREMENT] [UNIQUE [KEY]] [[PRIMARY] KEY]36 [COMMENT &#39;string&#39;]37 [COLLATE collation_name]38 [COLUMN_FORMAT &#123;FIXED|DYNAMIC|DEFAULT&#125;]39 [STORAGE &#123;DISK|MEMORY&#125;]40 [reference_definition]41 [check_constraint_definition]42 | data_type43 [COLLATE collation_name]44 [GENERATED ALWAYS] AS (expr)45 [VIRTUAL | STORED] [NOT NULL | NULL]46 [UNIQUE [KEY]] [[PRIMARY] KEY]47 [COMMENT &#39;string&#39;]48 [reference_definition]49 [check_constraint_definition]5051data_type:52 (see Chapter 11, Data Types)5354key_part: &#123;col_name [(length)] | (expr)&#125; [ASC | DESC]5556index_type:57 USING &#123;BTREE | HASH&#125;5859index_option:60 KEY_BLOCK_SIZE [&#x3D;] value61 | index_type62 | WITH PARSER parser_name63 | COMMENT &#39;string&#39;64 | &#123;VISIBLE | INVISIBLE&#125;6566check_constraint_definition:67 [CONSTRAINT [symbol]] CHECK (expr) [[NOT] ENFORCED]6869reference_definition:70 REFERENCES tbl_name (key_part,...)71 [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE]72 [ON DELETE reference_option]73 [ON UPDATE reference_option]7475reference_option:76 RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT7778table_options:79 table_option [[,] table_option] ...8081table_option:82 AUTO_INCREMENT [&#x3D;] value83 | AVG_ROW_LENGTH [&#x3D;] value84 | [DEFAULT] CHARACTER SET [&#x3D;] charset_name85 | CHECKSUM [&#x3D;] &#123;0 | 1&#125;86 | [DEFAULT] COLLATE [&#x3D;] collation_name87 | COMMENT [&#x3D;] &#39;string&#39;88 | COMPRESSION [&#x3D;] &#123;&#39;ZLIB&#39;|&#39;LZ4&#39;|&#39;NONE&#39;&#125;89 | CONNECTION [&#x3D;] &#39;connect_string&#39;90 | &#123;DATA|INDEX&#125; DIRECTORY [&#x3D;] &#39;absolute path to directory&#39;91 | DELAY_KEY_WRITE [&#x3D;] &#123;0 | 1&#125;92 | ENCRYPTION [&#x3D;] &#123;&#39;Y&#39; | &#39;N&#39;&#125;93 | ENGINE [&#x3D;] engine_name94 | INSERT_METHOD [&#x3D;] &#123; NO | FIRST | LAST &#125;95 | KEY_BLOCK_SIZE [&#x3D;] value96 | MAX_ROWS [&#x3D;] value97 | MIN_ROWS [&#x3D;] value98 | PACK_KEYS [&#x3D;] &#123;0 | 1 | DEFAULT&#125;99 | PASSWORD [&#x3D;] &#39;string&#39;100 | ROW_FORMAT [&#x3D;] &#123;DEFAULT|DYNAMIC|FIXED|COMPRESSED|REDUNDANT|COMPACT&#125;101 | STATS_AUTO_RECALC [&#x3D;] &#123;DEFAULT|0|1&#125;102 | STATS_PERSISTENT [&#x3D;] &#123;DEFAULT|0|1&#125;103 | STATS_SAMPLE_PAGES [&#x3D;] value104 | TABLESPACE tablespace_name [STORAGE &#123;DISK|MEMORY&#125;]105 | UNION [&#x3D;] (tbl_name[,tbl_name]...)106107partition_options:108 PARTITION BY109 &#123; [LINEAR] HASH(expr)110 | [LINEAR] KEY [ALGORITHM&#x3D;&#123;1|2&#125;] (column_list)111 | RANGE&#123;(expr) | COLUMNS(column_list)&#125;112 | LIST&#123;(expr) | COLUMNS(column_list)&#125; &#125;113 [PARTITIONS num]114 [SUBPARTITION BY115 &#123; [LINEAR] HASH(expr)116 | [LINEAR] KEY [ALGORITHM&#x3D;&#123;1|2&#125;] (column_list) &#125;117 [SUBPARTITIONS num]118 ]119 [(partition_definition [, partition_definition] ...)]120121partition_definition:122 PARTITION partition_name123 [VALUES124 &#123;LESS THAN &#123;(expr | value_list) | MAXVALUE&#125;125 |126 IN (value_list)&#125;]127 [[STORAGE] ENGINE [&#x3D;] engine_name]128 [COMMENT [&#x3D;] &#39;string&#39; ]129 [DATA DIRECTORY [&#x3D;] &#39;data_dir&#39;]130 [INDEX DIRECTORY [&#x3D;] &#39;index_dir&#39;]131 [MAX_ROWS [&#x3D;] max_number_of_rows]132 [MIN_ROWS [&#x3D;] min_number_of_rows]133 [TABLESPACE [&#x3D;] tablespace_name]134 [(subpartition_definition [, subpartition_definition] ...)]135136subpartition_definition:137 SUBPARTITION logical_name138 [[STORAGE] ENGINE [&#x3D;] engine_name]139 [COMMENT [&#x3D;] &#39;string&#39; ]140 [DATA DIRECTORY [&#x3D;] &#39;data_dir&#39;]141 [INDEX DIRECTORY [&#x3D;] &#39;index_dir&#39;]142 [MAX_ROWS [&#x3D;] max_number_of_rows]143 [MIN_ROWS [&#x3D;] min_number_of_rows]144 [TABLESPACE [&#x3D;] tablespace_name]145146query_expression:147 SELECT ... (Some valid select or union statement) 可以看到创建表的时候，主要分为三个部分，包括创建定义、表选项、分区选项等，实际上大多数时候用不到这么多功能。 不手动指定的话，模式使用InnoDB作为数据库引擎，InnoDB最大支持40亿张表。 一个简单的创建语句： 1create table test_table2(3 id int not null auto_increment primary key,4 name varchar(20),5 class varchar(20),6 age int7)engine&#x3D;InnoDB character set utf8mb4; 创建完成后，可以使用desc关键字查看表结构： 1desc tablename; desc命令输出的信息相对比较简单，可以使用如下语句查看详细的建表语句： 1show create table tablename; 修改表结构使用alter table语句： 修改表类型：ALTER TABLE table_name MODIFY [COLUMN] column_definition [FIRST|AFTER colname] 增加表字段：ALTER TABLE table_name ADD [COLUMN] column_definition [FIRST|AFTER col_name] 删除表字段：ALTER TABLE table_name DROP [COLUMN] col_name 修改字段名：ALTER TABLE tablename CHANGE [COLUMN] old_col_name column_definition [FIRST|AFTER col_name] 修改字段排列顺序：前面介绍的的字段增加和修改语法(ADD/CNAHGE/MODIFY)中,都有一个可选项 first|aftercolumn_name,这个选项可以用来修改字段在表中的位置,默认 ADD 增加的新字段是加在表的最后位置,而 CHANGE/MODIFY 默认都不会改变字段的位置。 重命名表，老版本的MySQL一般使用：ALTER TABLE test_table RENAME to test_table02; MySQL 8.0中支持rename关键字：rename table test_table02 to test_table; DML语句插入记录表创建好后,就可以往里插入记录了,插入记录的基本语法如下: 1INSERT INTO tablename (field1,field2,......fieldn) VALUES(value1,value2,......valuesn); 更新记录对于表里的记录值,可以通过 update 命令进行更改,语法如下: 1UPDATE tablename SET field1&#x3D;value1,field2.&#x3D;value2,......fieldn&#x3D;valuen [WHERE CONDITION] 删除记录如果记录不再需要,可以用 delete 命令进行删除,语法如下: 1DELETE FROM tablename [WHERE CONDITION] 查询记录SELECT 的语法很复杂,所有这里只介绍最基本的语法: 1SELECT * FROM tablename [WHERE CONDITION] 太复杂的就不重复了，这些都是基础内容，网上的教程一大把。 DCL语句DCL 语句主要是 DBA 用来管理系统中的对象权限时所使用,一般的开发人员很少使用。只记录几条经常用的吧。 这里经常遇到的就是MySQL 8.0和之前版本的不一致，往往在网上搜到一个命令，拿过来不能用，MySQL 8.0的变动以下几个方面： 验证插件和密码加密方式的变化在 MySQL 8.0 中，caching_sha2_password 是默认的身份验证插件而不是之前版本的 mysql_native_password，默认的密码加密方式是 SHA2 。 用户授权和修改密码之前版本： 1GRANT ALL PRIVILEGES ON *.* TO &#96;mike&#96;@&#96;%&#96; IDENTIFIED BY &#39;000000&#39; WITH GRANT OPTION; MySQL 8.0版本中正确的授权语句： 1CREATE USER &#39;mike&#39;@&#39;%&#39; IDENTIFIED BY &#39;000000&#39;;2GRANT ALL ON *.* TO &#39;mike&#39;@&#39;%&#39; WITH GRANT OPTION; 可以看到，创建用户和授权是分开的，不再能一个语句搞定。 修改密码： 1ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;;2FLUSH PRIVILEGES; 也可以使用mysqladmin命令: 1mysqladmin -u root -p password 新密码 还有很多其他内容，作为开发不用完全掌握，可以详见这个博文：MySQL 8.0用户和角色管理 MySQL数据类型数值类型MySQL 支持所有标准 SQL 中的数值类型,其中包括严格数值类型(INTEGER、SMALLINT、DECIMAL 和 NUMERIC),以及近似数值数据类型(FLOAT、REAL 和 DOUBLE PRECISION),并在此基础上做了扩展。扩展后增加了TINYINT、MEDIUMINT 和 BIGINT 这 3 种长度不同的整型,并增加了 BIT 类型,用来存放位数据。 整型数据对于整型数据,MySQL 还支持在类型名称后面的小括号内指定显示宽度,例如 int(5)表示当数值宽度小于 5 位的时候在数字前面填满宽度,如果不显示指定宽度则默认为 int(11)，一般配合 zerofill 使用。 设置了宽度限制后,如果插入大于宽度限制的值,不会对插入的数据有任何影响,还是按照类型的实际精度进行保存。 小数对于小数的表示，MySQL 分为两种方式:浮点数和定点数。浮点数包括 float(单精度)和 double(双精度),而定点数则只有 decimal 一种表示。定点数在 MySQL 内部以字符串形式存放,比浮点数更精确,适合用来表示货币等精度高的数据。 浮点数和定点数都可以用类型名称后加“(M,D)”的方式来进行表示，“(M,D)”表示该值一共显示 M 位数字（整数位 + 小数位），其中 D 位位于小数点后面，M 和 D 又称为精度和标度。 由于float和double分别以 32 位和 64 位存储，所以实际上是可能产生精度丢失的。比如一个 float 类型的列，插入 0.123456789 的时候，实际存储到数据库的只是近似值 0.123457,，当位数更多的时候这个问题更明显，比如分别向 float 和 double 类型的列中插入 987654321.123456789，会得到 987654000 和 987654321.1234568。 所以想要存储高精度、准确的数字，还是需要用 decimal 类型。 日期时间类型MySQL中的事件类型有：DATE、DATETIME、TIMESTAMP、TIME、YEAR 这些数据类型的主要区别如下: 如果要用来表示年月日,通常用 DATE 来表示。 如果要用来表示年月日时分秒,通常用 DATETIME 表示。 如果只用来表示时分秒,通常用 TIME 来表示。 如果需要经常插入或者更新日期为当前系统时间,则通常使用 TIMESTAMP 来表示。TIMESTAMP 值返回后显示为“YYYY-MM-DD HH:MM:SS”格式的字符串,显示宽度固定为 19 个字符。如果想要获得数字值,应在 TIMESTAMP 列添加+0。 如果只是表示年份,可以用 YEAR 来表示,它比 DATE 占用更少的空间。YEAR 有 2 位或4 位格式的年。默认是 4 位格式。在 4 位格式中,允许的值是 19012155 和 0000。在2 位格式中,允许的值是 7069,表示从 1970~2069 年。 MySQL 以 YYYY 格式显示 YEAR值。 字符串类型MySQL 包括了 CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM 和 SET 等多种字符串类型。下面的表详细列出了这些字符类型的比较。 CHAR和VCHAR类型CHAR 和 VARCHAR 很类似,都用来保存 MySQL 中较短的字符串。二者的主要区别在于存储方式的不同：CHAR 列的长度固定为创建表时声明的长度，长度可以为从 0255 的任何值；而 VARCHAR 列中的值为可变长字符串，长度可以指定为065535( 5.0.3以后)之间的值。在检索的时候，CHAR 列删除了尾部的空格，而 VARCHAR 则保留这些空格。 BINARY 和 VARBINARY 类型BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR,不同的是它们包含二进制字符串而不包含非二进制字符串。 ENUM 类型ENUM 中文名称叫枚举类型，它的值范围需要在创建表时通过枚举方式显式指定，对 1255 个成员的枚举需要 1 个字节存储；对于 25565535 个成员，需要 2 个字节存储。最多允许有 65535 个成员。 SET 类型Set 和 ENUM 类型非常类似，也是一个字符串对象，里面可以包含 0~64 个成员。根据成员的不同，存储上也有所不同。 1~8 成员的集合,占 1 个字节。 9~16 成员的集合,占 2 个字节。 17~24 成员的集合,占 3 个字节。 25~32 成员的集合,占 4 个字节。 33~64 成员的集合,占 8 个字节。 其他类型MySQL 8中还支持多种空间类型，应该是用来存储地理信息之类的结构化信息的，但是没找到太多中文资料，平时也没用过，就不误人子弟了。 另外还有新增的 JSON 类型支持。 创建 JSON 字段创建 JSON 类型的字段很简单，跟其他数据结构没什么区别： 1CREATE TABLE t1 (jdoc JSON); 插入 JSON1INSERT INTO t1 VALUES(&#39;&#123;&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;&#125;&#39;); 数据库会对JSON的格式进行校验，如果插入错误的格式会报错。 1mysql&gt; INSERT INTO t1 VALUES(&#39;[1, 2,&#39;);2ERROR 3140 (22032) at line 2: Invalid JSON text:3&quot;Invalid value.&quot; at position 6 in value (or column) &#39;[1, 2,&#39;. 查询 JSON查询 JSON 中的数据用 column-&gt;path 的形式，其中对象类型 path 这样表示 $.path, 而数组类型则是 $[index] 查询 testproject 表 student 字段中 JSON 对象 id 为 1 的记录： 1SELECT * FROM testproject WHERE student-&gt;&#39;$.id&#39;&#x3D; 1; 查询 testproject 表 student 字段中 JSON 对象 id 为 1 或 5 的记录： 1SELECT * FROM testproject WHERE student-&gt;&#39;$.id&#39; in (1,5);2SELECT * FROM testproject WHERE student-&gt;&#39;$.id&#39; &#x3D; 1 or student-&gt;&#39;$.id&#39; &#x3D; 5; 更新数据MySQL 并不支持 column-&gt;path 的形式进行更新操作。如果是整个 JSON 更新的话，和插入时类似的。如果需要更新JSON中的某个值，需要用系统提供的函数： json_array_append：在json后面追加 json_array_insert：在指定下标插入 json_replace：只替换已经存在的旧值，不存在则忽略 json_set：替换旧值，并插入不存在的新值 json_insert：插入新值，但不替换已经存在的旧值 json_remove：删除元素 这部分详细操作可以参考官方文档The JSON Data Type和博客mysql支持原生json使用说明","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"InnoDB 表结构","slug":"InnoDB表结构","date":"2019-11-24T04:12:12.000Z","updated":"2022-02-20T11:27:13.892Z","comments":true,"path":"2019/11/24/InnoDB表结构/","link":"","permalink":"http://beritra.github.com/2019/11/24/InnoDB%E8%A1%A8%E7%BB%93%E6%9E%84/","excerpt":"本文大部分内容摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，小部分来源于自己理解和网络博客，由于书比较旧，网络博客也未必准确，我尽量对文中内容进行验证和对比 MySQL 官网文档，但仍然有可能有疏漏。","text":"本文大部分内容摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，小部分来源于自己理解和网络博客，由于书比较旧，网络博客也未必准确，我尽量对文中内容进行验证和对比 MySQL 官网文档，但仍然有可能有疏漏。 索引组织表在 InnoDB 存储引擎中，表都是根据主键顺序组织存放的，这种存储方式的表成为索引组织表（Index Organized table）。在 InnoDB 存储引擎表中，每张表都有个主键（Primary Key），如果在创建表的时候没有显式地定义主键，则 InnoDB 会按照以下方式选择或创建主键： 首次按判断表中是否有非空的唯一索引（Unique NOT NULL），如果有，则该列为主键。 如果没有，InnoDB 存储引擎会自动创建一个 6 字节大小的指针。 当表中有多个非空唯一索引的时候，InnoDB 存储引擎将选择建表的时候第一个定义的非空唯一索引作为主键。注意这里是定义索引的顺序，而不是建表的时候列的顺序。 InnoDB 逻辑存储结构从 InnoDB 存储应引擎的存储结构来看，所有的数据都被逻辑的存放在一个空间中，成为表空间（tablespace）。表空间又由段（segment）、区（extent）、页（page）组成。页在一些文档中也成为块（block），InnoDB存储引擎的逻辑存储结构大致如图： 表空间表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。默认情况下 InnoDB 存储引擎有一个共享表空间 ibdata1，所有的数据都存放在这个表空间中。如果用户开启了参数innodb_fiule_per_table，则每张表内的数据可以单独放在一个表空间中。 即使启用了innodb_fiule_per_table，每张表的表空间内从存放的也只是数据、索引和插入缓冲Bitmap页，其他类型的数据比如回滚（undo）、插入缓冲索引页、系统事务信息、二次写缓冲（Double write buffer）等还是存放在原有共享表空间。 段上面的图已经显示了表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等。因为前面已经介绍过了 InnoDB 存储引擎表是索引组织（index organized）的，因此数据即索引，索引即数据。那么数据段即 B+ 树的叶子节点（图中的 Leaf node segment），索引段即 B+ 树中的非索引节点（图中的Non-leaf node segment）。回滚段较为特殊。 区区是由连续的页组成的空间，在任何情况下，每个区的大小都为 1MB。为了保证区中页的连续性，InnoDB 存储引擎一次从磁盘中申请 4~5 个区。在默认情况下，InnoDB 存储引擎页的大小为 16KB，即一个区中一共有 64 个连续的页。 InnoDB 1.0.x 版本开始引入压缩页，即每个夜的大小可以通过KEY_BLOCK_SIZE设置为 2K、4K、8K，因此每个区对应页的数量就应该是 512、256、128。 InnoDB 1.2.x 版本新增了参数 innodb_page_size，通过该参数可以将默认页的大小设置为 4K、8K，但是页中的数据库不是压缩。这是区中页的数量同样页是 256、128。总之，不论页的大小怎么变化，区的大小总为 1M。 页同大多数数据库一样，InnoDB 有页（Page）的概念（也可以称之为块），页是 InnoDB 磁盘管理的最小单位。 在 InnoDB 存储引擎中，常见的页类型有： 数据页（B-tree Node） undo 页（undo Log Page） 系统页（System Page） 事务数据页（Transaction system Page） 插入缓冲位图页（Insert Buffer Bitmap） 插入缓冲空闲列表页（Inser Buffer Free List） 未压缩的二进制大对象页（Uncompressed BLOB Page） 压缩的二进制大对象页（compressed BLOB Page） 行InnoDB 存储引擎是面向列的（row-oriented），也就是说数据是按行进行存放的。每个页存放的行记录也是有硬性规定的，最多允许存放16KB/2-200行记录，即7992行。 InnoDB 行记录格式InnoDB 引擎和大多数数据库一样，记录以行的形式存储。这意味这页中保存着表中一行行的数据。在 InnoDB 1.0.x 版本之前，InnoDB 存储引擎提供了 Compact 和 Redundant 两种格式来存放行记录数据，这也是目前使用最多的一种格式。Redundant 格式是为了兼容之前版本而保留的，MySQL 5.1 之后的版本默认为 Compact 行格式。可以使用命令SHOW TABLE STATUS LIKE &#39;table_name&#39;来查看当前表使用的行格式。（由于书比较旧，我在 MySQL 官网上看到其实不止这两种格式，而默认的格式为 DYNAMIC，具体如下： Compact 行记录格式Compact 行记录格式的设计目标是高效的存储数据，简单来说就是一个页中存放的行数据越多，性能就越高。下图是详细的存储格式： 可以看到，Compact 行记录的格式首部是一个非 NULL 变长字段长度列表，并且是按照列的顺序逆序放置的，长度为： 若列的长度小于 255 字节，则用 1 字节表示； 若大于 255 个字节，用 2 字节表示。 变长字段的长度不可以超过两个字节，因为 VARCHAR 类型的最大长度限制为 65535。变长字段之后的第二个部分是 NULL 标志位，该位指示了该位数据中是否有 NULL 值，有则用 1 表示。接下来的部分是记录头信息（record header），固定占用 5 字节（40位），每位的含义如下： 最后的部分就是实际存储每个列的数据。需要特别注意的是，NULL 不占用该部分任何空间，即 NULL 除了占用 NULL 标志位之外，实际存储不占用空间。另外，每行数据除了用户定义的之外，还有两个隐藏列，事务ID列和回归指针列，分别是6和7字节大小。如果 InnoDB 表没有定义主键，还会增加一个6字节的 rowid 列。 行溢出数据InnoDB 存储引擎可以将一条记录中的某些数据存储在真正的数据页面之外。一般认为 BLOB 、 LOB 这类的大对象列类型的存储会把数据放在数据页面之外。但是不是这样的，BLOB 可以不放在溢出页面，而且即便 VARCHAR 这种类型也可能被存放为行溢出数据。 MySQL 数据库的 VARCHAR 类型理论上可以存放 65535 字节，实际上创建一个 65535 长度的列的时候，会得到错误信息： 1[42000][1118] Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. This includes storage overhead, check the manual. You have to change some columns to TEXT or BLOBs. 由于还有别的开销，经过实际测试，能存放 VARCHAR 类型的最大长度是65532。 这里需要注意，65535 是说的字节数，如果你是用的是多字节的字符集，比如我用utf8mb4，没个字占用4个字节，创建会提示[42000][1074] Column length too big for column &#39;varchars&#39; (max = 16383); use BLOB or TEXT instead.，即varchar字段已经被限制到了16383的长度。 此外，还要注意，MySQL 官方手册中定义的65535长度是指所有 VARCHAR 列的长度综合，如果综合超过了这个长度，依然无法创建，同样是上面那个错误提示。 即便能够存储65535个字节，但是，InnoDB 引擎的页为 16KB，即16384字节，怎么能存放65535个字节呐？因此，在一般情况下，InnoDB引擎存储的数据都是放在页类型为B-tree node中。但是当发生行溢出的时候，数据存放在页类型为 Uncompress BLOB 页中。 通过工具可以看到，数据也只保存 VARCHAR 的前768的前缀（prefix）数据，之后都是偏移量，指向行溢出页，也就是 Uncompressed BLOB Page。因此，对于行溢出数据，存放采用下图的方式： 那么新的问题又来了，多长的 VARCHAR 是保存在单个数据页中的，多长开始会保存在 BLOB 中？ InnoDB存储引擎表是索引组织的，即 B+Tree 结构，这样每个页中至少应该有两条记录（不然就退化成链表了）。因此，如果页中只能存下一条记录，那么 InnoDB 引擎就会自动将行数据放到溢出页中。 经过测试，这个数字是 8098 ，如果少于这个长度，一个页中就可以放入至少两行数据，VARCHAR 类型的行数据就不会被放到 BLOB 页中去。 另一个问题，对于 TEXT 和 BLOB 数据类型，他们也不是永远存放在 Uncompressed BLOB Page 中的，跟 VARCHAR 类似，至少保证一个页中能存放两条记录。当然一般 BLOB 不会这么小，大多数情况下还是会发生行溢出，数据页只保存前 768 字节，实际数据还是存在 BLOB 页中。 InnoDB数据页结构页是 InnoDB 存储引擎管理数据库的最小磁盘单位。也类型为 B-tree Node 的页存放的即是表中行的实际数据了。 InnoDB 数据页由以下7个部分组成，如图所示： File Header（文件头） Page Header（页头） Infimun 和 Supremum Records User Records（用户记录，即行记录） Free Space（空闲空间） Page Directory（页目录） File Trailer（文件结尾信息） 其中 File Header、Page Header、File Trailer 的大小是固定的，分别为 38、56、8 字节，这些空间用来标记也得一些信息，如 Checksum，数据页所以在的 B+ 树索引的层数等。User Records、Free Space、Page Dirctory 这些部分为实际的行记录存储空间，因此大小是动态的。 约束约束完整性关系型数据库和文件系统的一个不同点是，关系数据库本身能够保证存储数据的完整性，不需要应用程序的控制，而文件系统一般都需要在程序端进行控制。当前几乎所有的关系型数据库都提供了约束（constraint）机制，来保证数据的完整性。 对 InnoDB 存储引擎本身而言，提供了以下几种约束： Primary Key Unique Key Foreign Key Default NOT NULL 约束的创建和查找约束的创建可以用以下两种方式： 表建立的时候进行约束定义 利用ALTER TABLE命令来创建约束 对于 Unique Key （唯一索引）的约束，用户可以通过命令CREATE UNIQUE INDEX来建立。对于主键约束而言，默认约束名为 PRIMARY 。 约束和索引的区别创建约束的方法通常就是创建索引的方法，的确，当用户创建了一个唯一索引就创建了一个唯一约束。但是约束和索引的概念还是有所不同的，约束更是一个逻辑的概念，用来保证数据的完成性，而索引是一个数据结构，既有逻辑上的概念，在数据库中还代表着物理存储的方式。 对错误数据的约束在某些设置下，MySQL 允许非法的或不正确的数据的插入或更新，又或者可以在数据库内部将其转化为一个合法的值，如向 NOT NULL 的字段中插入一个 NULL值，MySQL 数据库会将其改为0再插入，因此数据库本身没有对数据的正确性进行约束。而通过设置 sql_mode参数，MySQL 数据库又可以堆输入的合法值进行约束。详细设置可以参考MySQL官方手册中对 sql_mode 的说明。 外键约束MyISAM 存储引擎本身不支持外键，而 InnoDB 则完整支持外键约束。现在有一定规模的项目都会不建议甚至强制不允许使用外键，具体原因可以再专门探讨，这里就不详细说明外键的使用方法和原理了，只记录几个知识要点。 可以定义的字表操作： CASCADE：父表变化时子表与父表进行一样的操作 SET NULL：父表变化时子表设为 NULL NO ACTION：父表抛出错误，阻止操作 RESTRICT：同上, 都是立即检查外键约束 数据库默认的设置是 RESTRICT。 视图在MySQL数据中，视图（View）是一个命名的虚表，它由一个 SQL 查询来定义，可以当做表使用。与持久表（permanent table）不同的是，视图中的数据没有实际的物理存储。 视图的作用视图的主要用途之一是被用作一个抽象装置，特别是对于一些应用程序，程序本身不需要关心基表（base table）的结构，只需要按照视图定义来取数据或者更新数据，因此，视图层在一定程度上起到一个安全层的作用。 创建视图的语法如下： 1CREATE2 [OR REPLACE]3 [ALGORITHM &#x3D; &#123;UNDEFINED | MERGE | TEMPTABLE&#125;]4 [DEFINER &#x3D; user]5 [SQL SECURITY &#123; DEFINER | INVOKER &#125;]6 VIEW view_name [(column_list)]7 AS select_statement8 [WITH [CASCADED | LOCAL] CHECK OPTION] 虽然视图是基于基表的虚拟表，但是用户可以对某些视图进行更新操作，其本质就是通过视图的定义来更新基表。 一般称可以进行更新操作的视图成为可更新视图（updatable view）。视图定义中的 WITH CHECK OPTION 就是针对于可更新的视图的，即更新的值是否需要检查。 分区表分区概述分区功能并不是在存储引擎层完成的，因此不是只有 InnoDB 存储引擎支持分区，常见的存储引擎 MyISAM、NDB 等都支持。但也并不是所有的存储引擎都支持，如 CSV、FEDORATED、MERGE 等就不支持。在使用分区功能前，应该对选择的存储引擎对分区的支持有所了解。 MySQL 数据库在5.1版本时就添加了对分区的支持。分区的过程是将一个表或索引分解为多个更小、更可管理的部分。就访问数据库的应用而言，从逻辑上讲，只有一个表或一个索引，但是在物理上，这个表或索引可能由数十个物理分区组成。每个分区都是独立的对象，可以独自处理，也可以作为一个更大对象的一部分进行处理。 MySQL 数据库支持的分区类型为水平分区，并不支持垂直分区。此外，MySQL 数据的分区是局部分区索引，一个分区中即存放了数据又存放了索引。而全局分区是指，数据存放在各个分区中，但是所有数据的索引放在一个对象中。目前，MySQL 数据库还不支持全局分区。 MySQL 数据库支持以下几种分区： RANGE 分区：行数据基于属于一个给定连续区间的列值被放入分区。MySQL 5.5 开始支持 RANGE COLUMNS 的分区。 LIST 分区：和 RANGE 分区类似，只是 LIST 分区面向的是离散的值。 MySQL 5.5 开始支持 LIST COLUMNS 的分区。 HASH 分区：根据用户自定义的表达式的返回值来进行分区，返回值不能为负数。 KEY 分区：根据 MySQL 数据库提供的哈希函数来进行分区。 无论创建何种类型的分区，如果分区表中存在主键或唯一索引时，分区列必须是唯一索引的一个组成部分。 唯一索引是允许 NULL 值的，并且分区列只要是唯一索引的一个组成部分，不需要是整个唯一索引列都是分区列。 另外，如果建表时没有指定主键，唯一索引，可以指定任何一个列为分区列。 分区类型RANGE 分区RANGE分区是最常用的一种分区。下面的CREATE TABLE语句创建了一个 id 列的区间分区表。当 id 小于10时，数据插入 p0 分区。当 id 等于10 小于 20时，数据插入 p1 分区。 1CREATE TABLE t&#123;2id INT3&#125;ENGINE&#x3D;INNODB4PARTITION BY RANGE(id)&#123;5PARTITION p0 VALUES LESS THAN(10),6PARTITION p1 VALUES LESS THAN(20));7&#125; 这时候查看磁盘上的物理文件，启用分区表之后，文件不再是由一个 ibd 文件组成了，而是由建立分区时的各个分区 ibd 文件组成。 可以通过查询information_cheme架构下的 PARATITIONS 表来查看每个分区的具体信息： 1SELECT * FROM information_scheme.PARTITIONS where table_scheme&#x3D;database() and table_name&#x3D;&#39;t&#39;\\G; RANGE 分区主要用于日期列的分区，例如销售类的表，按年进行分区存放销售记录。这样的好处就是，便于对表进行管理。比如要删除某一年的数据，不需要在 where 里面写大段的时间参数，只需删除对应年份所在的分区即可。另一个好处就是可以加快某些查询操作，比如查询某一年的销售额，使用预先设定好的分区可以加快查询速度。 LIST 分区LIST 分区个 RANGE 分区非常相似，只是分区列的值是离散的，而非连续的。比如： 1CREATE TABLE t(2a INT,3b INT)ENGINE&#x3D;INNODB4PARTITION BY LIST(b)(5PARTITION p0 VALUES IN(1,3,5,7,9),6PARTITION p1 VALUES IN(0,2,4,6,8)7); 不同于 RANGE 分区中定义的VALUES LESS TAHN语句，LIST 分区使用 VALUES IN。因为每个分区的值是离散的，因此只能定义值。 HASH 分区HASH分区的目的是将数据均匀的分不到预先定义的各个分区中，保证各个分区的数据量大致都一样的。在 RANGE 和 LIST 分区中，必须明确指定一个给定的列值或列值集合应该保存在哪个分区中；而在 HASH 分区中，MySQL 自动完成这些工作，用户需要做的只是基于将要进行哈希分区的列值指定一个列值或者表达式，以及指定被分区的表将要被分割的分区数量。 要使用 HASH 分区来分割一个表，要在 CREATE TABLE 语句上添加一个PARTITION BY HASH（expr）子句，其中expr是一个返回一个整数的表达式。它可以仅仅是字段类型为 MySQL 整型的列名。此外，用户很可能需要在后面再添加一个PARTITIONS num子句，其中 num 是一个非负的整数，它表示将要被分割成分区的数量。如果没有包含一个 PARTITIONS 字句，那么分区数量将默认为1。 下面的例子创建了一个 HASH 分区的表 t，分区按日期列 b 进行： 1CREATE TABLE t_hash(2a INT,3b DATETIME)ENGINE&#x3D;INNODB4PARTITION BY HASH(YEAR(b))5PARTITIONS 4; MySQL 数据库还支持一种成为 LINEAR HASH的分区，它使用一个更加复杂的算法来确定新行插入到已经分区的表中的位置。它的语法和 HASH 分区的语法相似，只是将关键字 HASH 改为 LINEAR HASH。下面创建的是一个 LINEAR HASH 的分区表： 1CREATE TABLE t_linear_hash(2a INT,3b DATETIME)ENGINE&#x3D;INNODB4PARTITION BY LINEAR HASH(YEAR(b))5PARTITIONS 4; LINEAR HASH 分区的优点在于，增加、删除、合并和拆分分区将变得更加快捷，这有利于处理含有大量数据库的表。缺点在于，与使用 HASH 分区得到的数据分布相比，各个分区间数据的分布可能不太均衡。 KEY 分区KEY 分区和 HASH 分区相似，不同之处在于 HASH 分区使用用户定义的函数进行分区，KEY 分区使用 MySQL 数据库提供的函数进行分区，对于 NDB Cluster 引擎，MySQL 数据库使用 MD5 函数来分区；对于其他存储引擎， MySQL 数据库使用其内部的哈希函数，这些函数基于与 PASSWORD() 一样的运算法则。 COLUMNS 分区前面介绍的4种分区，分区得条件是：数据必须是整形（integer），如果不是整型，那应该需要通过函数化为整型。MySQL 5.5 版本开始支持 COLUMNS 分区，可以视为 RANGE 分区和 LIST 分区的一种进化。COLUMNS 分区可以直接使用非整形的数据进行分区，分区根据类型直接比较而得，不需要转化为整形。此外 RANGE COLUMNS 分区可以对多个列的值进行分区。 COLUMNS 分区支持以下的数据类型： 所有整形类型，如 INT、SMALLINT、TINYINT、BIGINT。FLOAT 和 DECIMAL 则不予支持。 日期类型，如 DATE 和 DATETIME。其余的日期类型则不予支持。 字符串类型，如 CHAR、VARCHAR、BINARY 和 VARBINARY。BLOB 和 TEXT 类型不予支持。 对于日期类型的分区，我们不再需要YEAR()和TO_DODAY()函数了，而可以直接使用 COLUMNS。 子分区子分区（subpartitioning）是在分区的基础上在进行分区，有时也称这种分区为复合分区（composite partitioning）。MySQL 数据库允许在 RANGE 和 LIST 的分区上在进行 HASH 和 KEY 的子分区。 但是子分区的建立需要注意以下几个问题： 每个子分区的数量必须相同。 要在一个分区表的任何分区上使用 SUBPARTITION 来明确定义任何子分区，就必须定义所有的子分区。 每个 SUBPARTITION 子句必须包括子分区的一个名字。 子分区的名字必须是唯一的。 分区中的NULL值MySQL数据库允许堆 NULL 值进行分区，但是处理方法和其他数据库可能完全不同。MySQL 数据库的分区总是视 NULL 值小于任何一个非 NULL 值，这和 MySQL 数据库中处理 NULL 值的ORDER BY操作是一样的。因此对于不同的分区类型，MySQL 数据库对于 NULL 值的处理也是各不相同。 对于 RANGE 分区，如果向分区中插入了 NULL 值，则 MySQL 数据库会将该值放入最左边的分区。 LIST 分区必须显式指出向哪个分区中放入 NULL 值，否则会报错。 HASH 分区和 KEY 分区跟上面又不相同，任何分区函数都会将含有 NULL 值的记录返回为0。 分区和性能在合理使用分区之前，必须了解分区的使用环境。 数据库的应用分为两类：一类是 OLTP（在线事务处理），如 Blog、电子商务、网络游戏等；另一类是 OLAP（在线分析处理），如数据仓库、数据集市。 对于 OLAP 的应用，分区的确是能够很好地提高查询性能，因为 OLAP 应用大多数查询需要频繁的扫描一张很大的表。 然而对于 OLTP 的应用，分区应该非常小心。在这种应用下，通常不可能获取一张表中超过 10%，大部分通过索引返回几条记录即可。而根据 B+ 树索引的原理，一般的 B+ 树需要2~3次的磁盘 IO ，因此 B+ 树已经可以很好地完成操作，不需要分区的帮助，而且设计不好的分区会带来严重的性能问题。 在表和分区间交换数据在MySQL 5.6 开始支持ALTER TABLE ... EXCHANGE PARTITION语法。该语句允许分区或子分区中的数据与另一个非分区的表中的数据进行交换。如果非分区表中的数据为空，那么相当于将分区中的数据移动到非分区表中。若分区表中的数据为空，则相当于将外部表中的数据导入到分区中。 要使用ALTER TABLE ... EXCHANGE PARTITION语句，必须满足： 要交换的表和分区表有相同的结构，但是不能含有分区 在非分区表中的数据必须在交换的分区定义中 被交换的表中不能含有外键，或者其他的表含有对该表的外键引用 用户除了需要 ALTER、INSERT 和 CREATE 权限外，还需要 DROP 的权限 此外，还有两个小的细节需要注意： 使用该语句的时候，不会触发交换表和被交换表上的触发器 AUTO_INCREMENT 列将被重置 参考文章： InnoDB逻辑存储结构","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"InnoDB","slug":"InnoDB","permalink":"http://beritra.github.com/tags/InnoDB/"}]},{"title":"MySQL原理和优化","slug":"MySQL原理和优化","date":"2019-10-01T04:12:12.000Z","updated":"2022-02-20T11:27:13.888Z","comments":true,"path":"2019/10/01/MySQL原理和优化/","link":"","permalink":"http://beritra.github.com/2019/10/01/MySQL%E5%8E%9F%E7%90%86%E5%92%8C%E4%BC%98%E5%8C%96/","excerpt":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。注意，该书内容是基于MySQL 5.6 及更早的版本，具体的实现方式有可能发生了改变，有些内容我做了注释和补充，但是不保证百分百的完整和正确。","text":"本文主要摘抄自《MySQL技术内幕（InnoDB存储引擎）第二版》，少部分来自网络博客和自己补充。注意，该书内容是基于MySQL 5.6 及更早的版本，具体的实现方式有可能发生了改变，有些内容我做了注释和补充，但是不保证百分百的完整和正确。 存储引擎MySQL 5.0 支持的存储引擎包括 MyISAM、 InnoDB、 BDB、 MEMORY、 MERGE、 EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED 等，其中 InnoDB 和 BDB 提供事务安全表，其他存储引擎都是非事务安全表。 使用show engines命令可以查看支持的存储引擎。我在 MySQL 8.0 中执行命令得到的结果如下： 一般使用的都是 InnoDB 引擎，所以主要还是研究 InnoDB 的特性和原理。 自动增长列InnoDB 表的自动增长列可以手工插入，但是插入的值如果是空或者 0，则实际插入的将是自动增长后的值。如果插入一个大于当前自增id的数的时候，自增id会变成这个数字，中间的就被跳过了。 可以通过ALTER TABLE *** AUTO_INCREMENT = n;语句强制设置自动增长列的初识值，默认从 1 开始，但是该强制的默认值是保留在内存中的，如果该值在使用之前数据库重新启动，那么这个强制的默认值就会丢失，就需要在数据库启动以后重新设置。 对于 InnoDB 表,自动增长列必须是索引。如果是组合索引,也必须是组合索引的第一列。 外键约束MySQL 支持外键的存储引擎只有 InnoDB，在创建外键的时候,要求父表必须有对应的索引，子表在创建外键的时候也会自动创建对应的索引。 在创建索引的时候，可以指定在删除、更新父表时，对子表进行的相应操作，包括 RESTRICT、CASCADE、SET NULL 和 NO ACTION。其中 RESTRICT 和 NO ACTION 相同，是指限制在子表有关联记录的情况下父表不能更新； CASCADE 表示父表在更新或者删除时，更新或者删除子表对应记录；SET NULL 则表示父表在更新或者删除的时候，子表的对应字段被 SET NULL。选择后两种方式的时候要谨慎，可能会因为错误的操作导致数据的丢失。 《阿里巴巴Java规范》中强制要求不能使用外键，所有表之间的关联逻辑应该在业务逻辑中实现。 存储方式在 8.0 里面，InnoDB 把数据字典进行重构，大家应该知道数据字典是什么，就是表结构，你的用户定义，所有的跟 DDL 相关放到数据字典里面去。在 8.0 之前，数据字典有两份，一份是存储在 .frm 文件里，另一份是 InnoDB 的数据表里。在8.0之后 .frm 被干掉了，只保留了 InnoDB 中的数据。8.0 中数据库的 innodb 表全部放至在 datadir 下的 mysql.ibd 中，不再把表结构放在 .frm 文件中，而是存放在元数据表中。 索引索引概述所有 MySQL 列类型都可以被索引，对相关列使用索引是提高 SELECT 操作性能的最佳途径。根据存储引擎可以定义每个表的最大索引数和最大索引长度，每种存储引擎(如 MyISAM、InnoDB、BDB、MEMORY 等)对每个表至少支持 16 个索引，总索引长度至少为 256 字节。大多数存储引擎有更高的限制。 InnoDB存储引擎支持以下几种常见的索引： B+ 树索引 全文索引 哈希索引 B+树索引就是传统意义上的索引，这是目前关系型数据库系统中查找最为常用和最有效的索引。B+树索引的构造类似与二叉树，根据键值（Key Value）快速找到数据。 设计索引的原则和技巧 搜索的索引列，不一定是所要选择的列。换句话说，最适合索引的列是出现在 WHERE子句中的列，或连接子句中指定的列，而不是出现在 SELECT 关键字后的选择列表中的列。 使用惟一索引。考虑某列中值的分布。索引的列的基数越大，索引的效果越好。例如，存放出生日期的列具有不同值，很容易区分各行。而用来记录性别的列，只含有“ M”和“F”，则对此列进行索引没有多大用处，因为不管搜索哪个值，都会得出大约一半的行。 使用短索引。如果对字符串列进行索引，应该指定一个前缀长度，只要有可能就应该这样做。 利用最左前缀。在创建一个 n 列的索引时，实际是创建了 MySQL 可利用的 n 个索引。多列索引可起几个索引的作用，因为可利用索引中最左边的列集来匹配行。这样的列集称为最左前缀。另外，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任调整。 不要过度索引。不要以为索引“越多越好”，什么东西都用索引是错误的。 对于 InnoDB 存储引擎的表，记录默认会按照一定的顺序保存，如果有明确定义的主键,则按照主键顺序保存。如果没有主键，但是有唯一索引，那么就是按照唯一索引的顺序保存。如果既没有主键又没有唯一索引，那么表中会自动生成一个内部列，按照这个列的顺序保存。 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 = 和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 索引的操作创建索引的详细定义如下： 1CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX index_name2 [index_type]3 ON tbl_name (key_part,...)4 [index_option]5 [algorithm_option | lock_option] ...67key_part: &#123;col_name [(length)] | (expr)&#125; [ASC | DESC]89index_option:10 KEY_BLOCK_SIZE [&#x3D;] value11 | index_type12 | WITH PARSER parser_name13 | COMMENT &#39;string&#39;14 | &#123;VISIBLE | INVISIBLE&#125;1516index_type:17 USING &#123;BTREE | HASH&#125;1819algorithm_option:20 ALGORITHM [&#x3D;] &#123;DEFAULT | INPLACE | COPY&#125;2122lock_option:23 LOCK [&#x3D;] &#123;DEFAULT | NONE | SHARED | EXCLUSIVE&#125; 索引的删除语法为： 1DROP INDEX index_name ON tbl_name B+树索引的原理关于 B 树和 B+ 树的结构，可以参考这篇博文：B树和B+树的插入、删除图文详解 B+ 树相比 B 树，优点包括： B+ 树的层级更少：相较于 B 树 B+ 每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快； B+ 树查询速度更稳定：B+ 所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比 B 树更稳定; B+ 树天然具备排序功能：B+ 树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比 B 树高。 B+ 树全节点遍历更快：B+ 树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像 B 树一样需要对每一层进行遍历，这有利于数据库做全表扫描。 而 B 树相对于 B+ 树的优点是，如果经常访问的数据离根节点很近，而 B 树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比 B+ 树快。 聚簇索引 &amp; 非聚簇索引聚集索引与非聚集索引的区别是：叶节点是否存放一整行记录 InnoDB 主键使用的是聚簇索引，MyISAM 不管是主键索引，还是二级索引使用的都是非聚簇索引。 下图形象说明了聚簇索引表(InnoDB)和非聚簇索引(MyISAM)的区别： 对于非聚簇索引表来说（右图），表数据和索引是分成两部分存储的，主键索引和二级索引存储上没有任何区别。使用的是 B+ 树作为索引的存储结构，所有的节点都是索引，叶子节点存储的是索引 + 索引对应的记录的数据。 对于聚簇索引表来说（左图），表数据是和主键一起存储的，主键索引的叶结点存储行数据（包含了主键值），二级索引的叶结点存储行的主键值。使用的是 B+ 树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录（数据内容）。 聚簇索引的优点 当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。 当通过聚簇索引查找目标数据时理论上比非聚簇索引要快，因为非聚簇索引定位到对应主键时还要多一次目标记录寻址,即多一次 I/O。 使用覆盖索引扫描的查询可以直接使用页节点中的主键值。 聚簇索引的缺点 插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于 InnoDB 表，我们一般都会定义一个自增的 ID 列为主键。 更新主键的代价很高，因为将会导致被更新的行移动。因此，对于 InnoDB 表，我们一般定义主键为不可更新。 二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。二级索引的叶节点存储的是主键值，而不是行指针（非聚簇索引存储的是指针或者说是地址），这是为了减少当出现行移动或数据页分裂时二级索引的维护工作，但会让二级索引占用更多的空间。 采用聚簇索引插入新值比采用非聚簇索引插入新值的速度要慢很多，因为插入要保证主键不能重复，判断主键不能重复，采用的方式在不同的索引下面会有很大的性能差距，聚簇索引遍历所有的叶子节点，非聚簇索引也判断所有的叶子节点，但是聚簇索引的叶子节点除了带有主键还有记录值，记录的大小往往比主键要大的多。这样就会导致聚簇索引在判定新记录携带的主键是否重复时进行昂贵的 I/O 代价。 联合索引联合索引是指对表上多个列进行索引，创建方法和单个索引一样，不同之处在于有多个索引列。 联合索引比对每个列分别建索引更有优势，因为索引建立得越多就越占磁盘空间，在更新数据的时候速度会更慢。另外建立多列索引时，顺序也是需要注意的，应该将严格的索引放在前面，这样筛选的力度会更大，效率更高。 联合索引的优势： 避免回表 在执行计划中，table access by index rowid代表是回表动作。如在 user 的 id 列建有索引，select id from user这个不用回表，直接从索引中读取id的值，而select id,name from user中，不能返回除 id 列其他的值，所以必须要回表。如果建有了 id 和 name 列的联合索引，则可以避免回表。 另外，建立了 id 和 name 的联合索引(id列在在前)，则select id from user可以避免回表，而不用单独建立 id 列的单列索引 两个单列查询返回行较多，同时查返回行较少，联合索引更高效。 如果 select * from user where id=2 和 select * from user where name=&#39;tom&#39; 各自返回的行数比较多，而 select * from user where id=2 and name=&#39;tom&#39; 返回的行数比较少，那么这个时候使用联合索引更加高效。 索引优化和最佳实践先来个偷来的大长图： 一些其他的技巧 通常来说简单的把可为 NUL 的列改为 NOT NULL 不会对性能提升有多少帮助，只是如果计划在列上创建索引，就应该将该列设置为 NOT NULL。 对整数类型指定宽度，比如 INT(11)，没有任何卵用。INT 使用32位（4个字节）存储空间，那么它的表示范围已经确定，所以 INT(1) 和 INT(20) 对于存储和计算是相同的。 UNSIGNED 表示不允许负值，大致可以使正数的上限提高一倍。 大表 ALTER TABLE 非常耗时，MySQL 执行大部分修改表结果操作的方法是用新的结构创建一个张空表，从旧表中查出所有的数据插入新表，然后再删除旧表。 一些不走索引的情况：MySQL 内部优化器会对 SQL 语句进行优化，如果优化器估计使用全表扫描要比使用索引快，则不使用索引。所以实际上不走索引的情况可能非常多，很难一一列举。以下是一些比较典型的情况。 非独立的列“独立的列”是指索引列不能是表达式的一部分，也不能是函数的参数。比如： 1select * from where id + 1 &#x3D; 5 我们很容易看出其等价于 id = 4，但是 MySQL 无法自动解析这个表达式，使用函数是同样的道理。 另外在查询条件里面用函数计算的话，也是一样的。 模糊查询当然不是全部模糊查询都不走索引，如果查询条件前半部分是确定的，就会走索引，即like &#39;xxx%&#39;的情况。 1select * from compare_result where request_id like &#39;%0173f%&#39;;#不走索引2select * from compare_result where request_id like &#39;0173f%&#39;;#走索引 不等于 类似where id !=2或者where id &lt;&gt; 2都不会走索引，尽量使用 UNION 关键字进行表述。 范围查询如果是连续的范围，优先用 between 而不是 in。 最后又偷了一个别人的图： 参考文章： MySQL索引原理及慢查询优化 MySQL优化/面试，看这一篇就够了","categories":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"优化","slug":"优化","permalink":"http://beritra.github.com/tags/%E4%BC%98%E5%8C%96/"}]},{"title":"Keepalived+MySQL双主高可用配置","slug":"离线环境下Keepalived+MySQL 8.0安装和双主高可用配置","date":"2019-01-01T04:12:12.000Z","updated":"2022-02-20T11:20:43.711Z","comments":true,"path":"2019/01/01/离线环境下Keepalived+MySQL 8.0安装和双主高可用配置/","link":"","permalink":"http://beritra.github.com/2019/01/01/%E7%A6%BB%E7%BA%BF%E7%8E%AF%E5%A2%83%E4%B8%8BKeepalived+MySQL%208.0%E5%AE%89%E8%A3%85%E5%92%8C%E5%8F%8C%E4%B8%BB%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE/","excerpt":"记录一个Keepalived+MySQL 双主高可用配置的配置过程。","text":"记录一个Keepalived+MySQL 双主高可用配置的配置过程。 系统环境准备两台服务器IP： 10.17.55.201主要的 10.17.55.202次要的 两台系统版本均为 CentOS Linux release 7.4.1708 (Core) MySQL官网上下载mysql-8.0.18-1.el7.x86_64.rpm-bundle.tar，即适用于CentOS的MySQL 8.0.18 组件安装CentOS可能带有默认的Mariadb，如果不想用的话可以执行rpm -qa|grep mariadb找到所有安装包，然后依次执行rpm -r 每个文件名卸载Mariadb。 然后从官网上下载相应的MySQL安装文件。 安装顺序为 1rpm -ivh mysql-community-common-5.7.17-1.el7.x86_64.rpm 2rpm -ivh mysql-community-libs-5.7.17-1.el7.x86_64.rpm 3rpm -ivh mysql-community-client-5.7.17-1.el7.x86_64.rpm 4rpm -ivh mysql-community-server-5.7.17-1.el7.x86_64.rpm 5rpm -ivh mysql-community-devel-5.7.17-1.el7.x86_64.rpm 安装最后一个的时候有可能出现依赖openssl的问题，重新安装openssl无效，输入参数-e --nodeps可以解决，暂不清楚原理 数据库配置先从主备模式开始进行配置，方便排错。 主实例修改配置文件/etc/my.cnf 1[mysqld]2log-bin&#x3D;mysql-bin #日志名称3server-id&#x3D;1 #数据库id,两个实例不能一样 然后创建用户，用于数据同步，MySQL 8.0之后的用户创建方式和之前不一样 1CREATE USER &#39;repl&#39;@&#39;10.17.55.202&#39; IDENTIFIED WITH mysql_native_password BY &#39;1&#39;;2GRANT REPLICATION SLAVE ON *.* TO &#39;repl&#39;@&#39;10.17.55.202&#39;; 然后刷新权限 1flush privileges; 查看主节点的binary log文件名和位置： 1show master status; 记录下来File和Position两个字段，下面要用。 备用实例在从节点上设置参数： 1CHANGE MASTER TO MASTER_HOST&#x3D;&#39;10.17.55.201&#39;,MASTER_USER&#x3D;&#39;repl&#39;,MASTER_PASSWORD&#x3D;&#39;1&#39;,MASTER_LOG_FILE&#x3D;&#39;mysql-bin.000002&#39;,MASTER_LOG_POS&#x3D;1141; 然后启动从节点start slave，并且查看节点状态show slave status\\G 如果看到节点的状态信息中包含一下的两条，证明连接成功。 1Slave_IO_State: Waiting for master to send event2Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates 然后验证一下： 切换回主节点，新建一个数据库或者表，从节点中应该同步添加了。 双主环境以上搭建的是MySQL主备模式，双主模式其实一样，只不过两者之间互为主备。 同样的，把之前的备用节点当做主节点，之前的主节点当做备用节点，再来一遍。不要忘记修改my.cnf。 配置完成之后继续新建一个数据库或者表，测试是否同步。 Keepalived安装和配置CentOS上直接可以用yum安装Keepalived，省了不少事情，不过可能遇到错误 1Error: Package: 1:net-snmp-agent-libs-5.7.2-43.el7.x86_64 (base)2 Requires: libmysqlclient.so.18()(64bit)3Error: Package: 1:net-snmp-agent-libs-5.7.2-43.el7.x86_64 (base)4 Requires: libmysqlclient.so.18(libmysqlclient_18)(64bit) 安装MySQL安装包中的libs-compat即可 然后对keepalived进行配置，最简配置如下： 主节点： 1! Configuration File for keepalived23global_defs &#123;4 router_id master5&#125;67vrrp_instance VI_1 &#123;8 state MASTER9 interface eth110 virtual_router_id 5111 priority 10012 advert_int 113 unicast_src_ip 172.18.0.214 unicast_peer &#123;15 1172.18.0.316 &#125;17 authentication &#123;18 auth_type PASS19 auth_pass 111120 &#125;21 virtual_ipaddress &#123;22 172.18.0.1023 &#125;24&#125; 从节点： 1! Configuration File for keepalived23global_defs &#123;4 router_id master5&#125;67vrrp_instance VI_1 &#123;8 state MASTER9 interface eth110 virtual_router_id 5111 priority 9012 advert_int 113 unicast_src_ip 172.18.0.314 unicast_peer &#123;15 1172.18.0.216 &#125;17 authentication &#123;18 auth_type PASS19 auth_pass 111120 &#125;21 virtual_ipaddress &#123;22 172.18.0.1023 &#125;24&#125; 主要有几点重要的地方，一个是router_id不能一样，这个是区分每个实力上的标志。 然后state可以都为master，即按照权重来强占。 interface填写的是网卡名称，这个要结合自己实际的网卡名。 virtual_router_id在同一个网络环境下不可以重复，不然会出问题。 priority是权重，按照这个值来确定主节点。 一般采用单播的形式，方便配置而且不影响其他服务，所以两个配置文件中的unicast_src_ip和unicast_peer是相反的。 virtual_ipaddress填写需要使用的虚拟IP就可以了。 如果是在容器中创建虚拟IP，有可能遇到错误： 1IPVS: Can&#39;t initialize ipvs: Protocol not available 开启的必要条件有两个： 容器放开权限，即添加参数--privileged 宿主机同样需要开启ipvasdm 测试输入ip a应该就可以看到虚拟ip出现在网络信息中。 没有这个命令的话通过yum install iproute安装 附录MySQL状态监测脚本1#!/bin/bash2pgrep -x mysqld &amp;&gt; /dev/null3if [ $? -ne 0 ]4then5 echo \"not running\"6else7 echo \"running\"8fi Nginx状态监测脚本FastDFS检测脚本FTP检测脚本","categories":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/tags/%E8%BF%90%E7%BB%B4/"},{"name":"MySQL","slug":"MySQL","permalink":"http://beritra.github.com/tags/MySQL/"},{"name":"Keepalived","slug":"Keepalived","permalink":"http://beritra.github.com/tags/Keepalived/"}]},{"title":"深入理解JVM虚拟机笔记","slug":"深入理解JVM虚拟机笔记","date":"2019-01-01T04:12:12.000Z","updated":"2022-02-20T11:27:13.896Z","comments":true,"path":"2019/01/01/深入理解JVM虚拟机笔记/","link":"","permalink":"http://beritra.github.com/2019/01/01/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JVM%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AC%94%E8%AE%B0/","excerpt":"本文是《深入理解JVM虚拟机》的读书笔记和摘要","text":"本文是《深入理解JVM虚拟机》的读书笔记和摘要 内存管理运行时数据区域根据 Java1.7 版本的虚拟机规范，Java虚拟机包括以下几个运行时数据区。 一、程序计数器程序计数器(Program Counter Register)是一块较小的内存空间，可以看做当前线程所执行的字节码的行号指示器。分支、循环、跳转、异常处理、线程恢复等等都依赖于计数器完成。 每个线程拥有独立的计数器，互相不影响，独立存储。 执行 Java 方法的时候计数器记录的是虚拟机字节码指令的地址，如果执行的是 Native 方法，那么计数器则为空(Undefined)。该内存区域是唯一一个在 Java 虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 二、虚拟机栈和程序计数器一样，Java 虚拟机栈(Java Virtual Machine Stacks)也是线程私有的，它的生命周期和线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个帧栈(Stack Frame)用来存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成，都对应着一个栈帧在虚拟机中入栈到出栈的过程。 常规把Java内存区分为堆内存和栈内存的方法过于粗糙，实际上的区域划分更加复杂。 局部变量表存放了编译期可知的各种基本类型(boolean、byte、char、short、int、float、long、double)、对象引用(reference类型，它不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置)、和returnAddress类型（指向一条字节码指令的地址）。 64 位长度的 long 和 double 类型会占用两个局部变量空间(Slot)，其余的数据类型占用一个。进入一个方法的时候，局部表量表的大小就是确定的，运行期间不会在改变。 Java 虚拟机规范中对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将会抛出StackOverFlowError异常。如果虚拟机栈可以动态扩展，扩展时无法申请足够内存，会抛出OutOfMemoryError异常。 总空间一定的情况下，局部变量表内容越多，栈帧越大，栈深度越小。进行大量递归的时候就有可能导致栈溢出。 三、本地方法栈本地方法栈(Native Method Stack)和虚拟机栈作用很相似，两者区别是后者为 Java 方法（字节码）服务，前者则为虚拟机使用到的 Native 方法服务。有的虚拟机就干脆合二为一（Sun HotSpot虚拟机），本地方法栈可能抛出的异常也是上面那两个。 四、Java 堆对于多数应用来说，Java 堆（Java Heap）是Java虚拟机管理的最大一块内存。Java 堆被所有线程共享，在虚拟机启动的时候创建。此内存区域的唯一目的就是存放对象实例。Java 虚拟机规范中规定所有的对象实例和数组都要在堆上分配，但是随着发展，现在变得并不这么绝对。 Java 堆内存是垃圾收集管理器管理的主要区域，所以也成为 GC 堆。从内存回收的角度来看，现代收集器都采用分代收集算法，所以 Java 堆还可以细分为：新生代和老年代。再细致一点有 Eden 空间、From Survivor 空间、To Survivor 空间。 Java 堆可以处在物理不连续的内存空间上，只要逻辑连续即可。 五、方法区方法区（Method Area）也是所有线程共享的内存区域，用于存储虚拟机加载的类信息、常量、静态变量、即时编译器编译的代码等数据。也称作 Non-Heap 非堆内存。 Java 虚拟机规范堆方法区的限制非常宽松，可以选择不实现垃圾收集，但是这部分区域的回收确实是有必要的。 平时，说到永久带(PermGen space)的时候往往将其和方法区不加区别。这么理解在一定角度也说的过去。因为，《Java虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 同时，大多数用的 JVM 都是 Sun 公司的 HotSpot。在 HotSpot 上把 GC 分代收集扩展至方法区，或者说使用永久代来实现方法区。 在 JDK1.8 及以后版本，永久带被移除，新出现的元空间（Metaspace）替代了它。元空间属于 Native Memory Space 在 1.8 中，可以使用如下参数来调节方法区的大小 XX:MetaspaceSize 元空间初始大小 XX:MaxMetaspaceSize 元空间最大大小超过这个值将会抛出 OutOfMemoryError 异常：java.lang.OutOfMemoryError: Metadata space 六、运行时常量池运行时常量池（Runtime Constant Pool）是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池（Constant Pool Table），用于存放编译期生成的各种字面量和符号引用，这部分内容在类加载后进入方法区的运行时常量池中存放。 Java 虚拟机堆 Class 文件的每一部分格式都有严格要求，要符合要求才能被虚拟机认、装载和执行。但是对于运行时常量池，Java 虚拟机规范没有做任何细节要求。另外运行时常量池的一个重要特征就是具有动态性，运行期间可一个将新的常量放入池中 ，这种特性被开发人员利用得比较多的便是 String 类的intern()方法。 七、直接内存直接内存（Direct Memory）不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是也被频繁使用，也有可能导致OutOfMemoryError异常出现。 在JDK1.4中新加入的NIO类，引入了基于通道与缓冲区的 I/O 模式，可以使用 Native 函数库直接分配堆外内存，然后通过存储在 Java 堆中的DirectByteBuffer对象作为内存的引用进行操作。 这部分内存不受到Java堆大小的限制，但是仍然收到本机内存空间和处理器寻址空间的限制，也有可能出现OutOfMemoryError异常。 对象创建过程类加载虚拟机遇到一条 new 指令的时候，会首先检查指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否被加载、解析和初始化过。如果没有，就限制性类加载过程。这个部分后续讨论。 分配内存类加载检查完成之后，虚拟机会对新的对象分配内存。对象所需的内存大小在类加载完成之后就可以完全确定，为对象分配空间的任务等同于把一块确定大小的内存从堆内存中划分出来，这里有两种划分方式： 如果内存绝对规整，分配内存就是把指针向后移动与对象大小相等的距离。这种方法叫做指针碰撞（Bump the Pointer） 如果堆中的内存不是规整的，已经使用的和空闲的内存相互交错，分配的时候就需要找到一块足够大的空间划分给对象实例，并维护一个列表记录地址。 堆内存是否规整是由垃圾收集器是否带有压缩整理功能决定的。 除此之外，还需要考虑的是对象创建中线程安全的问题，假如两个线程同时移动内存指针，就有可能出现错误，解决这个问题也有两个方案： 使用 CAS 搭配失败重试的方式保证更新操作的原子性。 或者把内存的分配动作按照线程划分在不同的空间之中进行，即每个线程在堆中预先分配一小块内存，成为本地内存分配缓冲区（Thread Local Allocation Buffer，TLAB），虚拟机是否启用 TLAB 可以用参数-XX:+/-TLAB来设定。 初始化信息内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。这一操作保证了对象的实例字段在 Java 代码中可以不赋初值就直接使用。 然后，虚拟机需要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。这些信息存放在对象的对象头（Object Header）中，根据虚拟机当前的运行状态不同，比如是否启用偏向锁等，对象头会有不同的设置方式。 从虚拟机的视角，一个新的对象已经产生，但是从 Java 程序的视角来看，对象创建才刚刚开始，init 方法还没执行，所有字段还都为零。所以，执行 new 指令之后会接着执行 init 方法，把对象按照程序员意愿进行初始化。 对象的内存布局在 HotSpot 虚拟机中，对象可以分为三个区域：对象头（Header）、实例数据（Instance Date）和对齐填充（Padding）。 对象头HotSpot 虚拟机的对象头包括两部分信息： 对象自身的运行时数据，如哈希码、GC 分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据在 32 位和 64 位虚拟机上分别为 32bit 和 64bit，官方称它为“Mark Word”。当对象需要存储的运营时数据很多时，它会根据对象的状态复用自己的存储空间。 类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 如果对象是一个 Java 数组，那在对象头中还必须有一块记录数组长度的数据，因为虚拟机可以通过普通 Java 对象的原数据确定 Java 对象的大小，但是从数组的原数据中却无法确定。 实例数据实例数据是对象真正存储的有效信息，也就是在程序代码中所定义的各种类型的字段信息，无论从父类继承下来的，还是在子类中定义的，都需要记录起来。这部分的存储顺序收到虚拟机分配策略参数和在源码中定义的顺序影响。 对齐填充这部分不是必然存在，没有特殊的含义，仅仅起着占位符的作用。HotSpot VM 的自动内存管理系统要求对象其实地址必须是8字节的整数倍，因此当对象实例数据部分没有对齐的时候，就需要通过对齐填充来补全。 对象的访问定位对象建立之后，Java 程序需要通过栈上的 reference 数据来操作堆上的具体内容。由于 reference 类型在 Java 虚拟机规范中之规定了一个指向对象的引用，并没有定义这个引用应该如何定位、访问堆中对象的具体位置，所以对象访问方式取决于虚拟机如何实现，当前主流有使用句柄和直接指针两种。 使用句柄访问的话，Java 堆中会单独划分一部分区域作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据和类型数据各自的具体地址信息。 如果使用直接指针访问，那么 Java 对对象的布局中就必须考虑如何放置访问类型数据的相关信息，而 reference 中存储的直接就是对象地址。 这两种方式各有优势，使用句柄的好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。 使用直接指针的最大好处是速度更快，节省了一次指针定位的开销。HotSpot 使用的是第二种方式。 垃圾收集器和内存分配策略概述垃圾收集出现的时间远比 Java 要早，从出现起，垃圾收集就需要考虑三个问题： 哪些内存需要回收 什么时候回收 如何回收 在 Java 内存运行时区域里，程序计数器、虚拟机栈、本地方法栈随线程而生，随线程而灭；栈中的栈帧随着方法的进入和退出有条不紊的进行出栈和入栈操作。每个栈帧中分配的内存基本是在类结构确定下来的时候就是已知的。Java 堆和方法区不一样，只有在运行时才知道创建那些对象，所以这部分内存的分配和回收都是动态的，垃圾收集器所关注的也是这部分内存。 哪些内存需要回收引用计数器法引用计数器法是这样的：给对象添加一个引用计数器，每当有一个地方引用它时，计数器 +1，引用失效的时候，计数器 -1，任何时刻计数器为 0 的对象就不可能再被使用。 主流的 Java 虚拟机中没有选用这个方法来管理内存的，最主要的原因是很难解决对象循环引用的问题。 可达性分析主流的商用程序语言的主流实现中，都是通过可达性分析（Reachability Analysis）来判定对象是否存活的。这个算法的基本思路是通过一系列的成为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称之为引用链（Reference Chain），当一个对象到达 GC Roots 没有任何引用链相连时，证明此对象不可用。 引用类型无论通过计数器，还是通过可达性分析，判断对象是否存活都与“引用”有关。在 JDK1.2 之后，Java对引用状态进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。 强引用就是指在代码中普遍存在的，类似于Object obj=new Object()这类引用，只要强引用还存在，垃圾收集器就永远不会回收被引用的对象。 软引用用来描述一些还有用但是不必须的对象。在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之内进行二次回收。如果这次回收之后还是没有足够内存，才会抛出内存溢出异常。 弱引用也是用来描述非必须对象达到，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾回收发生之前。当垃圾收集器工作的时候，无论当前内存是否充足，都会回收掉被弱引用关联的对象。 虚引用也称为幽灵引用或者幻影引用，是最弱的一种引用关系。一个对象是否有虚引用的的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用的唯一目的就是在这个对象被垃圾回收的时候收到一个系统通知。 生存还是死亡即使在可达性分析算法中不可达的对象，也不是必定被回收，他们暂时处于“缓刑”阶段。要真正宣告一个对象死亡，至少要经历两次标记过程：第一次在可达性分析中发现没有引用链，会被标记并且进行筛选，筛选条件条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize()方法或者已经被虚拟机调用过，虚拟机将这两种情况视为“没有必要执行”。 如果有必要执行，对象会被放置在一个叫 F-Queue 的队列中，稍后由一个虚拟机建立的、低优先级的 Finalizer 线程去执行。这个“执行”是指虚拟机触发这个方法，但是不承诺等待执行完毕。finalize()方法是对象逃脱死亡的最后一次机会，如果对象要在其中拯救自己，那么与任何一个引用链上的对象建立关联即可。如果建立的关联，比如将自己赋值给了某个类变量，那么第二次标记的时候就会被移除“即将回收”的集合。 垃圾收集算法由于垃圾收集算法的实现涉及大量的程序细节，而且各个平台的虚拟机操作内存的方法有各不相同，所以只介绍几种算法的思想和发展过程。 标记-清除算法最基础的算法是“标记-清除”（Mark-Sweep）算法，如名字一样，算法分为“标记”和“清除”两个阶段：首先标记需要回收的对象，然后统一回收。他是最基础的收集算法，但是主要有两点不足： 效率问题，标记和清除效率都不高 空间问题，标记清除之后会有大量的不连续内存碎片，空间碎片太多导致后续分配大对象内存的时候，无法找到足够的连续内存而不得不触发另一次垃圾收集动作。 复制算法复制算法提高了效率，他可以将内存按照容量划分为大小相等的两块，每次使用其中一块。当一块的内存用完之后，把存活的对象移动到另一块，然后把已经使用的空间一次性清理掉。这个方式实现简单运行高效，但是代价是内存缩小为原来的一半。 现在的商业虚拟机都是采用这种收集算法来回收新生代，由于 98% 的对象都是“朝生夕死”的，所以不需要 1:1 的比例划分内存空间，而是划分一块较大的 Eden 和两块较小的 Survivor 空间，每次使用 Eden 和其中一块 Survivor。回收的时候，将 Eden 和 Survivor 中还存活的对象一次性复制到另一块 Survivor 中，然后清理掉 Eden 和使用的 Survivor 空间。HotSpot 虚拟机中默认的 Eden和 Survivor 空间比例划分是 8:1。但是，98% 的对象可回收只是一般场景下的数据，我们不能保证每次回收都是这样，所以当 Survivor 空间不够使用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 当一块 Survivor 空间中没有足够的空间存放上一次新生代收集下来的存活对象的时候，这些对象会直接通过分配担保机制进入老年代。 标记-整理算法复制收集算法在对象存活率较高的时候就要进行较多的复制操作，效率将会变低。更关键的是如果不想浪费 50% 的空间，就需要额外空间进行担保，所以老年代不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”算法，标记过程与“标记-清除”一样，但是后续不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法当代商业虚拟机的垃圾收集都是采用分代收集（Generational Collection）算法，根据对象存活周期将不同内存划分为几块。一般是把 Java 堆分为新生代和老年代，这样可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集都有大量的对象死去，只有少量存活，那就选用复制算法。而老年代存活率高、没有额外空间进行担保，就必须使用“标记-清理”或者“标记-整理”算法来回收。 HotSpot算法实现枚举根节点可以作为 GC Roots 的节点主要在全局的引用（例如常量或者静态属性）与执行上下文（栈帧中的本地变量表）中，如果逐个检查里面的引用，会耗费很多时间。 另外，可达性分析对执行的敏感还体现在 GC 的停顿上，这项分析工作必须在能确保一致性的快照中进行，不可以出现分析过程中对象还在不断变化的情况。所以这导致 GC 进行时需要停顿所有的Java执行线程（Sun将这个事情称为“Stop The Word”） 由于目前主流的 Java 虚拟机使用的都是准确式 GC，所以当执行系统停顿下来之后，不需要一个不漏的检查玩所有执行上下文和全局的引用位置，虚拟机有办法直接得知哪些地方存放着对象引用。在 HotSpot 的实现中，是通过使用一组称为 OopMap 的数据接口来达到这个目的。在类加载完成的时候，HotSpo t就把对象内偏移量上是什么类型的数据计算出来，在 JIT 编译过程中，也会在特定的位置记录下帧和寄存器中哪些位置是引用的。 安全点在 OopMap 的帮助下，HotSpot 可以快速的完成GC Root的枚举，但是随着引用变化，不可能每次指令都生成新的 OopMap。实际上只有“特定的位置”才会暂停开始 GC，这个位置叫做安全点（Safepoint）。Safepoint 的选定既不能太少以至于 GC 等待时间过长，也不能太频繁以至于增大运行时负荷。 对于 Safepoint 另一个需要考虑的问题是如何在 GC 发生时让所有线程都跑在最近的安全点上再停下来。这里有两个方案： 抢先式中断（Preemptive Suspension）：不需要线程的执行代码主动配合，GC 发生的时候先中断全部线程，如果发现有的线程中断地方不在安全点上，就恢复线程，让他跑到安全点上。现在几乎没有虚拟机实现抢先式中断来暂停线程。 主动式中断（Voluntary Suspension）：当 GC 需要中断线程的时候，不直接对线程操作，而是仅仅设置一个标志，各个线程主动轮询这个标志，发现标志为真的时候主动中断挂起。轮讯标志的地方和安全点是重合的。 安全区域程序执行的时候，通过 Safepoint 可以解决如何进入 GC 的问题，但是程序“不执行”的时候呐？比如线程处于 Sleep 状态或者 Blocked 状态，这时候线程无法相应 JVM 的中断请求，JVM 也不太可能等待线程被重新分配 CPU 时间。对于这种情况，需要安全区域（Safe Region）来解决。 安全区域是指在一段代码中，引用关系不会发生变化，在这个区域中的任何位置进行 GC 都是安全的。当线程执行到 Safe Region 中的代码时，会标识自己已经进入了 Safe Region，这样发起 GC 的时候就不用管进入 Safe Region 状态的线程了。 垃圾收集器如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 常见的垃圾收集器有以下几种： 他们分别适用于不同分代，连线说明可以配合使用。 jdk1.7 默认垃圾收集器Parallel Scavenge（新生代）+Serial Old（老年代） jdk1.8 默认垃圾收集器Parallel Scavenge（新生代）+Serial Old（老年代） jdk1.9 默认垃圾收集器G1 jdk10 默认垃圾收集器G1 Serial 收集器Serial 收集器是最基本、历史最悠久的收集器。它是一个单线程收集器，不仅仅是说只是用一个 CPU 进行垃圾收集工作，更重要的是进行垃圾收集的时候，必须暂停 其他所有工作线程直到收集结束。 Serial 依然是 Client 模式下新生代默认的收集器，他也有着优于其他收集器的地方：简单而高效（单线程环境下）。 ParNew 收集器ParNew 其实就是 Serial 收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为包括 Serial 收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与 Serial 收集器完全一样。 ParNew 是许多运行在 Server 模式下的虚拟机中首选的新生代收集器，有一个重要的原因是除了 Serial 收集器外，它是唯一能与 CMS 收集器配合工作。ParNew 在单 CPU 环境中不会比 Serial 有更好的效果，甚至由于线程切换的开销可能性能更低。 Parallel Scavenge 收集器Parallel Scavenge 收集器是一个新生代收集器，也是使用复制算法的收集器，又是并行的多线程收集器。 Parallel Scavenge 收集器的特点是它的关注点与其他收集器不同，CMS 等收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 收集器的目的是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是 CPU 用于运行用户代码的时间和总的消耗时间的比值。虚拟机总共运行了 100 分钟，99 分钟用来运行代码，1 分钟垃圾收集，那么吞吐量就是 99%。 Parallel Scavenge 收集器提供了两个参数进行进准控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX:MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX:GCTimeRatio参数。 MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽可能的保证内存回收花费的时间不超过设定值。但是这个值不是越小越好，GC 停顿时间缩短是以牺牲吞吐量和新生代空间换取的，停顿时间降下来很可能吞吐量也下降了。 GCTimeRatio参数的值应当是一个大于 0 且小于 100 的整数，也就是垃圾收集时间占总时间的比率，相当于吞吐量的倒数。 Serial Old 收集器Serial Old 是 Serial 收集器的老年代版本，他同样是一个单线程收集器，使用“标记-整理”算法。这个收集器的主要意义也是给 Client 模式下的虚拟机使用。 CMS 收集器CMS（Concurrent Mark Sweep）收集器是一种以最短回收停顿时间为目标的收集器。从名字就可以看出，CMS 收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几个收集器来说更复杂一点，整个过程分为 4 个步骤： 初始标记（initial mark） 并发标记（concurrent mark） 重新标记（remark） 并发清除（concurrent sweep） 其中，初始标记和重新标记仍然需要“Stop The World”。初始标记仅仅标记一下 GC Roots 能直接关联到的对象，速度很快，并发标记阶段是进行 GC Roots Tracing 的过程，而重新标记则是为了修正并发标记期间用户程序继续运作而导致的标记变动，这个阶段的停顿时间一般会比初始阶段稍长，但是远比并发标记阶段耗时短。 由于整个过程中耗时最长的并发标记和并发清除过程都可以和用户线程一起工作，所以总体来说CMS收集器的内存回收过程是和用户线程一起并发执行的。 但是，CMS 同样有以下明显的缺点： CMS 收集器对 CPU 资源非常敏感，默认启动了（CPU数量+3）/4个线程执行回收，也就是当 CPU 在4个以上并发回收时收集线程占用不少于25%的CPU资源，并且随着 CPU 数量增加而下降。CPU 数量少的时候对用户的影响就很大了。 CMS 无法处理浮动垃圾（Floating Garbage），可能出现Concurrent Mode Failure失败而导致另一个 Full GC 产生。即 CMS 在并发清除过程中产生的新的垃圾，只能在下一次GC时再处理。 最后一个缺点，由于 CMS 基于“标记-清除”算法实现，意味着收集结束可能会有大量的空间碎片产生。空间碎片过多会对大内存分配产生麻烦，导致分配内存的时候不得不提前触发 Full GC。 G1 收集器G1 是 JDK9 版本之后JVM默认的垃圾收集器，它具有以下特点： 并行和并发：G1 可以充分利用多 CPU、多核环境下的硬件优势，使用多个 CPU 来缩短 Stop The World 停顿的时间。 分代收集：分代概念仍然在 G1 中保留，但是 G1 可以不需要与其他收集器配合独自管理整个 GC 堆。 空间整合：与 CMS 的“标记-清理”算法不同，G1 从整体上看是基于“标记-整理”算法实现，但是从局部（两个Region）来看是基于“复制”算法实现的。无论如何，这两种算法一位置 G1 运作期间不会产生内存碎片，收集完成后可以提供规整可用的内存。 可预测的停顿：这是 G1 相对于 CMS 的另一大优势，G1 除了追求低停顿之外，还能建立可预测的停顿时间模型，让使用者明确指定在一个长度为 M 毫秒的时间片断内，消耗在垃圾收集上的时间不超过 N 毫秒。 G1 之前的收集器进行收集的范围都是整个新生代或者老年代，而 G1 不再是这样。使用 G1 收集器时，Java 堆内存的布局就与其他收集器有很大差别，他将整个 Java 堆划分为多个大小相等的独立区域（Region）虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离，都是一部分 Region（不需要连续）的集合。 G1 收集器之所以能建立可预测的时间模型，因为它有计划地避免在整个Java堆中进行全区域的垃圾收集。G1 追踪各个 Region 里面垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的时间优先回收价值最大的 Region。 在 G1 收集器中，Region 之间的对象应用以及其他收集器中的新生代及老年代之间的对象引用，虚拟机都是使用 Remembered Set 来避免全堆扫描的。G1 中每个 Region 都有一个与之对应的 Remembered Set，虚拟机发现程序对 Reference 类型的数据进行写操作的时候，会产生一个 Write Barrier 暂时中断写操作，检查 Reference 引用的对象是否处于不同的 Region 之中，如果是，就通过 CardTable 把相关引用信息记录到被引用的对象所属的 Region 的 Remembered Set 中。内存回收的时候，在 GC 根节点的枚举范围中加入 Remembered Set 即可保证不对全堆扫描也不会有遗漏。 除了维护 Remembered Set 的操作，G1 收集器的运作大致可以分为以下几个步骤： 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Data Counting and Evacuation） 可以看出这几个步骤的运作过程和CMS有很多相似之处。初始标记阶段只是标记一下 GC Roots 能直接关联到的对象，并且修改TAMS（Next Top at Mark Star）的值，让下一阶段用户程序并发运行的时候，能在正确可用的 Region 中创建对象，这个阶段需要停顿线程，但是耗时很短。并发标记阶段是从 GC Roots 开始进行可达性分析，这个阶段耗时较长但是可以和用户程序并发执行。最终标记阶段则是修正并发标记阶段产生的标记变动，合并到 Remembered Set 中。这个阶段需要停顿线程，但是可以并行执行。最后筛选阶段首先对各个 Region 的回收价值和成本排序，根据用户指定的 GC 停顿时间来制定回收计划。 GC日志一般情况可以通过两种方式来获取 GC 日志，一种是使用命令动态查看，一种是在容器中设置相关参数打印 GC 日志。 可以通过 jstat 命令查看当前正在运行的 Java 进程的 GC 状态，命令如下： 1jstat -gc java进程号 毫秒单位的时间间隔 结果含义： S0C：年轻代中第一个survivor（幸存区）的容量 (字节) S1C：年轻代中第二个survivor（幸存区）的容量 (字节) S0U：年轻代中第一个survivor（幸存区）目前已使用空间 (字节) S1U：年轻代中第二个survivor（幸存区）目前已使用空间 (字节) EC：年轻代中Eden（伊甸园）的容量 (字节) EU：年轻代中Eden（伊甸园）目前已使用空间 (字节) OC：Old代的容量 (字节) OU：Old代目前已使用空间 (字节) PC：Perm(持久代)的容量 (字节) PU：Perm(持久代)目前已使用空间 (字节) YGC：从应用程序启动到采样时年轻代中gc次数 YGCT：从应用程序启动到采样时年轻代中gc所用时间(s) FGC：从应用程序启动到采样时old代(全gc)gc次数 FGCT：从应用程序启动到采样时old代(全gc)gc所用时间(s) GCT：从应用程序启动到采样时gc用的总时间(s) NGCMN：年轻代(young)中初始化(最小)的大小 (字节) NGCMX：年轻代(young)的最大容量 (字节) NGC：年轻代(young)中当前的容量 (字节) OGCMN：old代中初始化(最小)的大小 (字节) OGCMX：old代的最大容量 (字节) OGC：old代当前新生成的容量 (字节) PGCMN：perm代中初始化(最小)的大小 (字节) PGCMX：perm代的最大容量 (字节) PGC：perm代当前新生成的容量 (字节) S0：年轻代中第一个survivor（幸存区）已使用的占当前容量百分比 S1：年轻代中第二个survivor（幸存区）已使用的占当前容量百分比 E：年轻代中Eden（伊甸园）已使用的占当前容量百分比 O：old代已使用的占当前容量百分比 P：perm代已使用的占当前容量百分比 S0CMX：年轻代中第一个survivor（幸存区）的最大容量 (字节) S1CMX ：年轻代中第二个survivor（幸存区）的最大容量 (字节) ECMX：年轻代中Eden（伊甸园）的最大容量 (字节) DSS：当前需要survivor（幸存区）的容量 (字节)（Eden区已满） TT： 持有次数限制 MTT ： 最大持有次数限制 GC 参数JVM 的 GC 日志的主要参数包括如下几个： -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2017-09-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log 日志文件的输出路径 在生产环境中，根据需要配置相应的参数来监控JVM运行情况 内存分配和回收策略Java 的内存管理可以归结为内存的自动分配和自动回收，上面讲了内存回收的策略，下面再说一下内存分配。 对象优先在Eden分配大多数情况下，对象在新生代 Eden 区中分配，当 Eden 区没足够空间的时候，虚拟机将发起一次 Mino GC。 虚拟机提供了-XX:+PrintGCDetail这个参数，告诉虚拟机在发生垃圾内存收集行为的时候打印内存回首日志，并且在进程退出的时候输出当前内存各个区域的分配情况。 大对象直接进入老年代所谓大的对象是指需要大量连续空间的 Java 对象，最典型的是很长的字符串和数组。 虚拟机提供了-XX:PretenureSizeThreshold参数，另大于这个设置值的对象直接在老年代分配。目的是避免 Eden 区及两个 Survivor 区之间大量的内存复制。 长期存活的对象进入老年代 为了分配哪些对象放在新生代，哪些放在老年代，虚拟机给每个对象定义了一个对象年龄（Age）计数器，如果对象在 Eden 出生并经过第一次 Minor GC 后仍然存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并且对象年龄设为1。对象在 Survivor 中每经过一次 Minor GC，年龄就增加一岁，当年龄增加到一定程度（默认是 15 岁），就会被晋升到老年代，这个阈值可以通过参数-XX:MaxTenuringThreshold设置。 动态对象年龄判定为了适应不同程序的内存状况，不是只有对象年龄大于 MaxTenuringThreshold 才能晋升老年代。如果 Survivor 空间中相同年龄所有对象大小总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代。 空间分配担保在发生 Minor GC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有的对象总空间，如果这个条件成立，那么 Minor GC 可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于晋升到老年代对象的平均大小。如果大于就尝试进行一次 Minor GC，尽管这次 GC 是有风险的。如果小于，或者不允许冒险，就改为进行一次 Full GC。 虚拟机性能监控和故障处理命令行工具 名称 主要作用 jps JVM Process Status Tool，显示制定系统内所有的 HotSpot 虚拟机进程 jstat JVM Statistics Monitoring Tool，用于收集 HotSpot 虚拟机各方面的运行数据 jinfo Configuration Info for Java，显示虚拟机配置信息 jmap Memory Map for Java，生成虚拟机的内存转储快照（heapdump文件） jhat JVM Heap Dump Browser，用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果 jstack Stack Track for Java，显示虚拟机的线程快照 jps：虚拟机进程状况工具jps 除了名字像 ps 命令之外，功能也很类似：可以列出正在运行的虚拟机进程，并且显示虚拟机执行主类（Main Class，Main()函数所在的类）名称以及这些进程的本地虚拟机唯一ID（Local Virtual Machine Identifier，LVMID）。虽然功能简单，但是它是最常用的工具，因为其他 JDK 工具需要输入它查询出来的 LVMID 来确定监控的是哪个虚拟机进程。在本地虚拟机进程来说，LVMID 和 ps 命令或者 Windows 中任务管理器中查到的进程号是一致的。但是如果启动了多个虚拟机进程，就要依赖 jps 命令根据主类来区分了。 选项 作用 -q 只输出 LVMID，省略主类的名称 -m 输出虚拟机级进程启动的时候传给主类 main() 函数的参数 -l 输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 包路径 -v 输出虚拟机进程启动时 JVM 参数 jstat：虚拟机统计信息监视工具jstat（JVM Statistics Monitoring Tool）适用于监视虚拟机各种运行状态信息的命令行工具。它可以显示本地或远程虚拟机中的类装载、内存、垃圾收集、JIT 编译等运行数据。 jstat 命令格式为： 1jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] 对于命令格式中的 VMID 与 LVMID 需要说明的是，如果是本地虚拟机进程，VMID 与 LVMID 是一致的，如果远程的虚拟机进程， VMID 格式应当是： 1[protocol]:[&#x2F;&#x2F;]lvmid[@hostname[:port]&#x2F;servername] 参数 interval 和 count 代表查询间隔和次数，如果省略这两个参数，说明只查询一次。 选项 option 代表着用户希望查询的虚拟机信息，主要分为 3 类：类装载、垃圾收集、运行期编译状况。 选项 作用 -class 监视类装载、卸载数量、总空间以及类装载所耗费的时间 -gc 监视Java堆状况，包括 Eden 区、两个 Survivor 区、老年代、永久带等的容量、已用空间、GC 时间合计等信息 -gccapacity 监视内容与-gc基本相同，但是输出主要关注 Java 堆各个区域使用的最大、最小空间 -gcutil 监视内容与-gc基本相同，但是输出主要关注已使用空间占总空间的百分比 -gccause 与-gcutil功能一样，但是会额外输出导致上一次 GC 产生的原因 -gcnew 监视新生代 GC 状况 -gcnewcapacity 监视内容与-gcnew基本相同，输出主要关注使用到的最大、最小空间 -gcold 监视老年代 GC 状况 -gcoldcapacity 监视内容与-gcold基本相同，输出主要关注使用到的最大、最小空间 -gcpermcapaciry 输出到永久带使用到的最大、最小空间。注意，由于永久带在 JDK1.8 之后已经被元空间替代，所以 1.8 之后都没有了这个选项 -compiler 输出 JIT 编译器编译过的方法、耗时等信息 -printcompilation 输出已经被 JIT 编译的方法 jinfo：Java 配置信息工具jinfo(Configuration Info for Java)的作用是实时地查看和调整虚拟机各项参数。 用法如下： 1Usage:2 jinfo [option] &lt;pid&gt;3 (to connect to running process)4 jinfo [option] &lt;executable &lt;core&gt;5 (to connect to a core file)6 jinfo [option] [server_id@]&lt;remote server IP or hostname&gt;7 (to connect to remote debug server)89where &lt;option&gt; is one of:10 -flag &lt;name&gt; to print the value of the named VM flag11 -flag [+|-]&lt;name&gt; to enable or disable the named VM flag12 -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value13 -flags to print VM flags14 -sysprops to print Java system properties15 &lt;no option&gt; to print both of the above16 -h | -help to print this help message 比如使用-flag查看 JVM 参数 jinfo -flag MaxMetaspaceSize 18348，得到结果-XX:MaxMetaspaceSize=536870912，即MaxMetaspaceSize为512Mjinfo -flag ThreadStackSize 18348，得到结果-XX:ThreadStackSize=256，即Xss为256K jinfo 也可以调整 JVM 参数： 如果是布尔类型的 JVM 参数：jinfo -flag [+|-]&lt;name&gt; PID，enable or disable the named VM flag 如果是数字/字符串类型的 JVM 参数：jinfo -flag &lt;name&gt;=&lt;value&gt; PID，to set the named VM flag to the given value 那么怎么知道有哪些 JVM 参数可以动态修改呐？可以用下面这个命令： 1Linux环境：java -XX:+PrintFlagsInitial | grep manageable2Window环境：java -XX:+PrintFlagsInitial | findstr manageable jmap:Java 内存映像工具jmap(Memory Map for Java)命令用于生成堆转储快照（一般称为 heapdump 或者 dump 文件）。如果不适用 jmap 命令，想要获取 Java 堆转储快照，还有一些比较暴力的手段：比如使用-XX:+HeapDumpOnOutOfMemoryError参数，可以让虚拟机在 OOM 异常出现之后自动生成dump文件，通过-XX:+HeapDumpOnCtrlBreak参数则可以使用[ctrl]+[Break]键让虚拟机生成 dump 文件，又或者在 Linux 系统下通过Kill -3命令发送进程退出信号“吓唬”一下虚拟机，也能拿到 dump 文件。 除此之外，jmap 还可以查询 finalize 执行队列，Java 堆和永久带的详细信息，如空间使用率、当前使用的哪种收集器等。 jmap 命令格式： jmap [option] vmid option 选项的合法值与具体含义如下： 选项 作用 -dump 生成 Java 堆转储快照。格式为：-dump:[live, ]format=b，files=&lt;filename&gt;，其中 live 子参数说明是否只 dump 出存活的对象 -finalizerinfo 显示在 F-Queue 中等待 finalizer 线程执行 finalizer 方法的对象。只在 Linux/Solaris 平台下有效 -heap 显示 Java 堆详细信息，如使用哪种回收器、参数配置、分代状况等。只在 Linux/Solaris 平台下有效。 -histo 显示堆中对象统计信息，包括类、实例数量、合计容量 -permstat 以 ClassLoader 为统计口径显示永久带内存状态。只在 Linux/Solaris 平台下有效 -F 当虚拟机进程没有对-dump选项响应时，可以使用这个选项强制生成 dump 快照。只在 Linux/Solaris 平台下有效 -clstats 打印类加载器的统计信息 -J 传递参数给 jmap 的 JVM jhat：虚拟机堆转储快照分析工具Sun JDK 提供 jhat(JVM Heap Analysis Tool)命令与 jmap 搭配使用，来分析 jmap 生成的堆转储快照。jhat 内置了一个微型的 HTTP/HTML 服务器，生成 dump 文件的分析成果后，可以在浏览器中查看。 执行命令jhatp 文件名，屏幕显示Server is ready的提示后，打开浏览器 localhost 的 7000 端口就可以看到分析结果。 jstack:Java 堆栈跟踪工具jstack(Stack Track for Java)命令用于生成虚拟机当前时刻的线程快照（一般称为 threaddump 或者 javacore 文件）。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合，生成快照的主要目的是定位线程出现长时间停顿的原因，如线程死锁、死循环、请求外部资源导致的长时间等待等。 jstack 命令的格式： jstack [option] vmid 选项 作用 -F 当正常输出的请求不被相应时，强制输出线程堆栈 -l 除堆栈外，显示关于锁的附加信息 -m 如果调用本地方法的话，可以显示 C/C++ 的堆栈 java.lang.Thread类的getAllStackTraces()方法可以获取所有线程的StackTraceElement对象。使用这个方法可以用简单的几行代码完成 jstack 的大部分功能。 Jconsole:Java 监视与管理控制台JConsole（Java Monitoring and Management Console）是一种基于JMX 的可视化管理工具。 通过 JDK/bin 目录下的jsonsole应用就可以启动 JConsole，启动后将自动搜索本机运行的所有虚拟机进程。双击选择一个进城之后即可开始监控。 监控页面一目了然，不需要做太多说明。JConsole 可以监控包括堆内存、线程、类、CPU 等各个使用情况， VisualVM：多合一故障处理工具VisualVM（All-in-One Java Troubleshooting Tool）是目前为止随 JDK 发布的功能最强大的故障处理工具，并且在可预见的未来都是官方主力发展的虚拟机故障处理工具。 VisualVM 可以做到： 显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的 CPU、GC、堆、方法去以及线程的信息（jps、jstack）。 dump 以及分析堆转储快照（jamp、jhat）。 方法记得性能运行性能分析，找出被调用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程 dump、内存 dump 等信息建立一个快照，可以将快照发送给开发者处进行 Bug 反馈。 其他 Plugins Intellij IDEA 已经集成了 VisualVM，详细的使用方法先留个坑，后续再填。 类文件结构Class 文件的结构Class 文件是一组以 8 位字节为基础的二进制流，各个数据项目严格按照顺序紧凑的排列在 Class 文件之中，中间没有添加任何分隔符，这使得整个 Class 文件中存储的内容几乎全部是程序运行的必要数据，没有空隙存在。当遇到占用 8 字节以上空间的数据项时，会按照高位在前的方式分割成若干 8 位字节进行存储。 根据J ava 虚拟机规范的规定，Class 文件格式 采用一种类似于 C 语言结构体的伪结构来存储数据，这种伪结构只有两种数据类型：无符号数和表。 类加载机制Class 文件中描述的各种信息，最终都要加载到虚拟机之中才能运行和使用。而虚拟机如何加载这些 Class 文件？Class 文件中的信息进入到虚拟机后会发生什么变化？这些都是本章需要讲解的内容。 虚拟机把描述类的数据从 Class 文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的 Java 类型，就是虚拟机的类加载机制。在 Java 语言中，类型的加载、连接和初始化过程都是在程序运行期间完成的。 类加载的时机类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading） 共七个阶段。其中验证、准备、解析三个部分统称连接（Linking），这七个阶段的发生顺序如图所示： 在图中，加载、验证、准备、初始化和卸载这 5 个阶段是顺序的，类的加载必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后开始，这是为了支持 Java 语言的运行时绑定（也称为动态绑定或晚期绑定）。注意在，这里写的是“开始”，而不是按部就班的“进行”或者“完成”。强调这点的原因是这些阶段通常都是交叉混合进行的，通常会在一个阶段执行的过程中调用、激活下一个阶段。 虚拟机规范严格规定了有且只有 5 种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前开始）： 遇到 new、getstatic、putstatic、invokestatic这 4 条字节码指令的时候，如果类没有进行过初始化，则先需要触发其初始化。生成这 4 条指令的最常见的 Java 代码场景是：使用new关键字实例化对象的时候、读取或者设置一个类的静态字段（被final修饰、已在编译器把结果放入常量池的静态字段除外）的时候，以及调用一个类的静态方法的时候。 使用java.lang.reflect包的方法对类进行反射调用的时候。如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则先需要出发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 当使用 JDK 1.7 的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果是REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。 当一个接口中定义了JDK 8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。 对于这 6 种会触发类进行初始化的场景，虚拟机规范中使用了一个很强烈的限定于：“有且只有”，这 6 种场景中的行为成为对一个类进行主动引用。除此之外，所有的引用类的方式都不会触发初始化，称为被动引用。 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过子类来引用父类中定义的静态字段，只会出发父类的初始化而不会触发子类的初始化。至于是否要触发子类的加载和验证，在虚拟机规范中尚未明确规定，这点取决于虚拟机的具体实现。对于 Sun HotSpot 虚拟机来说，可以通过-XX:TraceClassLoading参数观察到此操作会导致子类的加载。 使用类的常亮也不会对这个类进行初始化，而是在编译阶段通过常量传播优化，将该常量存储到使用这个常亮的类的常量池中，后续对该常量的调用都会转化为该类堆自身常量池的引用。 接口的加载过程和类加载过程稍有一些不同，针对接口需要做一些特殊说明：接口也有初始化过程，这一点与类是一致的。接口中不能使用static{}代码块，但是编译期仍然会为接口生成&lt;clinit&gt;()类构造器，用于初始化接口中所定义的成员变量。接口与类真正有区别的是前面所讲说的 5 种“有且只有”需要开始初始化场景中的第 3 种：当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部都完成了初始化，只有在真正用到父接口的时候（如引用接口定义的常亮）才会初始化。 类加载的过程加载、验证、准备、解析和初始化这 5 个阶段的具体动作。 加载加载是“类加载”（Class Loading）的一个阶段，在加载阶段，虚拟机需要完成以下 3 件事： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 虚拟机规范的这三点都不具体，因此虚拟机实现和具体引用的灵活度都是相当大的。所以出现过很多种加载方式： 从 ZIP 包中读取，这很常见，最终成为日后 JAR、EAR、WAR 格式的基础。 从网络中获取，这种场景最典型的应用就是 Applet。 运行时计算生成，这种场景使用得最多的就是动态代理技术，在java.lang.reflect.Proxy中，就是利用ProxyGenerator.generateProxyClass来为特定接口生成形式为$Proxy的代理类二进制字节流。 从其他文件中读取，典型场景是 JSP 引用，即由 JSP 文件生成对应的 Class 类。 从数据库中读取，这种场景相对少见，例如有些中间件服务器（如 SAP Netweaver）可以选择把程序安装到数据库中来完成程序代码在集群见的分发。 相对于类加载过程的其他阶段，一个非数组类的加载阶段（准确的说，是加载阶段中获取类的二进制字节流的动作）是开发人员可控性最强的，因为加载阶段既可以使用系统提供的引导类加载器来完成，也可以由用户自定义的类加载器去完成，开发人员可以通过定义自己的类加载器去控制字节流的获取方式（即重写一个类加载器的loadClass()方法）。 对于数组类来说情况有所不同，数组类本身不是通过类加载器区创建，它是由 Java 虚拟机直接创建的。但是数组类和类加载器仍有很密切的关系，因为数组类的元素类型（Element Type，值得是数组去掉所有维度的类型）最终仍要靠类加载去去创建，一个数组类创建过程就遵循以下规则： 如果数组的组件类型（Component Type，指的是数组去掉一个维度的类型）是引用类型，那就递归采用上面的加载过程去加载这个组件类型，数组将在加载该组件类型的类加载器的类空间上被标识。 如果数组的组件类型不是引用类型（如int[]数组），Java 虚拟机将会把数组标记为与引导类加载器关联。 数组的可见性与他的组件类型的可见性一致，如果组件类型不是引用类型，那数组类的可见性将默认为public。 加载阶段完成后，虚拟机外部的二进制字节流文件就按照虚拟机所需的格式存储在方法去之中，方法区中的数据存储格式由虚拟机实现自行定义，虚拟机规范未规定此区域的具体结构。然后在内存中实例化一个java.lang.Class类的对象（并没有明确在 Java 堆中，对于 HotSpot 虚拟机而言，Class 对象比较特殊，它虽然是对象，但是存放在方法区里面），这个对象将作为程序访问方法区中的这些类型数据的外部接口。 加载阶段与连接阶段的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 验证验证是连接阶段的第一步，这一阶段的目的是确保 Class 文件中的字节流中包含的信息符合虚拟机要求，并且不会危害虚拟机自身的安全。 Java 语言本身是相对安全的语言，使用纯粹的 Java 代码无法做到诸如访问数组边界以外的数据、将对象转型为它未实现的类型、跳转到不存在的代码行之类的事情，如果这样做了，编译器会拒绝编译。验证阶段大致上会完成以下 4 个阶段的检验动作： 文件格式验证：主要验证字节流是否符合 Class 文件格式的规范，并且能够被当前版本的虚拟机处理。 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合 Java 语言规范的要求。 字节码验证：第三阶段是整个验证过程中最复杂的一个阶段，主要目的是通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。在第二阶段对元数据信息中的数据类型做完校验后，这个阶段将对类的方法体进行教研分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事件。如果一个类方法体的字节码没有通过字节码验证，那肯定是有问题的，但是如果一个方法体通过了字节码验证，也不能说明其一定就是安全的。 符号引用验证：最后一个阶段的校验发生在虚拟机将符号引用转化为直接引用的时候，这个转化动作将在连接的第三阶段——解析阶段中发生就。符号引用验证的目的是确保解析动作能正常执行，如果无法通过符号引用验证，那么将会抛出java.lang.IncompatibleClassChangeError异常的子类。 对于虚拟机的类加载机制来说，验证阶段是一个非常重要的、但不是一定必要（因为对程序运行期没有影响）的阶段。如果所运行的全部代码已经被反复使用和验证过，那么在实施阶段可以考虑使用-Xverify:none参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。假设一个类变量的定义为： 1public static int value=123; 那变量 value 在准备阶段过后的初始值为 0 而不是 123，因为这个时候尚未开始执行任何 Java 方法，而把 value 赋值为 123 的putstatic指令是程序被编译后，存放于类构造器&lt;clinit&gt;()方法之中，所以把 value 赋值为 123 的动作将在初始化阶段才会执行。下表列出了 Java 中所有基本数据类型的零值。 有一些特殊情况：如果类字段的字段属性表中存在ConstantValue属性，那么准备阶段变量 value 就会被初始化为ConstantValue属性所指的值，假设上面类变量 value 的定义变为： 1public static final int value=123; 编译时 Javac 将会为 value 生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将 value 赋值为 123。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。直接引用与符号引用的关系： 符号引用（Symbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量。符号引用与虚拟机实现的内存布局无关，引用的目标不一定已经加载到内存中。 直接饮用（Direct References）：直接引用可以是直接指向目标的指针、相对偏移量或一个能间接定位到目标的句柄。直接引用是和虚拟机实现的内存布局相关的，如果有了直接饮用，那引用的目标一定已经在内存中存在了。 初始化类初始化阶段是加载过程的最后一步，在前面的过程中，除了加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制的。到了初始化阶段，才真正开始执行类中定义的 Java 程序代码。 在准备阶段，变量已经赋值过一次系统要求的初始值，而在初始化阶段，则根据程序员通过程序定制的主观计划区初始化类变量和其他字段，或者从另外一个角度表达：初始化阶段是执行类构造器&lt;clinit&gt;()方法的过程。 &lt;clinit&gt;()方法是由编译器自动收集类中所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序所决定。静态语句块中只能访问到定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块可以赋值，但是不能访问。 &lt;clinit&gt;()方法与类的构造函数不同，它不需要显式地调用父类构造器，虚拟机会保证在子类的&lt;clinit&gt;()方法执行之前，父类的&lt;clinit&gt;()方法已经执行完毕。因此在虚拟机中第一个被执行的&lt;clinit&gt;()方法的类肯定是java.lang.Object。 由于父类的&lt;clinit&gt;()方法先执行，所以意味着父类中定义的静态语句块要优先于子类的变量赋值操作。 &lt;clinit&gt;()方法对于类或借口来说并不是必须的，如果一个类中没有静态语句块，也没有对变量的赋值操作，那么编译器可以不为这个类生成&lt;clinit&gt;()方法。 接口中不能使用静态语句块，但仍然有变量初始化的赋值操作，因此接口与类一样都会先生成&lt;clinit&gt;()方法。但是接口与类不同的是，执行接口的&lt;clinit&gt;()方法不需要先执行父类的&lt;clinit&gt;()方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也不会执行接口的&lt;clinit&gt;()方法。 虚拟机会保证一个类的&lt;clinit&gt;()方法在多线程环境下被正确的加锁、同步。如果多个线程去初始化一个类，那么只会有一个线程去执行这个类的&lt;clinit&gt;()方法，其他线程都需要阻塞等待，直到活动线程执行&lt;clinit&gt;()方法完毕。 类加载器类与类加载器类加载器虽然只用于实现类的加载动作，但是它在 Java 程序中起到的作用却远远不限于类加载阶段。对于任何一个类，都需要由加载它的类加载器和这个类本身一同确立其在 Java 虚拟机中的唯一性，每个类加载器，都拥有一个独立的类名称空间。表达的更通俗一些：比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这两个类就必定不同。 双亲委派模型从虚拟机角度来讲，只有两种不同的类加载器：一种是启动类加载器（Bootstrap ClassLoader），这个类加载器使用 C++ 语言实现，是虚拟机自身的一部分；另一种就是所有其他的类加载器，这些类加载器都是由 Java 实现，独立于虚拟机外部，并且全部继承自抽象类java.lang.ClassLoader。 从 Java 开发人员的角度来看，类加载器还可以划分的更细致： 启动类加载器（Bootstrap ClassLoader）：这个类加载器负责将存放在&lt;JAVA_HOME&gt;\\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机是别的类库加载到虚拟机内存中。 扩展类加载器（Extension ClassLoader）：这个类加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载&lt;JAVA_HOME&gt;\\lib\\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）：这个类加载器由sun.misc.Launcher$ExtClassLoader实现。由于这个类加载器是 ClassLoader 中的getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。它负责加载用户路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器。 我们的应用程序都是由这 3 种类加载器相互配合进行加载的，如果有必要，还可以加入自己定义的类加载器。这些类加载器之间的关系一般如图所示： 这种层次关系，称之为类加载器的双亲委派模型（Parents Delegation Model）。双亲委派模型的要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。这里类加载器之间的父子关系一般不会以继承（Inheritance）的关系来实现，而是通过使用组合（Compositon）关系来复用父加载器的代码。 双亲委派模型的工作过程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围内没有找到所需的类）时，子加载器才尝试自己加载。 使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是 Java 类随着它的类加载器一起具备了带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar中，无论哪一个类加载器需要加载这个类，最终都是委派个处于模型最顶端的启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果没有使用双亲委派模型，由各个类加载器去自行加载的话，如果用户自己编写了一个java.lang.Object的类，也放在程序的ClassPath下，那么系统就会出现多个不同的Object类，Java 类型基础的行为也不能保证了。 双亲委派模型对于保证 Java 程序的稳定运行非常重要，但是实现却很简单，实现双亲委派模型的代码都集中在java.lang.ClassLoader的loadClass()方法中，逻辑清晰易懂：先检查是否已经被加载过，如果没有加载则调用父加载器的loadClass()方法，若父加载器为空则默认使用启动类加载器作为父加载器。如果父加载器加载失败，抛出ClassNotFoundException异常后，再调用自己的findClass()方法进行加载。 双亲委派模型上文提到过双亲委派模型并不是一个具有强制性约束的模型，而是 Java 设计者推荐给开发者们的类加载器实现方式。在 Java 的世界中大部分的类加载器都遵循这个模型，但也有例外的情况，直到 Java 模块化出现为止，双亲委派模型主要出现过 3 次较大规模“被破坏”的情况。 第一次破坏是由于双亲委派机制是 JDK 1.2 才出现的，所以面对已经存在的用户自定义类加载器的代码，需要进行兼容，所以进行了妥协。 第二次被破坏是由这个模型自身的缺陷导致的，有时候基础类型需要调用用户编写的类，比如 JNDI。JNDI 服务引入线程上下文类加载器加载所需的 SPI 服务代码，这是一种父类加载器去请求子类加载器完成类加载的行为，这种行为实际上是打通了双亲委派模型的层次结构来逆向使用类加载器，已经违背了双亲委派模型的一般性原则，但也是无可奈何的事情。Java 中涉及 SPI 的加载基本上都采用这种方式来完成，例如 JNDI、JDBC、JCE、JAXB 和 JBI 等。 第三次“被破坏”是由于用户对程序动态性的追求而导致的，这里所说的“动态性”指的是一些非常“热”门的名词：代码热替换（Hot Swap）、模块热部署（HotDeployment）等。 参考目录： jinfo命令详解","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"虚拟机","slug":"虚拟机","permalink":"http://beritra.github.com/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"JVM","slug":"JVM","permalink":"http://beritra.github.com/tags/JVM/"}]},{"title":"RESTful API","slug":"RESTful API","date":"2018-04-17T13:57:19.000Z","updated":"2019-12-15T14:39:58.908Z","comments":true,"path":"2018/04/17/RESTful API/","link":"","permalink":"http://beritra.github.com/2018/04/17/RESTful%20API/","excerpt":"RESTful API到底怎么玩？记录一下。","text":"RESTful API到底怎么玩？记录一下。 是啥按照是啥、有啥用、怎么用的分析步骤，先解决RESTful是什么的问题。先上高大上的定义，wiki这么说： 表现层状态转换（英语：Representational State Transfer，缩写：REST） ##有啥用 我的理解RESTful就是一种设计风格，通俗点说就是我们设计API的一种指导思想。主要有几个特点： 统一 无状态 可缓存 分层 其他特征没觉得有什么用，暂且不说。个人觉得无状态是RESTful最大的特点，和http协议的设计思路类似。另外RESTful应该着重强调对资源的控制，即每一个URI代表一种资源，客户端通过HTTP动词，对服务端的资源进行操作。 但是回到有啥用这个问题上，个人觉得风格统一，容易理解，方便使用是比较大的优点，其他的倒是没有了。 怎么用RESTful的核心思想是用五个HTTP动词对资源进行操作： GET(获取) 从服务器获取一个资源或者资源列表 POST(创建) 在服务器上创建一个新资源 PUT(更新) 以整体方式更新服务器上的一个资源 PATCH(更新) 更新服务器上的某个资源的部分属性 DELETE(删除) 删除服务器上的资源 另外有两个不常用的动词： HEAD 获取资源元数据 获取信息。这个cors解决跨域的时候有可能用到。 以下是一篇文章中的栗子： GET /zoos:获取所有动物园信息 POST /zoos: 创造一个新的动物园 GET /zoos/ZID: 获取整个动物园对象 PUT /zoos/ZID: 更新整个动物园对象 PATCH /zoos/ZID: 更新动物园对象中的某些属性 DELETE /zoos/ZID: 删除动物园 GET /zoos/ZID/animals: 获取ZID这个动物园下的所有动物 可以看出这里的栗子URL采用了path info的模式，所有查询参数都在URL路径中。但是RESTful风格一般没有对这个做要求，放在后面的参数中也可以。 ##认证怎么搞 常用的接口认证有如下几种方式： Token(JSON Web Token)JWT 是JSON风格轻量级的授权和身份认证规范，可实现无状态、分布式的Web应用授权，JWT主要由以下三部分构成，由.进行连接 Header Payload Signature 所以完整的JWT格式应该是类似xxxxx.yyyyy.zzzzz这样。 Headerheader主要声明token类型和使用的加密算法，比如： 1&#123;2 \"alg\": \"HS256\",3 \"typ\": \"JWT\"4&#125; 然后将头部进行Base64加密，得到第一部分。 Payloadpayload包含你进行认证需要的数据，比如： 1&#123;2 \"sub\": \"1234567890\",3 \"name\": \"John Doe\",4 \"admin\": true5&#125; 然后将有效Payload用Base64进行编码，以形成JWT的第二部分。 Signature要创建签名部分，必须要有已经编码的header，编过码的Payload，一个密匙（secret）和加密算法。 例如，如果想使用HMAC SHA256算法，签名将按以下方式创建： 1HMACSHA256(base64UrlEncode(header) + \".\" +base64UrlEncode(payload),secret) 更详细的文档可以参见官方网站：https://jwt.io/ 跨域既然前后端分离了，就不可避免的要遇到跨域问题。跨域问题单独拎出来总结吧。 //TODO 参考资料： 架构风格与基于网络应用软件的架构设计 RESTful API 设计指南 wiki REST 理解RESTful架构 好RESTful API的设计原则 RESTful API风格基于Token的鉴权机制分析(与JWT结合)","categories":[{"name":"最佳实践","slug":"最佳实践","permalink":"http://beritra.github.com/categories/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"}],"tags":[{"name":"RESTful","slug":"RESTful","permalink":"http://beritra.github.com/tags/RESTful/"},{"name":"最佳实践","slug":"最佳实践","permalink":"http://beritra.github.com/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"}]},{"title":"机器学习（一）线性回归","slug":"机器学习（一）线性回归","date":"2018-04-17T13:57:19.000Z","updated":"2019-12-15T14:09:09.225Z","comments":true,"path":"2018/04/17/机器学习（一）线性回归/","link":"","permalink":"http://beritra.github.com/2018/04/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","excerpt":"注意到大多数机器学习的课程都是从线性回归开始讲的，而且这部分直觉上的最容易理解，所以我也从线性回归开始学习。","text":"注意到大多数机器学习的课程都是从线性回归开始讲的，而且这部分直觉上的最容易理解，所以我也从线性回归开始学习。 概述面对素不相识的概念，我们先来个素质三连：What、Why、How。 什么是线性回归？线性回归是机器学习中最简单、基础的模型，维基百科线性回归条目中如此介绍： 在统计学中，线性回归（Linear regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。 这种函数是一个或多个称为回归系数的模型参数的线性组合。 只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归 举一个简单的栗子： 有如下一组数据，我们想根据这些数据研究他们之间的规律。 用脚趾头都能看出来，这横轴和纵轴数据是线性相关的，但是你不能用脚趾头证明它，所以需要有一种模型描述他们之间的规律，要尽量保证误差最小，从而能通过已有的数据预测未知的数据。 为什么要用线性回归？这个问题太显而易见反而不容易回答，现实中很多数据之间的关系是线性的，只有一个因变量一个自变量就可以表述它们之间的关系，直觉上也知道用简单线性回归再合适不过了。当然，多个参数自变量也可以是线性回归，不过叫多元线性回归。但是需要注意，线性回归对异常值非常敏感，所以要不要用，怎么用还得仔细考量。 ###怎么回归？ 突然感觉在概述中问这三个问题好蠢，都说清楚的话这篇文章就讲完了…所以下面就详细说怎么回归，先从最简单的一元线性回归，又叫简单线性回归开始。 ##一元线性回归 小学大概就学过，在直角坐标系中描述一条直线的方程是y=kx+b，这里也是一样的，我们的目的是寻找一个合适的方程，能最好的描述x和y的关系，这个方程在大多数课程里叫假设函数 hypothesis function，所以一元线性回归方程就是： 两个Theta怎么选，我们的目的应该是尽量拟合所有的点，所以需要一个指标来描述我们选的值合不合适，我们把它称作损失函数cost function 或者 loss function，这两个名称的含义似乎有些微不同，但是我没搞很清楚，先留个疑问。损失函数使用每个点上的对h(x)预期值与实际值的偏差先平方再取平均来描述，如下图所示。 它这里前面参数多了个二分之一，只是为了后续方便计算。我们的目标就是计算大量的cost function，然后找到其中的最小值。 参考： 机器学习(一) 简单的背景介绍、线性回归、梯度下降","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://beritra.github.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"机器学习","slug":"机器学习","permalink":"http://beritra.github.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"Rancher CI/CD Pipeline 初步学习","slug":"Rancher-CI-CD-Pipeline-初步学习","date":"2018-02-06T04:00:50.000Z","updated":"2019-12-15T15:45:08.513Z","comments":true,"path":"2018/02/06/Rancher-CI-CD-Pipeline-初步学习/","link":"","permalink":"http://beritra.github.com/2018/02/06/Rancher-CI-CD-Pipeline-%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/","excerpt":"最近的工作任务是研究Docker、Rancher上CI/CD 流程，记录一些学习过程。","text":"最近的工作任务是研究Docker、Rancher上CI/CD 流程，记录一些学习过程。 什么是CI/CDCI/CD的意思就是持续集成(Continuous integration)和持续\b交付(continuous delivery)或持续部署(continuous deployment)。这三个步骤组成了现在软件开发的基本流程。 初次接触到这些概念基本是懵的，尤其是小公司或者传统软件行业，对软件迭代速度没那么高的要求。\b什么叫集成？怎么算持续？集成的意思就是说快速的把新开发的特性合并到主干上，可以理解为git上把某一个feature分支合并到master上，不过这一过程还包括了程序的构建和测试。持续集成一般是指\b每天进行很多次集成，它的目的主要是快速的更新产品，既能方便调整方向，又能及时发现bug，从而实现了“小步快跑\b”的策略。 而持续交付是指频繁的将产品交到用户或者质量评审团队手里，以供评审，通过之后就可以进入到生产。持续部署是指自动化部署到生产环境。通过实施CI/CD，可以在一天进行很多次的产品迭代，从而提升竞争力。 CI/CD的一般流程也就是软件开发的生命周期，只不过完全将其自动化。从git进行代码提交开始，webhook检测到提交事件之后，开始\b进行单元测试，测试通过之后，将代码合并到主分支，然后进行构建。构建完成在进行集成测试、系统测试，测试无误就可以交付了。然后进行部署，其中可能会有灰度发布，或者用户体验不佳然后进行版本回滚的操作。 基于Docker的CI/CD有什么优势由于自身理解不深，搜集了一些博文的观点： 一个完整的流程入是这样的，用户（也就是开发人员）将包含Dockerfile的源码从本地push到Git服务器上，然后触发Jenkins进行构建源码，源码构建完成后紧接着进行Docker image的构建，一切构建完成之后，顺带将构建成功的image上传到企业内部的镜像仓库，到此刻为止，其实一个基本的CI（持续集成）已经算是结束，剩下的部分就是持续部署或者进行持续的交付开发产物了。在以前传统的软件发布模式中，持续集成的产物是编译打包好的代码，如果想要发布程序，发布系统需要在持续集成的制品库中去获得对应的代码，然后根据一系列的环境检查来准备应用的运行时环境，而在此过程中往往会涉及到比较多的基本组件依赖，所以在整体的发布周期内来看，还是有一些问题的。在Docker或者容器时代，我们将容器的镜像构建部分融入到持续集成（CI）环节，最终持续集成的产出物是一些已经处理好依赖关系，基本不需要人工进行二次干预的Docker image，而在CD环节，发布系统只需要设置和管理很少的信息就能够很快将image运行起来，快速地将业务发布出去。在上面整个环节中，其实无非就是增加了Docker的那一层处理，但其实在整个软件开发的生命周期中，它是产生了极大的影响的。首先，部署系统不需要为统一的部署框架去做更多逻辑抽象，业务研发在开发代码的过程中选择自己依赖的base image即可，最终运行起来的业务也就是你当时提供的base image的模样；其次，由于base image已经处理好了相关的依赖，所以当发布系统拿到业务的image的时候，发布操作将会变得异常迅速，这对于互联网时代可谓是非常重要的；最后一点，也是我感受最深的，就是研发构建好的image可以在任何的Docker环境中run起来，研发人员不需要再关系环境一致性的问题，他们在自己本地的测试环境能够运行起来的应用，那么到生成环境也一定可以。——基于Docker的CI/CD流水线实践 一个想象的Java CI/CD 流程说了一堆理论的东西，回归到实际环境，由于我们公司主要开发语言是Java，我就先想象了一下对于Java语言，应该是个怎么样的CI/CD流程。首先，研发人员开发完成某一阶段之后，提交代码到主分支，然后触发webhook，开启pipeline。首先，应该在一个带有JDK的容器中下载代码，然后用maven下载依赖、编译、打包，然后运行测试，测试通过之后把class文件、jar包和配置文件转移到一个只含有jre的容器，进行镜像的构建步骤，然后推送到镜像仓库，最后将镜像发布，生\b成新的容器实例。 重点关注问题\b关于CI/CD Pipeline，主要需要从以下几个方面着手考虑。 崇伟提到重点关注的几个问题 git分支管理大致怎样？ 构建镜像的版本号如何管理？ 测试环节，如何整合自动化测试？ 推送镜像的目标harbor，是否区分开发环境/测试环境/正式环境？ 发布流程是否可以控制，比如整合到运维的发布流管理系统？ 灰度发布，调研时也需要关注 分支管理Rancher Pipeline的触发是用git某一个分支的webhook。每一个stage可以设置触发条件，包括commit\b ID、分支名、仓库url\b这三种进行区分。 这里需要考虑的主要问题是要如何指导研发人员的的git工作流，怎么设置分支，目标是尽量少的对现有方案进行改动。 这里可以参照两篇文章关于两种CI/CD策略与git分支模型的思考、A successful Git branch model 版本号Pipeline构建过程中可以使用以下变量： NAME DESC CICD_GIT_COMMIT git commit sha CICD_GIT_BRANCH git branch CICD_GIT_URL git repository url CICD_PIPELINE_ID pipeline id CICD_PIPELINE_NAME pipeline name CICD_TRIGGER_TYPE trigger type CICD_NODE_NAME jenkins node name CICD_ACTIVITY_ID pipeline history record id CICD_ACTIVITY_SEQUENCE run number of pipeline history record 都可以作为版本号，其中视频教程中推荐使用CICD_ACTIVITY_SEQUENCE，这基本可以当做一个Pileline的自增序号。 \b这里主要考虑得是在发布流中对版本号的控制。 测试环节整合自动化测试在一个步骤中选择类型为task之后，都可以执行shell，在这里可以调用自动化测试的命令。 \bharbor是否区分环境\b可以区分，不过通过仓库、镜像名、tag都可以\b作为区分，是不是还有必要分开多个环境？ 发布流程发布流程每一步都可以设置权限控制，相关人员确认后再进行下一步。 关于是否可以整合Jenkins插件，视频中说现在不支持，建议不要混搭。 灰度发布灰度发布的关键在于流量控制和版本控制，要求能快速切换版本，控制每个版本的流量。Rancher Pipeline中现有的相关操作是upgrade service可以设置步长，即选择保留的版本数量，同时可以选择先上线新版本再关闭旧版本，或是先关闭再上线，这个操作的时间间隔也可以设置。在Rancher操作界面中也可以随时会滚版本。关于流量控制可以使用HAproxy等负载均衡来做。 现有的工具询问多个同事得知，各个项目组还没有统一的流程，旭哥的eSim团队可能是相对比较健全的，之前使用过GitLab和Jenkins，现在基于方便考虑，切换到了同一家出品的Bitbucket+Bamboo。以ESim为例，当前的发布流程是这样： 提交代码到各自feature分支 每日把各自feature分支合并到develop分支（触发自动化build），同时执行单元测试用例。有冲突解决冲突，直至冲突解决完成 develop拉release分支提测 测试通过在bamboo上将release发布 Rancher Pipeline Demo 环境搭建启动环境Rancher Pipeline的搭建非常简单，首先需要满足Rancher server版本在v1.6.13以上，这里我们选择了v1.6.14。然后在商店里搜索pipeline就可以看到，不需要额外配置，启动服务就可以了。 启动完成之后，Rancher UI界面顶部菜单会出现一个新的按钮”流水线”，点击即可进入\bPipeline界面。 仓库认证和授权进入Pipeline界面之后，点击添加流水线，它会让你先完成仓库授权和添加代码仓库： 当前支持的代码仓库只有两个，GitHub和GitLab，我这里选择了自己部署的GitLab。首先打开Gitlab，登陆，点击右上角自己的头像，在下拉菜单中选择setting： 然后在新的页面里选择application，name里面取个名字，Redirect URI是Pipeline提供给你的，复制过来就行，然后选一个权限，我选择的api，最后保存： 其中，Redirect URI 在这里： \b都完成之后，GitLab会生成一个ID和密钥，将这两个值复制到Pipeline中： 然后别忘了选中”使用私有GitLab部署”，然后填上ip，就可以了，最后点击gitlab验证。 页面会跳到GitLab，点击认证，这一步就完了。 \b流水线配置回到流水线界面，点击添加流水线，会让你选择git用户、仓库和分支，按照各自情况选好需要跟踪的分支，点击添加，第一个阶段就完成了。 然后点击添加第二个阶段，这里可以选择并行或者串行，这里的并行或者串行是指本构建阶段(stage)下的每个步骤(step)执行逻辑。同时可以看到，这里有两个可选选项，分别提供条件筛选和审核。如果不满足条件，步骤不执行。或者执行到这一步的，等待审核人审核，审核通过之前不会执行。 添加完阶段之后添加一个步骤，Rancher教程以go语言为例，选择步骤类型为task，然后镜像为golang:1.8，输入需要执行的shell，这一步的作用就是把代码复制到相应的容器中，然后进行编译，使用容器作为编译环境保证了环境的一致性。这里的那些参数只是为了演示的时候加快变异速度，其实最重要的就是一步go build 进行到这里，无误的话源代码已经编译完成，应该执行单元测试。所以下一个阶段基本相同，再添加一个阶段，添加一个步骤，步骤类型task，命令稍有不同。 单元测试完成，开始构建镜像。添加一个阶段，添加一个步骤，这里步骤类型选择build，这里的build是指build image。\bDockerfile可以在这里填写，或者直接从代码中拉取，这里选择的是从代码中拉，由于只是demo，Dockerfile非常简单: 1FROM alpine2EXPOSE 80803COPY .&#x2F;bin&#x2F;outyet &#x2F;usr&#x2F;bin&#x2F;outyet4ENTRYPOINT &#x2F;usr&#x2F;bin&#x2F;outyet 同时可以看到镜像标签那里使用了上面说到的变量CICD_ACTIVITY_SEQUENCE，可以看做是一个自增序列。这一步顺利完成之后，就构建好了一个名为outyet的镜像，版本号是一个数字，然后被推送到2.8.0.3:8001/pipeline仓库。 进行到这里，自动集成的步骤就完成了。然后应该把镜像推送到Rancher中，产生容器实例，进行自动发布。新建一个阶段，添加一个步骤，这里步骤类型选择为upgradeStack\b。编写docker-compose配置文件，Pipeline会自动从仓库中拉取镜像，然后产生容器实例，更新之前发布的版本。 实际应用下的步骤\b上面基于GoLang语言的Demo是一个最小化的步骤示例，其实对于go语言来讲，编译是很简单的一个步骤，因为没有其他语言复杂的包依赖，构建产物也是直接的可执行文件，最后直接执行即可。但是对于公司广泛应用的\bJava来说，构建步骤需要考虑Jar包依赖的情况，而一般以来的Jar包都是通过maven管理，git上面是不会记录的，所以拉下来的源代码没办法直接进行编译。 \b第一反应是想到了两种方式： \b研发使用统一的基础镜像，在本地进行编译和打包，我们后续只对镜像进行管理。Intellij IDEA提供了插件倒是方便了直接生成镜像。 提供maven环境，统一在服务器上进行编译和打包，研发还是只提供源码就好。 第一种想法很快就被舍弃，因为研发小伙伴未必都对docker了解，水平参差不齐的话很难对镜像进行要求和管理，学习成本略高。\bmaven环境的话只需要把第二个阶段里golang镜像换成一个包含jdk、maven的镜像即可。 所以，结合现有的git flow，建议一个完整的开发流程如下： 研发人员在各自负责的模块新开feature分支，强制要求每日合并到develop分支，强制要求编写单元测试用例。 在完成阶段性开发之后，develop分支合并到release分支或者master分支，触发webhook，开启Pipelin。 \bRancher Pipeline 将代码下载到一个包含JDK和maven的镜像中，进行编译和打包。 \b执行单元测试。 测试无误之后，以一个只含有JRE的基础镜像中构建一个新的应用镜像，推送到Harbor。 测试环境从Harbor拉取镜像，产生容器示例，供QA进行完整的测试。 测试无误后发布到正式环境，这一步应当选择QA人员为审核人，审核通过的话自动发布。 其他\b了解相关信息的过程中，也听到了一些其他的方案，比如Gogs搭配jenkins，或者GitOps和Kubernetes。但是前者集成度不佳，后者依赖于我们尚未使用的K8s，所以暂不做考虑。Rancher分享中提到的Drone似乎也是一个不错的选择，可以直接对接企业微信，推送状态，精力原因还没做详细了解，后续再进行对比。 参考：Rancher Pipeline Referince Guide","categories":[{"name":"容器","slug":"容器","permalink":"http://beritra.github.com/categories/%E5%AE%B9%E5%99%A8/"}],"tags":[{"name":"容器云","slug":"容器云","permalink":"http://beritra.github.com/tags/%E5%AE%B9%E5%99%A8%E4%BA%91/"},{"name":"Rancher","slug":"Rancher","permalink":"http://beritra.github.com/tags/Rancher/"},{"name":"Docker","slug":"Docker","permalink":"http://beritra.github.com/tags/Docker/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://beritra.github.com/tags/CI-CD/"}]},{"title":"ELK应用栈的搭建和测试","slug":"ELK应用栈的搭建和测试","date":"2018-01-26T10:17:10.000Z","updated":"2020-03-09T15:44:42.079Z","comments":true,"path":"2018/01/26/ELK应用栈的搭建和测试/","link":"","permalink":"http://beritra.github.com/2018/01/26/ELK%E5%BA%94%E7%94%A8%E6%A0%88%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8C%E6%B5%8B%E8%AF%95/","excerpt":"本篇记录搭建、测试ELK应用栈的过程。","text":"本篇记录搭建、测试ELK应用栈的过程。 Logstash 安装和使用安装运行Logstash依赖于Java，所以需要预先安装jre，建议1.8版本及以上。然后在官网可以直接下载二进制压缩文件，解压即可。 创建最简单的示例Logstash管道有两个最基本元素，输入和输出，和一个可选的元素，拦截器。输入插件从一个输入源中消费数据，拦截器插件按照你指定的方式修改数据，然后输出插件将数据写入到一个目的地。 想要验证Logstash的安装是否成功，可以运行最基本的Logstash管道： 1cd logstash2bin&#x2F;logstash -e &#39;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&#39; -e标志可以直接通过命令行指定配置，从而让你快速的测试配置而不用多次修改一个配置文件。这个示例中的管道从标准输入stdin中获取输入，然后用一个结构化的格式把数据移动到标准输出stdout。 在启动Logstash完成之后，看到Pipline started提示后，可以在命令行中输入hello world，你会看到： 1hello world22018-01-25T09:26:53.134Z caih-OptiPlex-7050 hello world Logstash在信息中添加了\b时间戳和主机名。 使用Logstash解析日志前例中，我们创建了一个最基本的Logstash管道用来测试Logstash是否启动成功。实际应用中，Logstash管道会复杂很多，它会有多个不同类型的输入，有拦截器，还有输出插件。 直接使用Logstash跟踪日志文件Logstash也可以不借助Filebeat，直接跟踪日志文件。 这里我们写一个简单的shell脚本，名为log.sh，每一秒钟往日志文件里输出一个时间戳，然后跟踪这个日志文件。 1# &#x2F;bin&#x2F;bash2while :3do4 current&#x3D;&#96;date &quot;+%Y-%m-%d %H:%M:%S&quot;&#96;5 echo $current &gt;&gt; log.log6 sleep 1s7done 使用命令nohup bash log.sh &amp;让这个脚本在后台执行。然后tail -f log.log\b就可以看到日志文件在不断增加。 回到logstash的目录下bin文件夹，创建一个简单的配置文件，名为logstash.conf: 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log&quot;]4 &#125;5&#125;6output &#123;7 stdout&#123;&#125;8&#125; 这个配置非常简单，就是在追踪刚才写的日志作为输入，然后在命令行输出。执行命令./logstash -f logstash.conf，稍等之后就可以看到日志中打印的时间戳出现啦，同时每一条日志前面会有logstash添加的时间戳和主机名。这时候遇到了一个问题，就是logstash运行之前的那些历史日志，会有乱序。我怀疑是多个线程同时收集日志导致的顺序问题。这时候修改config文件夹里面的logstash.yml，将pipieline.worker改为1，这个情况有了好转，但是仍然偶尔可见顺序问题，尚有待解决。 可以注意到，每次重启logstash之后，日志都是从上次的结束为止开始输出的，那么如何每次都从日志文件的开头开始输出呐？可以修改配置如下： 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log&quot;]4 start_position &#x3D;&gt; &quot;beginning&quot;5 sincedb_path &#x3D;&gt; &quot;&#x2F;dev&#x2F;null&quot;6 &#125;7&#125;8output &#123;9 stdout&#123;&#125;10&#125; Logstash跟踪日志的时候，标记追踪位置的数据存储在sincedb中，这里sincedb_path就是指定sincedb文件的位置，然后把它配置为linux中自带的空位置，就实现了每次从头开始的目的。注意，start_position只是说每次从sincedb中标记的起点开始，不是从文件头开始，所以只添加这一项是不行的。 然后同样的，可以把文件位置改为系统日志： 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;var&#x2F;log&#x2F;*.log&quot;]4 type &#x3D;&gt; &quot;system&quot;5 start_position &#x3D;&gt; &quot;beginning&quot;6 sincedb_path &#x3D;&gt; &quot;&#x2F;dev&#x2F;null&quot;7 &#125;8&#125;9output &#123;10 stdout&#123;&#125;11&#125; 可以看到系统日志的情况。 借助Filebeat采集日志这部分我们创建一个Logstash管道，使用Filebeat将Apache的日志作为输入，解析这些日志，然后从中创建特定的、命名规范的字段，然后把数据输入到Elasticsearch集群中。这里使用配置文件来定义管道，而不是使用命令行参数。 关于FilebeatLogstash可以Filebeat配合使用来收集日志，那么Filebeat是个什么东西呐？ Filebeat是一个本地文件的日志信息收集器。安装后可以作为你服务端的代理，Filebeat监控日志目录或者指定的日志文件，不断输出文件然后将他们发送到Elasticsearch和Logstash进行索引。 Filebeat的工作模式是这样的：你启动Filebeat之后，它会开启一个或者多个勘探者（prospector）来跟踪本地路径下你指定的日志文件。对于每一个勘探者定位到的文件，Filebeat会启动一个收割者（harvester）。每个收割者读取单个日志文件，发送到libbeat，libbeat会聚合所有时间，然后发送到你配置好的输出中。 Filebeat的安装很简单，在Ubuntu上只需要执行： 1curl -L -O https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;downloads&#x2F;beats&#x2F;filebeat&#x2F;filebeat-6.1.2-amd64.deb2sudo dpkg -i filebeat-6.1.2-amd64.deb 安装完成之后，可以在/etc/filebeat/filebeat.yml文件中查看和更改配置。在Docker环境中，这个文件的位置是在/usr/share/filebeat/filebeat.yml。 这里以一个简单的配置为例： 1filebeat.prospectors:2- type: log3 enabled: true4 paths:5 - &#x2F;var&#x2F;log&#x2F;*.log6 #- c:\\programdata\\elasticsearch\\logs\\* 更详细的文档格式可以参看官方文档，这里只配置一个简单的栗子。 配置Filebeat来发送日志行到Logstash在创建Logstash管道之前，需要配置Filebeat用来发送日志到Logstash。Filebeat客户端是一个轻量级，资源友好型工具，用来从服务端的文件中收集日志，然后发送这些日志到Logstash实例中处理。Filebeat的设计理念是高可用和低延时。Filebeat有一个\b宿主机上轻量的资源占用，而且Beats input插件将Logstash实例上的资源需求做到了最小化。 创建Filebeat的配置文件名为filebeat.yml，内容如下： 1filebeat.prospectors:2- type: log3 paths:4 - &#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log5output.logstash:6 hosts: [&quot;127.0.0.1:5044&quot;] 其中的paths就是需要跟踪的日志地址。 然后修改Logstash的配置文件: 1input &#123;2 beats &#123;3 port &#x3D;&gt; &quot;5044&quot;4 &#125;5&#125;6output &#123;7 stdout &#123; codec &#x3D;&gt; rubydebug &#125;8&#125; 然后分别启动两者就可以了，启动Filebeat：filebeat -e -c /home/caih/docker/filebeat/filebeat.yml -d &quot;publish&quot;；启动Logstash：./logstash -f logstash.conf，随后就可以看到日志源源不断地在命令行出现啦！ Elasticsearch 安装和使用安装运行Elasticsearch的安装非常简单，需要依赖只有Java 1.8之后的版本。然后在官网下载文件，解压文件即可。 进入解压后的文件夹，bin目录下有名为Elasticsearch的可执行文件，执行./elasticsearch就可以启动了。 ./elasticsearch --help查看命令帮助，可以使用./elasticsearch -d在后台执行，不在当前命令行输出日志。 配置进入config文件夹可以看到配置文件，其中elasticsearch的配置是在elasticsearch.yml中，另外的jvm.options可以修改Java虚拟机的相关配置，log4j2.properties可以修改el本身的日志配置。 在elasticsearch.yml中，修改netword.host的值为0.0.0.0，然后就可以其他主机访问了。其他常用的配置还包括集群名称，节点名称，日志地址，端口号等等，更多的信息不再赘述。 在浏览器输入ip:9200，可以看到返回: 1&#123;2 &quot;name&quot; : &quot;t_ksdEI&quot;,3 &quot;cluster_name&quot; : &quot;elasticsearch&quot;,4 &quot;cluster_uuid&quot; : &quot;LdO3LVVyRPGwTARorxrNBA&quot;,5 &quot;version&quot; : &#123;6 &quot;number&quot; : &quot;5.5.3&quot;,7 &quot;build_hash&quot; : &quot;9305a5e&quot;,8 &quot;build_date&quot; : &quot;2017-09-07T15:56:59.599Z&quot;,9 &quot;build_snapshot&quot; : false,10 &quot;lucene_version&quot; : &quot;6.6.0&quot;11 &#125;,12 &quot;tagline&quot; : &quot;You Know, for Search&quot;13&#125; 这样表示Elasticsearch启动成功。 添加一条文档在命令行输入： 1curl -XPUT &#39;localhost:9200&#x2F;customer&#x2F;doc&#x2F;1?pretty&amp;pretty&#39; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39;2&gt; &#123;3&gt; &quot;name&quot;: &quot;John Doe&quot;4&gt; &#125;5&gt; &#39; 然后就添加了一条最简单的文档，稍后可以在\bKibana中看到。 Kibana 安装和使用\bKibana的安装同样简单，同样依赖于Java，官网下载tar包解压，然后进入bin目录，使用命令./kibana --help可以查看启动参数。 输入./kibana serve即可启动，这里需要注意与Elasticsearch版本是否匹配。我这里用的都是最新版6.1.2。\bKiana默认寻找得是本地9200端口的Elasticsearch，如果要指定别的地方的实例，可以在/config/kiban.yml中修改。还有一点，\bKibana默认host是本地，所以要修改为0.0.0.0然后才能在其他机器上访问。 输入ip:5601就可以看到Kibana的ui界面了，刚开始会让你输入一个index pattern，可以先输入*匹配所有索引，然后再Discover中就可以看到Elasticsearch中的数据了。 从日志文件到Kibana日志到Logstash已经联通，Elasticsearch和\bKibana也连上了，现在只需要把Logstash的output设置为Elasticsearch。 修改logstash.conf如下： 1input &#123;2 file &#123;3 path &#x3D;&gt; [&quot;&#x2F;home&#x2F;caih&#x2F;docker&#x2F;logstash&#x2F;log&#x2F;log.log&quot;]4 &#125;5&#125;6output &#123;7 elasticsearch &#123; hosts &#x3D;&gt; [&quot;localhost:9200&quot;] &#125;8&#125; 然后Logstash默认的索引是以logstash-开头，所以把kibana中的index pattern修改为logstash-*就可以过滤结果了。这时候在discover部分就可以看到每秒钟一条的时间戳。 Kibana默认的配置中，这里的信息是不自动刷新的，所以想要修改刷新频率，在Management -&gt; advance中，把timepicker:refreshIntervalDefaults这一项修改为{ &quot;display&quot;: &quot;5 seconds&quot;, &quot;pause&quot;: false, &quot;value&quot;: 5000 }，然后就可以看到日志五秒刷新一次，不断更新。 以上就是ELK栈收集、处理、展示日志最简单的demo。","categories":[{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"容器云","slug":"容器云","permalink":"http://beritra.github.com/tags/%E5%AE%B9%E5%99%A8%E4%BA%91/"},{"name":"ELK","slug":"ELK","permalink":"http://beritra.github.com/tags/ELK/"},{"name":"日志","slug":"日志","permalink":"http://beritra.github.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"运维","slug":"运维","permalink":"http://beritra.github.com/tags/%E8%BF%90%E7%BB%B4/"}]},{"title":"dockerfile指南及最佳实践","slug":"dockerfile指南及最佳实践","date":"2018-01-11T01:45:34.000Z","updated":"2019-12-15T14:34:22.075Z","comments":true,"path":"2018/01/11/dockerfile指南及最佳实践/","link":"","permalink":"http://beritra.github.com/2018/01/11/dockerfile%E6%8C%87%E5%8D%97%E5%8F%8A%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"学习Docker的时候遇到了不少问题，也有同事询问的时候不能回答上来，所以系统的记录一些Docker原理方面的学习过程。先从dockerfile入手，本篇主要是官网文档的翻译。","text":"学习Docker的时候遇到了不少问题，也有同事询问的时候不能回答上来，所以系统的记录一些Docker原理方面的学习过程。先从dockerfile入手，本篇主要是官网文档的翻译。 Docker的内核基础提到Docker，基本都知道其本质是宿主机上面的一个进程，通过namespace实现了资源隔离。通过cgroups实现了资源限制，通过写时复制（copy-on-write）实现了高效的文件操作。从Linux内核3.8版本开始，提供了namespace功能，主要分为以下六项隔离： UTS：主机名与域名 IPC：信号量、消息队列和共享内存 PID：进程编号 Network：网络设备、网络栈、端口等 Mount：挂载点（文件系统） User：用户和用户组 Docker自底向上的结构构建一个Docker应用可以分为以下三层： Stack Service Container 使用Dockerfile定义一个容器Dockerfile定义了你的容器内的环境发生了什么。在这个环境里，资源的获取比如网络接口或者磁盘驱动都是虚拟化的，并且与你系统的其他部分是隔离开的，所以你必须将端口映射到外面，而且你必须制定那些文件需要“复制”到这个环境以内。 Docker可以通过Dockerfile的命令构建一个镜像，使用Docker build命令可以创建一个连续的命令行指令进行自动构建。 Dockerfile指南用法docker build命令从Dockerfile和上下文中构建镜像。构建的上下文是在特定位置的文件的集合，比如PATH和URL，PATH是你本地文件系统的目录，URL是git仓库地址。 上下文是递归处理的，所以PATH包括了子目录，URL也包括了仓库和它的子模块。比如把整个当前路径都作为上下文： 1$ docker build .2Sending build context to Docker daemon 6.51 MB3··· 为了在构建上下文的时候使用一个文件，Dockerfile使用一个命令去指定某个文件，比如COPY命令。为了增加构建过程的性能，可以通过添加.dockerignore来排除某些文件。 通常情况下，Dockerfile就叫Dockerfile，你可以使用-f命令指定使用某个Dockerfile,用法如下： 1docker build -f &#x2F;path&#x2F;to&#x2F;a&#x2F;Dockerfile . 如果构建成功，你也可以指定一个仓库和标签来说明在哪里存储你的镜像： 1docker build -t shykes&#x2F;myapp . 为了在构建完成之后，给镜像标注多个仓库，可以添加多个-t: 1docker build -t shykes&#x2F;myapp:1.0.2 -t shykes&#x2F;myapp:latest . 在Docker守护进程执行Dockerfile中的命令之前，它会执行一个初步的验证，如果语法不正确会返回错误： 1$ docker build -t test&#x2F;myapp .2Sending build context to Docker daemon 2.048 kB3Error response from daemon: Unknown instruction: RUNCMD Docker守护进程会依次逐条执行Dockerfile中的命令，在返回最终镜像的ID之前，如果需要，每一条命令的结果都会提交成为一个新的镜像。Docker守护进程会自动清理你发送的上下文。 需要注意，每一条命令都是独立运行的，而且会导致新的镜像被创建，所以RUN cd /tmp不会在对下一条命令产生任何影响。 如果可能，Docker会重新利用中间镜像（缓存），从而让docker build过程显著加快。你可以从控制台输出中看到Using cache的提示。（想要了解更多信息，参照Dockerfile最佳实践中的构建缓存部分） 1$ docker build -t svendowideit&#x2F;ambassador .2Sending build context to Docker daemon 15.36 kB3Step 1&#x2F;4 : FROM alpine:3.24 ---&gt; 31f630c650715Step 2&#x2F;4 : MAINTAINER SvenDowideit@home.org.au6 ---&gt; Using cache7 ---&gt; 2a1c91448f5f8Step 3&#x2F;4 : RUN apk update &amp;&amp; apk add socat &amp;&amp; rm -r &#x2F;var&#x2F;cache&#x2F;9 ---&gt; Using cache10 ---&gt; 21ed6e7fbb7311Step 4&#x2F;4 : CMD env | grep _TCP&#x3D; | (sed &#39;s&#x2F;.*_PORT_\\([0-9]*\\)_TCP&#x3D;tcp:\\&#x2F;\\&#x2F;\\(.*\\):\\(.*\\)&#x2F;socat -t 100000000 TCP4-LISTEN:\\1,fork,reuseaddr TCP4:\\2:\\3 \\&amp;&#x2F;&#39; &amp;&amp; echo wait) | sh12 ---&gt; Using cache13 ---&gt; 7ea8aef582cc14Successfully built 7ea8aef582cc 构建缓存只有在那些拥有本地父母链（local parent chain）的镜像中被使用。这意味着那些镜像是被前一阶段的构建产物所创造，或者被docker load装载的整个镜像链条所创造。如果你想指定某一个镜像使用构建缓存，你可以用--cache-from选项来指定。 完成你的构建过程之后，你就可以准备浏览将存储仓库推入注册 格式Dockerfile的格式是这样的： 1# Comment2INSTRUCTION arguments 指令是大小写不敏感的，但是惯例是把指令都写成大写，方便与参数区分开。 Docker会按照顺序执行Dockerfile中的指令，一个Dockerfile必须从FROM命令开始，FROM命令指定了一个你要构建的基础镜像。 Docker会把以#为起始的一行视为注释，除非这一行是一个有效的解析指令(parse directives)。一个在其他位置出现的#会被当做是参数的一部分，所以允许这样的语句： 1# Comment2RUN echo &#39;we are running some # of cool things&#39; 解析指令 Parse directives解析指令是可选的，而且会影响Dockerfile中后续指令行的处理方式。解析指令不会增加新的构建层数，也不会被认为是一个构建步骤。它的写法是类似于特殊的注释：# directive=value一个解析指令只被使用一次。 每当一个注释、空行或者构建指令被处理之后，Docker不会再去寻找解析指令。取而代之的是它会把解析指令格式的命令行视为一个注释，而且不会尝试去验证这是否是一个解析指令。因此，所有的解析指令都应该在Dockerfile的最上方。 解析指令大小写不敏感，按照惯例我们写成小写格式，而且后面添加一个空行。解析指令不支持行延长符号，所以如下是无效的： 1# direc \\2tive&#x3D;value 重复出现两次也是无效的: 1# directive&#x3D;value12# directive&#x3D;value234FROM ImageName 出现在构建指令之后的解析指令会被当做普通注释： 1FROM ImageName2# directive&#x3D;value 出现在普通注释之后的解析指令也会被当做普通注释： 1# About my dockerfile2# directive&#x3D;value3FROM ImageName 非法的解析指令会被当做普通注释，此外，紧随其后的合法解析指令也会被当做注释，因为出现在一条注释之后。 1# unknowndirective&#x3D;value2# knowndirective&#x3D;value 不换行空格允许出现在解析指令之中。因此，下列行被认为已知： 1#directive&#x3D;value2# directive &#x3D;value3# directive&#x3D; value4# directive &#x3D; value5# dIrEcTiVe&#x3D;value 支持以下解析指令：escape escape1# escape&#x3D;\\ (backslash) 或 1# escape&#x3D;&#96; (backtick) escape指令指定Dockerfile中的转义字符，如果没有指定，默认的转义字符是\\. 转义字符不仅作用在行中的转义字符，也作用在换行符。这允许一个Dockerfile指令跨越多行。需要注意，无论escape解释语句是否出现在Dockerfile中，转义不会在RUN命令中生效，除非在行尾。 将转义字符设置为`在Windows中尤其有用，因为\\是地址分隔符，`与Windows PowerShell相一致。 考虑到下面的例子会在Windows中以一个不明显的方式失败。在第二行结束位置的第二个\\会被当做换行符，而不是第一个\\转义的目标。同样，在第三行末尾位置的\\，假设它实际上是作用为一个指令，他被当做一个行延长符。这个Dockerfile的结果是第二行和第三行被当做一个单独的指令。 1FROM microsoft&#x2F;nanoserver2COPY testfile.txt c:\\\\3RUN dir c:\\ 结果： 1PS C:\\John&gt; docker build -t cmd .2Sending build context to Docker daemon 3.072 kB3Step 1&#x2F;2 : FROM microsoft&#x2F;nanoserver4 ---&gt; 22738ff49c6d5Step 2&#x2F;2 : COPY testfile.txt c:\\RUN dir c:6GetFileAttributesEx c:RUN: The system cannot find the file specified.7PS C:\\John&gt; 一个解决方案是使用/作为COPY指令和dir指令的目标。然而最好的情况下，这个语法在Windows上也并不自然，令人困惑，在比较坏的情况下，在Windows上使用/作为地址分隔符有可能出现错误。 通过添加escape解析指令，下面的Dockerfile成功的使用原生的系统地址分割语法如期执行： 1# escape&#x3D;&#96;23FROM microsoft&#x2F;nanoserver4COPY testfile.txt c:\\5RUN dir c:\\ 结果如下： 1PS C:\\John&gt; docker build -t succeeds --no-cache&#x3D;true .2Sending build context to Docker daemon 3.072 kB3Step 1&#x2F;3 : FROM microsoft&#x2F;nanoserver4 ---&gt; 22738ff49c6d5Step 2&#x2F;3 : COPY testfile.txt c:\\6 ---&gt; 96655de338de7Removing intermediate container 4db9acbb16828Step 3&#x2F;3 : RUN dir c:\\9 ---&gt; Running in a2c157f842f510 Volume in drive C has no label.11 Volume Serial Number is 7E6D-E0F71213 Directory of c:\\141510&#x2F;05&#x2F;2016 05:04 PM 1,894 License.txt1610&#x2F;05&#x2F;2016 02:22 PM &lt;DIR&gt; Program Files1710&#x2F;05&#x2F;2016 02:14 PM &lt;DIR&gt; Program Files (x86)1810&#x2F;28&#x2F;2016 11:18 AM 62 testfile.txt1910&#x2F;28&#x2F;2016 11:20 AM &lt;DIR&gt; Users2010&#x2F;28&#x2F;2016 11:20 AM &lt;DIR&gt; Windows21 2 File(s) 1,956 bytes22 4 Dir(s) 21,259,096,064 bytes free23 ---&gt; 01c7f3bef04f24Removing intermediate container a2c157f842f525Successfully built 01c7f3bef04f26PS C:\\John&gt; 环境替换环境变量（通过ENV命令声明的）也可以在用在某一指令中，像一个变量一样被Dockerfile解释。转义也被用作将类变量语法逐字逐句的包含到声明中。（Environment variables (declared with the ENV statement) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally.） 环境变量在Dockerfile中会表示为$variable_name或者${variable_name}。这两种表述方式等价，其中大括号方式通常被用作表述没有空格的变量名字，像${foo}_bar。 ${variable_name}这种语法同样支持几种标准的bash编辑方式如下： ${variable:-word}表示如果变量是一个集合，那么结果就是集合的值，否则结果是word。 ${variable:+word}表示如果变量是一个集合，那么结果是word，否则是空字符串。 在所有情况下，word可以使任何字符串，包括额外的环境变量。 可以通过在变量前添加\\进行转义：\\$foo或者\\${foo}。比如下面的例子，将会对$foo和${foo}逐字严格各自转换。 1FROM busybox2ENV foo &#x2F;bar3WORKDIR $&#123;foo&#125; # WORKDIR &#x2F;bar4ADD . $foo # ADD . &#x2F;bar5COPY \\$foo &#x2F;quux # COPY $foo &#x2F;quux 环境变量在所有下列指令中被支持： ADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUMN WORKDIR 同样的： ONBUILD（与其他上述指令一同使用的时候） 在整个指令中，环境变量替代物会为每一个变量使用同一个值。如下所示： 1ENV abc&#x3D;hello2ENV abc&#x3D;bye def&#x3D;$abc3ENV ghi&#x3D;$abc 结果是def的值为hello，而不是bye。然而，ghi的值是bye因为这不是将abc设置为bye的那条语句。 .dockerignore file在docker命令行把上下文发送给docker守护进程之前，它会在上下文路径的根目录下搜索文件名为.dockerignore的文件。如果文件存在，命令行就会把符合的文件从上下文中排除出去。 .dockerignore文件的示例如下： 1# comment2*&#x2F;temp*3*&#x2F;*&#x2F;temp*4temp? 这个文件会产生如下构建行为：命令|行为—|—#comment|忽略*/temp*|排除根路径下所有直接子目录内文件名或者目录名以temp为开头的文件或文件夹。比如：/somedir/temporary.txt就被排除了,或者路径/somedir/temp。*/*/temp*|排除所有根路径下二级子目录中以temp为起始文件名的文件或文件夹。比如：/somedir/subdir/temporary.txt。temp?|排除根目录下文件名或目录名是temp后面加一个字符。比如：/tempa和tempb。 具体用法和.gitignore类似，不再赘述。 FROM1FROM &lt;image&gt; [AS &lt;name&gt;] 或者 1FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;] 或者 1FROM &lt;image&gt;[@&lt;digest&gt;] [AS &lt;name&gt;] FROM指令初始化了一个新的构建阶段，而且为后续的指令准备了基础镜像。所以，一个有效的Dockerfile应该以FROM指令起始。镜像可以是任何有效的镜像，尤其简单的是可以从公共仓库下载镜像作为开始。 ARG是唯一可以出现在FROM之前的指令。 在一个Dockerfile中，FROM可以出现多次，创建多个镜像，或者把一个构建阶段作为另一个的依赖。只需要注意，在一条新的FROM指令提交之前把上一个的镜像ID记录下来。每一个FROM指令会清除之前指令创造的所有状态。 可选的，使用AS name语句可以赋予新的构建阶段可以一个名称。这个名称还可以在随后的FROM和COPY --from=&lt;name|index&gt;中使用以指定某个镜像。 tag或者digest选项是可选的。如果你都省略了，系统会缺省设置latest标签。如果找不到任何tag的值，构建器会返回一个错误。 理解ARG和FROM是如何相互需作用的FROM指令支持那些由ARG声明的，出现在自身之前的变量。 1ARG CODE_VERSION&#x3D;latest2FROM base:$&#123;CODE_VERSION&#125;3CMD &#x2F;code&#x2F;run-app45FROM extras:$&#123;CODE_VERSION&#125;6CMD &#x2F;code&#x2F;run-extras 一个FROM指令之前的ARG声明是独立于构建阶段之外的，所以不能在FROM之后的任何指令中使用。想要使用FROM指令之前的ARG指令的默认属性，你需要在构建阶段之内再使用一次不带赋值的ARG指令： 1ARG VERSION&#x3D;latest2FROM busybox:$VERSION3ARG VERSION4RUN echo $VERSION &gt; image_version RUNRUN命令有两种格式; RUN &lt;command&gt;（shell格式，命令在shell中执行，默认是Linux中的/bin/sh -c后者Windows中的cmd /s /c） RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]（执行格式） RUN指令会在当前镜像之上的新分层中执行任何命令，然后提交结果。产生的被提交镜像会在Dockerfile中的后续步骤中使用。 分层的RUN指令和不断产生的提交符合Docker的核心思想：提交应当是简易的，容器可以从镜像的历史中的任何一个时间点创建，这点很像代码控制。 执行格式避免了shell字符歧义，而且你可以不指定使用特定的shell可执行文件（bash or sh or ?）的情况下使用RUN指令。 你可以在shell格式中使用SEHLL命令指定使用特定的shell。 在shell格式中你可以使用\\（反斜杠）来在多行中延续一条RUN指令，比如： 1RUN &#x2F;bin&#x2F;bash -c &#39;source $HOME&#x2F;.bashrc; \\2echo $HOME&#39; 等价于： 1RUN &#x2F;bin&#x2F;bash -c &#39;source $HOME&#x2F;.bashrc; echo $HOME&#39; 注意： 要使用不同的shell，而不是’/bin/sh’，请使用在所需shell中传递的exec形式。例如：RUN [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;echo hello&quot;] 注意： exec形式作为JSON数组解析，这意味着您必须在单词之外使用双引号（”）而不是单引号（’）。 注意： 与shell格式不同，exec格式不调用命令shell。这意味着正常的shell处理不会发生。例如，RUN [“echo”,”$HOME”]不会在$HOME上进行可变替换。如果你想要shell处理，那么使用shell形式或直接执行一个shell，例如：RUN [“sh”,”-c”,”echo $HOME”]。当使用exec形式并直接执行shell时，正如shell形式的情况，它是做环境变量扩展的shell，而不是docker。 注意： 在JSON形式中，有必要转义反斜杠。这在Windows上特别相关，其中反斜杠是路径分隔符。因为不是有效的JSON，并且以意外的方式失败，以下行将被视为shell形式：RUN [&quot;c:\\windows\\system32\\tasklist.exe&quot;]此示例的正确语法为：RUN [&quot;c:\\\\windows\\\\system32\\\\tasklist.exe&quot;] RUN指令的缓存不会在下一次构建期间自动失效。一条指令的缓存类似RUN apt-get dist-upgrade -y会在下一个次构建的时候重用。RUN指令的缓存可以使用--no-cache标志取消，比如docker build --no-cache。 在Dockerfile最佳实践中看到更多的信息。 ADD指令也会使RUN指令的缓存失效，详情见下。 CMDCMD指令有三种格式： CMD [“executable”,”param1”,”param2”]（执行格式，这也是首选的格式） CMD [“param1”,”param2”]（作为ENTRYPOINT的默认参数） CMD command param1 param2（shell格式） Dockerfile中只能有一条CMD命令，如果有多条，那么只有最后一条生效。 CMD指令的主要用意是提供一个执行容器的默认值。这些默认值可以包括一个可执行文件，或者他们可以省略可执行文件。在这种情况下你必须指定一个ENTRTPOINT指令。 注意： 如果使用CMD为ENTRYPOINT指令提供默认参数，CMD和ENTRYPOINT指令都应以JSON数组格式指定。 注意： exec形式作为JSON数组解析，这意味着您必须在单词之外使用双引号（”）而不是单引号（’）。 注意： 与shell表单不同，exec表单不调用命令shell。这意味着正常的shell处理不会发生。例如，CMD [&quot;echo&quot;，&quot;$HOME&quot;]不会在$HOME上进行可变替换。如果你想要shell处理，那么使用shell形式或直接执行一个shell，例如：CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;echo $HOME&quot;]。当使用exec形式并直接执行shell时，正如shell形式的情况，它是做环境变量扩展的shell，而不是docker。 当你使用shell或者exec格式的时候，CMD指令会在容器启动的时候执行你输入的命令。 如果你使用CMD的shell格式，那么命令会以/bin/sh -c的形式执行： 1FROM ubuntu2CMD echo &quot;This is a test.&quot; | wc - 如果你想执行你的命令但是不指定某个shell，你必须用JSON形式表述你的命令，而且给出一个可执行文件的完整地址。这种数组形式是CMD的首选格式。任何附加的参数必须单独表述为数组中的字符串： 1FROM ubuntu2CMD [&quot;&#x2F;usr&#x2F;bin&#x2F;wc&quot;,&quot;--help&quot;] 如果你想要你的容器每次都运行相同的可执行文件，那你应该考虑将ENTRYPOINT指令和CMD一起使用。 如果用户指定了docker run的参数，那么这些参数就会覆盖掉CMD中指定的命令。 注意： 不用疑惑RUN和CMD的区别。RUN其实是运行了一条指令然后提交结果；CMD不在构建阶段执行任何指令，但是为镜像准备好指令（意思是说在docker run的时候执行）。 LABEL1LABEL &lt;key&gt;&#x3D;&lt;value&gt; &lt;key&gt;&#x3D;&lt;value&gt; &lt;key&gt;&#x3D;&lt;value&gt; ... LABEL指令添加镜像的元数据。一个LABEL就是一组键值对。想要在LABEL的值中添加空格，要像命令行中的转义那样使用引号和反斜杠，例子如下： 1LABEL &quot;com.example.vendor&quot;&#x3D;&quot;ACME Incorporated&quot;2LABEL com.example.label-with-value&#x3D;&quot;foo&quot;3LABEL version&#x3D;&quot;1.0&quot;4LABEL description&#x3D;&quot;This text illustrates \\5that label-values can span multiple lines.&quot; 一个镜像可以有多个标签，你可以在一行内指定多个标签。在Docker 1.10之前，这么做会减小最终镜像的大小，但是现在不再如此。你仍然可以选择在一条指令内指定多个标签，如下两个例子所示： 1LABEL multi.label1&#x3D;&quot;value1&quot; multi.label2&#x3D;&quot;value2&quot; other&#x3D;&quot;value3&quot; 1LABEL multi.label1&#x3D;&quot;value1&quot; \\2 multi.label2&#x3D;&quot;value2&quot; \\3 other&#x3D;&quot;value3&quot; 基础镜像或者双亲镜像（FROM中指定的镜像）中包含的标签会被你的镜像继承。如果一个标签已经存在但是有不同的值，会以最近设置的值为准，之前的会被覆盖。想要查看一个镜像的所有标签，可以使用docker inspect命令。 1&quot;Labels&quot;: &#123;2 &quot;com.example.vendor&quot;: &quot;ACME Incorporated&quot;3 &quot;com.example.label-with-value&quot;: &quot;foo&quot;,4 &quot;version&quot;: &quot;1.0&quot;,5 &quot;description&quot;: &quot;This text illustrates that label-values can span multiple lines.&quot;,6 &quot;multi.label1&quot;: &quot;value1&quot;,7 &quot;multi.label2&quot;: &quot;value2&quot;,8 &quot;other&quot;: &quot;value3&quot;9&#125;, MAINTAINER(弃用)标注作者名，已经弃用，推荐使用LABEL代替。 EXPOSE1EXPOSE &lt;port&gt; [&lt;port&gt;&#x2F;&lt;protocol&gt;...] EXPOSE指令提示Docker，容器会在运行时监听特定的网络端口。你可以指定端口是监听TCP还是UDP，而且如果没有指定，那么默认为TCP。 EXPOSE指令实际上不发布端口。它的作用是作为一个镜像构建者和容器使用者之间的文档，告知哪一个端口应当被发布。实际上想要在运行容器的时候发布端口，应当在docker run命令中使用-p参数以发布或者映射一个或者多个端口，或者用-P参数发布所有暴露的端口并且映射它们到高优先级（high-order）的端口上。 想要建立宿主机的端口重定向，你需要看文档中的using -P flag。docker netword命令支持在免于暴露特定端口的情况下建立容器之间的网络通信，因为容器可以通过任意的端口连接到网络。 ENV1ENV &lt;key&gt; &lt;value&gt;2ENV &lt;key&gt;&#x3D;&lt;value&gt; ... ENV指令把环境变量的key设为value。这个值会在所有Dockerfile的后续命令中存在，而且支持上面提到的环境变量替换。 ENV指令有两种格式，第一种格式，ENV &lt;key&gt; &lt;value&gt;，会把一个单一的键设为某一值，在空格后面出现的整个字符串会被认为是值，包括空格和引号等字符。 第二种格式，ENV &lt;key&gt;=&lt;value&gt; ...，允许一次性设置多个值。需要注意第二种方式的语法中使用等号，而第一种没有使用。就像命令行转义，引号和反斜杠可以用来包含值中的空格。如下： 1ENV myName&#x3D;&quot;John Doe&quot; myDog&#x3D;Rex\\ The\\ Dog \\2 myCat&#x3D;fluffy 或 1ENV myName John Doe2ENV myDog Rex The Dog3ENV myCat fluffy 两者作用相同，但是推荐使用第一种，因为只会产生一层缓存层。 ENV命令设置的环境变量会一直存在，包括容器从最终镜像运行后。你可以使用docker inspect查看它们的状态，也可以使用docker run --env &lt;key&gt;=&lt;value&gt;改变它们的状态。 注意： 环境变量的持续存留有可能产生不可预期的副作用。比如设置ENV DEBIAN_FRONTEND noninteractive有可能让基于Debian镜像的apt-get用户产生困惑。想要为单条命令设置一个值，使用RUN &lt;key&gt;=&lt;value&gt; &lt;command&gt;。 ADDADD有两种格式： ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt; ADD [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;](包含空格的路径需要使用这种格式) 注意： --chown特性只支持构建Linux容器的Dockerfile，而且不会在Windows容器中生效。由于用户和用户组概念不能再Linux和Windows之间转化，所以使用/etc/passwd和/etc/group把用户和用户组转化为ID这种特性只在基于Linux的容器下可行。 ADD指令从&lt;src&gt;复制新的文件、路径或者远程文件URL到镜像的文件系统中的&lt;dest&gt;位置。 可以指定多个&lt;src&gt;资源，但是如果他们是文件或者目录的话，他们的路径应该是相对于构建上下文的。 每一个&lt;src&gt;可以包括通配符，适配规则将按照Go语言的filepath.Match规则。比如： 1ADD hom* &#x2F;mydir&#x2F; # 添加所有文件名以 &quot;hom&quot; 开头的文件2ADD hom?.txt &#x2F;mydir&#x2F; # ? 可以替代一个单个字符，比如， &quot;home.txt&quot; &lt;dest&gt;是一个绝对路径，或者相对于WORKDIR的路径，在其中，资源将被复制到目标容器。 1ADD test relativeDir&#x2F; # adds &quot;test&quot; to &#96;WORKDIR&#96;&#x2F;relativeDir&#x2F;2ADD test &#x2F;absoluteDir&#x2F; # adds &quot;test&quot; to &#x2F;absoluteDir&#x2F; 当添加一个包含特殊符号(比如[和])的文件或者目录，你需要对这些字符进行符合Golang规则的转义，防止它们被视为一个统配规则。比如，想要添加一个文件名为arr[0].txt的文件，你需要： 1ADD arr[[]0].txt &#x2F;mydir&#x2F; # 复制一个名为 &quot;arr[0].txt&quot; 的文件到 &#x2F;mydir&#x2F; 一个新的文件或者目录被创造的时候，它的UID和GID将为0，除非用选项--chown指定了用户和用户组或者UID/GID组合请求指定的内容添加所有权。--chown选线的格式允许用户名或者用户组名以字符串或者整形形式的UID或GID组合。提供一个不带用户组的用户名或者不带GID的UID的话，会使用同样的UID作为GID。如果提供了一个用户名或者用户组名，容器的根目录文件系统/etc/passwd和/etc/group文件会被使用，来执行从名称到UID或GID的转换。下列栗子展示了合法的--chown选项的定义： 1ADD --chown&#x3D;55:mygroup files* &#x2F;somedir&#x2F;2ADD --chown&#x3D;bin files* &#x2F;somedir&#x2F;3ADD --chown&#x3D;1 files* &#x2F;somedir&#x2F;4ADD --chown&#x3D;10:11 files* &#x2F;somedir&#x2F; 如果容器的根目录文件系统不包含/etc/passwd或者/etc/group文件并且用户名和用户组名都没有使用--chown选项，那么进行ADD操作的时候构建将会失败。使用数字的IDs不需要查找，而且对根文件系统的内容没有要求。 在&lt;src&gt;是远程文件URL的情况下，目标将具有600的权限。如果正在检索的远程文件具有HTTP Last-Modified标头，则来自该标头的时间戳将用于设置目的地上的mtime文件。然而，像在ADD期间处理的任何其它文件一样，在决定文件是否被更改或者缓存是否被更新的时候，mtime不会被考虑进去。 注意： 如果通过传递一个Dockerfile通过STDIN（docker build - &lt;somefile）构建，没有构建上下文，所以Dockerfile只能包含一个基于URL的ADD指令。您还可以通过STDIN传递压缩归档文件：（docker build - &lt;archive.tar.gz），归档根目录下的Dockerfile和归档的其余部分将在构建的上下文中使用。 注意： 如果您的URL文件使用身份验证保护，则您需要使用RUN wget，RUN curl或从容器内使用其他工具，因为ADD指令不支持身份验证。 注意： 如果&lt;src&gt;的内容已更改，第一个遇到的ADD指令将使来自Dockerfile的所有后续指令的高速缓存无效。这包括使用于RUN指令的高速缓存无效。有关详细信息，请参阅Dockerfile最佳实践指南。 ADD遵循以下规则： &lt;src&gt;路径必须在构建的上下文中;你不能ADD ../something /something，因为docker构建的第一步是发送上下文目录（和子目录）到docker守护进程。如果&lt;src&gt;是URL并且&lt;dest&gt;不以尾部斜杠结尾，则从URL下载文件并将其复制到&lt;dest&gt;。 如果&lt;src&gt;是URL并且&lt;dest&gt;以尾部斜杠结尾，则从URL中推断文件名，并将文件下载到&lt;dest&gt;/&lt;filename&gt;。例如，ADD http://example.com/foobar /会创建文件/foobar。网址必须有一个非平凡的路径，以便在这种情况下可以发现一个适当的文件名（http://example.com不会工作）。 如果&lt;src&gt;是目录，则复制目录的整个内容，包括文件系统元数据。 注意： 目录本身不被复制，只是其内容。 如果&lt;src&gt;是识别的压缩格式（identity，gzip，bzip2或xz）的本地tar存档，则将其解包为目录。来自远程URL的资源不会解压缩。当目录被复制或解压缩时，它具有与tar -x相同的行为：结果是以下的联合： 无论目的地路径上存在什么，而且 原目标树的内容，冲突以逐个文件为基础解析为“2.”。 注意： 文件是否被识别为识别的压缩格式，仅基于文件的内容，而不是文件的名称。例如，如果一个空文件以.tar.gz结尾，则不会被识别为压缩文件，并且不会生成任何解压缩错误消息，而是将该文件简单地复制到目的地。 如果&lt;src&gt;是任何其他类型的文件，它会与其元数据一起单独复制。在这种情况下，如果&lt;dest&gt;以尾部斜杠/结尾，它将被认为是一个目录，并且&lt;src&gt;的内容将被写在&lt;dest&gt;/base(&lt;src&gt;)。 如果直接或由于使用通配符指定了多个&lt;src&gt;资源，则&lt;dest&gt;必须是目录，并且必须以斜杠/结尾。 如果&lt;dest&gt;不以尾部斜杠结尾，它将被视为常规文件，&lt;src&gt;的内容将写在&lt;dest&gt;。 如果&lt;dest&gt;不存在，则会与其路径中的所有缺少的目录一起创建。 COPYCOPY有两种格式： COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;](带空格的路径需要使用这种格式) 注意： --chown特性只支持构建Linux容器的Dockerfile，而且不会在Windows容器中生效。由于用户和用户组概念不能再Linux和Windows之间转化，所以使用/etc/passwd和/etc/group把用户和用户组转化为ID这种特性只在基于Linux的容器下可行。 COPY指令从&lt;src&gt;复制新的文件、路径或者远程文件URL到容器的文件系统中的&lt;dest&gt;位置。 可以指定多个&lt;src&gt;资源，但是如果他们是文件或者目录的话，他们的路径应该是相对于构建上下文的。 每一个&lt;src&gt;可以包括通配符，适配规则将按照Go语言的filepath.Match规则。比如： 1COPY hom* &#x2F;mydir&#x2F; # 添加所有文件名以 &quot;hom&quot; 开头的文件2COPY hom?.txt &#x2F;mydir&#x2F; # ? 可以替代一个单个字符，比如， &quot;home.txt&quot; &lt;dest&gt;是一个绝对路径，或者相对于WORKDIR的路径，在其中，资源将被复制到目标容器。 1COPY test relativeDir&#x2F; # adds &quot;test&quot; to &#96;WORKDIR&#96;&#x2F;relativeDir&#x2F;2COPY test &#x2F;absoluteDir&#x2F; # adds &quot;test&quot; to &#x2F;absoluteDir&#x2F; 当添加一个包含特殊符号(比如[和])的文件或者目录，你需要对这些字符进行符合Golang规则的转义，防止它们被视为一个统配规则。比如，想要添加一个文件名为arr[0].txt的文件，你需要： 1COPY arr[[]0].txt &#x2F;mydir&#x2F; # 复制一个名为 &quot;arr[0].txt&quot; 的文件到 &#x2F;mydir&#x2F; 一个新的文件或者目录被创造的时候，它的UID和GID将为0，除非用选项--chown指定了用户和用户组或者UID/GID组合请求指定的内容添加所有权。--chown选线的格式允许用户名或者用户组名以字符串或者整形形式的UID或GID组合。提供一个不带用户组的用户名或者不带GID的UID的话，会使用同样的UID作为GID。如果提供了一个用户名或者用户组名，容器的根目录文件系统/etc/passwd和/etc/group文件会被使用，来执行从名称到UID或GID的转换。下列栗子展示了合法的--chown选项的定义： 1COPY --chown&#x3D;55:mygroup files* &#x2F;somedir&#x2F;2COPY --chown&#x3D;bin files* &#x2F;somedir&#x2F;3COPY --chown&#x3D;1 files* &#x2F;somedir&#x2F;4COPY --chown&#x3D;10:11 files* &#x2F;somedir&#x2F; 如果容器的根目录文件系统不包含/etc/passwd或者/etc/group文件并且用户名和用户组名都没有使用--chown选项，那么进行COPY操作的时候构建将会失败。使用数字的IDs不需要查找，而且对根文件系统的内容没有要求。 注意： 如果你构建的时候使用STDIN(docker build - &lt; somefile)，那么就没有构建上下文，所以COPY不能使用。 COPY可以选择添加--from=&lt;name|index&gt;选项，用来设置源目标位置为前一个构建阶段（用FROM .. AS &lt;name&gt;创建）从而用来代替用户发送的构建上下文。这个选项会接受一个数字索引，这个索引是从FROM指令开始所有之前的构建阶段分配的。 COPY遵循以下规则： &lt;src&gt;路径必须在构建的上下文中;你不能COPY ../something /something，因为docker构建的第一步是发送上下文目录（和子目录）到docker守护进程。 如果&lt;src&gt;是目录，则复制目录的整个内容，包括文件系统元数据。 注意： 目录本身不被复制，只是其内容。 如果&lt;src&gt;是任何其他类型的文件，它会与其元数据一起单独复制。在这种情况下，如果&lt;dest&gt;以尾部斜杠/结尾，它将被认为是一个目录，并且&lt;src&gt;的内容将被写在&lt;dest&gt;/base(&lt;src&gt;)。 如果直接或由于使用通配符指定了多个&lt;src&gt;资源，则&lt;dest&gt;必须是目录，并且必须以斜杠/结尾。 如果&lt;dest&gt;不以尾部斜杠结尾，它将被视为常规文件，&lt;src&gt;的内容将写在&lt;dest&gt;。 如果&lt;dest&gt;不存在，则会与其路径中的所有缺少的目录一起创建。 ENTRYPOPTINTENTRYPOINT有两种形式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;](exec形式，推荐) ENTRYPOINT command param1 param2(shell形式) 一个ENTRYPOINT允许你配置一个将作为可执行文件的容器。 比如，下例将会启动一个默认内容的nginx，监听80端口： 1docker run -i -t --rm -p 80:80 nginx docker run &lt;image&gt;的命令行参数将会在添加在exec格式的ENTRYPOINT的所有参数后面，而且会覆盖所有CMD指令所指定的参数。这允许参数通过，进入到入口点。比如docker run &lt;image&gt; -d会传递-d参数到入口点。你可以通过使用docker run --entrypoint重写ENTRYPOINT指令。 shell格式阻止使用任何CMD或者run命令行参数，但是有缺点就是你的ENTRYPOINT将会从作为/bin/bash -c的子命令开始，这就不会传递信号了。这意味着可执行文件不会成为容器的PID 1，而且不会接受Unix信号，所以你的可执行文件不会收到docker stop &lt;container&gt;的SIGTERM。 只有Dockerfile中最后的ENTRYPOINT指令会生效。 Exec格式的ENTRYPOINT示例你可以使用exec形式的ENTRYPOINT用于设置相当稳定的默认命令行和参数，然后使用任意形式的CMD指令设置额外的、可能被修改的默认命令。 1FROM ubuntu2ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;]3CMD [&quot;-c&quot;] 当你运行一个容器，你可以看到top是唯一的进程： 1$ docker run -it --rm --name test top -H2top - 08:25:00 up 7:27, 0 users, load average: 0.00, 0.01, 0.053Threads: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie4%Cpu(s): 0.1 us, 0.1 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st5KiB Mem: 2056668 total, 1616832 used, 439836 free, 99352 buffers6KiB Swap: 1441840 total, 0 used, 1441840 free. 1324440 cached Mem78 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND9 1 root 20 0 19744 2336 2080 R 0.0 0.1 0:00.04 top 想要检查更多的结果，可以使用docker exec: 1$ docker exec -it test ps aux2USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND3root 1 2.6 0.1 19752 2352 ? Ss+ 08:24 0:00 top -b -H4root 7 0.0 0.1 15572 2164 ? R+ 08:25 0:00 ps aux 而且你可以优雅的使用docker stop test请求关闭top。 下例Dockerfile展示了使用ENTRYPOINT在前台运行Apache（作为PID 1）： 1FROM debian:stable2RUN apt-get update &amp;&amp; apt-get install -y --force-yes apache23EXPOSE 80 4434VOLUME [&quot;&#x2F;var&#x2F;www&quot;, &quot;&#x2F;var&#x2F;log&#x2F;apache2&quot;, &quot;&#x2F;etc&#x2F;apache2&quot;]5ENTRYPOINT [&quot;&#x2F;usr&#x2F;sbin&#x2F;apache2ctl&quot;, &quot;-D&quot;, &quot;FOREGROUND&quot;] 如果你需要写一个开始脚本作为单独可执行文件，你可以保证最终的可执行文件通过exec和gosu命令接受到Unix信号： 1#!&#x2F;usr&#x2F;bin&#x2F;env bash2set -e34if [ &quot;$1&quot; &#x3D; &#39;postgres&#39; ]; then5 chown -R postgres &quot;$PGDATA&quot;67 if [ -z &quot;$(ls -A &quot;$PGDATA&quot;)&quot; ]; then8 gosu postgres initdb9 fi1011 exec gosu postgres &quot;$@&quot;12fi1314exec &quot;$@&quot; 最终，如果你需要在关闭的时候做一些额外的清理工作（或是与其他容器进行通讯），或者联合多个可执行文件，你可能需要确认ENTRYPOINT脚本能够接受到Unix信号并且传递他们，然后做更多工作： 1#!&#x2F;bin&#x2F;sh2# Note: I&#39;ve written this using sh so it works in the busybox container too34# USE the trap if you need to also do manual cleanup after the service is stopped,5# or need to start multiple services in the one container6trap &quot;echo TRAPed signal&quot; HUP INT QUIT TERM78# start service in background here9&#x2F;usr&#x2F;sbin&#x2F;apachectl start1011echo &quot;[hit enter key to exit] or run &#39;docker stop &lt;container&gt;&#39;&quot;12read1314# stop service and clean up here15echo &quot;stopping apache&quot;16&#x2F;usr&#x2F;sbin&#x2F;apachectl stop1718echo &quot;exited $0&quot; 如果你用docker run -it --rm -p 80:80 --name test apache命令运行这个镜像，你可以通过docker exec检查容器的进程，或者用docker top，然后请求脚本来停止Apache： 1$ docker exec -it test ps aux2USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND3root 1 0.1 0.0 4448 692 ? Ss+ 00:42 0:00 &#x2F;bin&#x2F;sh &#x2F;run.sh 123 cmd cmd24root 19 0.0 0.2 71304 4440 ? Ss 00:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start5www-data 20 0.2 0.2 360468 6004 ? Sl 00:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start6www-data 21 0.2 0.2 360468 6000 ? Sl 00:42 0:00 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start7root 81 0.0 0.1 15572 2140 ? R+ 00:44 0:00 ps aux8$ docker top test9PID USER COMMAND1010035 root &#123;run.sh&#125; &#x2F;bin&#x2F;sh &#x2F;run.sh 123 cmd cmd21110054 root &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start1210055 33 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start1310056 33 &#x2F;usr&#x2F;sbin&#x2F;apache2 -k start14$ &#x2F;usr&#x2F;bin&#x2F;time docker stop test15test16real 0m 0.27s17user 0m 0.03s18sys 0m 0.03s 注意： 你可以使用--entrypoint来重写ENTRYPOINT，但是这只会设置二进制到exec(不会使用sh -c)。 注意： exec格式是解析成JSON数组的格式，这意味着你必须使用双引号(“)包围文字，而不是用单引号(‘)。 注意： 与shell格式不同，exec格式不会调用shell命令，这意味着shell处理不会进行。比如，ENTRYPOINT [&quot;echo&quot;,&quot;$HOME&quot;]不会对$HOME进行变量替换。如果你想要使用shell处理，那么就用shell格式或者直接执行shell，比如：ENTRYPOINT [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]。当使用exec形式并直接执行shell时，正如shell形式的情况，它是做环境变量扩展的shell，而不是docker。 Shell形式ENTRYPOINT的示例您可以为ENTRYPOINT指定一个纯字符串，它将在/bin/sh -c中执行。这中形式将使用shell处理来替换shell环境变量，并且将忽略任何CMD或docker run命令行参数。要确保docker stop将正确地发出任何长时间运行的ENTRYPOINT可执行文件，您需要记住用exec启动它： 1FROM ubuntu2ENTRYPOINT exec top -b 当你运行这个景象，你会看到单独一个PID 1进程： 1$ docker run -it --rm --name test top2Mem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cached3CPU: 5% usr 0% sys 0% nic 94% idle 0% io 0% irq 0% sirq4Load average: 0.08 0.03 0.05 2&#x2F;98 65 PID PPID USER STAT VSZ %VSZ %CPU COMMAND6 1 0 root R 3164 0% 0% top -b 这些会在docker stop的时候干净利落的退出： 1$ &#x2F;usr&#x2F;bin&#x2F;time docker stop test2test3real 0m 0.20s4user 0m 0.02s5sys 0m 0.04s 如果你忘记在ENTRYPOINT的开头加上exec： 1FROM ubuntu2ENTRYPOINT top -b3CMD --ignored-param1 你可以运行它（为了下一步的运行，需要给它个名字）: 1$ docker run -it --name test top --ignored-param22Mem: 1704184K used, 352484K free, 0K shrd, 0K buff, 140621524238337K cached3CPU: 9% usr 2% sys 0% nic 88% idle 0% io 0% irq 0% sirq4Load average: 0.01 0.02 0.05 2&#x2F;101 75 PID PPID USER STAT VSZ %VSZ %CPU COMMAND6 1 0 root S 3168 0% 0% &#x2F;bin&#x2F;sh -c top -b cmd cmd27 7 1 root R 3164 0% 0% top -b 你可以看到top的输出信息中指定的ENTRYPOINT不是PID 1。 如果你运行了docker stop test，容器不会退出的很干净，stop命令会在超时之后强制发送一个SIGKILL： 1$ docker exec -it test ps aux2PID USER COMMAND3 1 root &#x2F;bin&#x2F;sh -c top -b cmd cmd24 7 root top -b5 8 root ps aux6$ &#x2F;usr&#x2F;bin&#x2F;time docker stop test7test8real 0m 10.19s9user 0m 0.04s10sys 0m 0.03s 理解CMD和ENTRYPOINT是如何交互的CMD和ENTRYPOINT指令都定义了容器运行的时候命令是如何执行的，下面有几条规则描述了他们的关系： Dockerfile应当是定至少一条CMD或者ENTRYPOINT命令。 使用容器作为一个可执行文件的时候，ENTRYPOINT应当被定义。 CMD应该作为一种为ENTRYPOINT命令定义默认参数的方法，或者作为容器中执行临时命令的方式。 在使用交互式参数运行容器的时候，CMD将会被重写。 下表展示了不同的ENTRYPOINT/CMD组合是如何执行的：||No ENTRYPOINT|ENTRYPOINT exec_entry p1_entry|ENTRYPOINT [“exec_entry”, “p1_entry”]-|-|-|-No CMD|error,not allowed|/bin/sh -c exec_entry p1_entry|exec_entry p1_entryCMD [“exec_cmd”, “p1_cmd”]|exec_cmd p1_cmd|/bin/sh -c exec_entry p1_entry|exec_entry p1_entry exec_cmd p1_cmdCMD [“p1_cmd”, “p2_cmd”]|p1_cmd p2_cmd|/bin/sh -c exec_entry p1_entry|exec_entry p1_entry p1_cmd p2_cmdCMD exec_cmd p1_cmd|/bin/sh -c exec_cmd p1_cmd|/bin/sh -c exec_entry p1_entry|exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd VOLUME1VOLUME [&quot;&#x2F;data&quot;] VOLUME指令创建了一个挂载点，并且指定了一个名字，然后标记这个挂载点来承载来自原生宿主机或者其他容器的额外被挂载数据卷。它的值可以使一个JSON数组，VOLUME [&quot;/var/log/&quot;]，或者一个普通的有多个参数的字符串，比如VOLUME /var/log或VOLUME /var/log /var/db。想看更多的信息/举例和通过Docker客户端进行挂载的指令，可以参考Share Directories via Volumes文档。 docker run命令用基础映像中指定位置存在的任何数据初始化新创建的卷。比如，思考下例Dockerfile片段： 1FROM ubuntu2RUN mkdir &#x2F;myvol3RUN echo &quot;hello world&quot; &gt; &#x2F;myvol&#x2F;greeting4VOLUME &#x2F;myvol 这个Dockerfile产生了一个镜像使docker run创建了一个新的在/myvol的挂载点而且复制greeting文件到新创建的数据卷上。 注意指定数据卷记住下列关于Dockerfile中关于数据卷的内容： 基于Windows的容器的数据卷：当你使用基于Windows的容器的时候，容器的数据卷的目标地址必须是下列之一： 一个不存在的或者是空的目录 除了C:的驱动 改变Dockerfile内的数据卷：如果任何构建步骤在被声明之前修改了数据卷内的数据，那么这些改变将被抛弃。 JSON格式化：这个列表会被解析为一个JSON数组，所以你必须用双引号(“)包含字符，而不是用单引号(“)。 宿主目录是在docker运行时声明：宿主路径(挂载点)，源于它自身的特性，依赖于宿主。这是为了保护镜像的可移植性，因为一个指定的宿主路径不能保证在所有主机上可得。基于这个原因，你不能在Dockerfile中挂载主机目录。VOLUMN指令不支持制定一个host-dir参数。你必须在创建或者运行容器的时候指定一个挂载点。 USER1USER &lt;user&gt;[:&lt;group&gt;] or2USER &lt;UID&gt;[:&lt;GID&gt;] USER指令在使用CMD、RUN或者ENTRYPOINT命令启动一个镜像的时候，设置用户名(或者UID)并且可选择指定用户组(或者GID)。 注意： 当一个用户没有一个首选用户组的时候，镜像（或者下一个指令）将会用root用户组来运行。 在Windows下，必须先创建用户，如果不是內建用户的情况。这可以通过作为Dockerfile的一部分调用的net user命令来完成。 1FROM microsoft&#x2F;windowsservercore2# Create Windows user in the container3RUN net user &#x2F;add patrick4# Set it for subsequent commands5USER patrick WORKDIR1WORKDIR &#x2F;path&#x2F;to&#x2F;workdir WORKDIR指令为Dockerfile中后续的RUN、CMD、ENTRYPOINT、COPY和ADD指令设置工作路径。如果WORKDIR不存在，那么即便没使用Dockerfile中的任何指令，也会被创建。 WORKDIR指令会在Dockerfile中被多次使用。如果提供了一个相关的路径，那么它就会与前面的WORKDIR指令的路径相关联。比如： 1WORKDIR &#x2F;a2WORKDIR b3WORKDIR c4RUN pwd 最终pwd命令的输出结果会是/a/b/c。 WORKDIR指令可以处理之前使用ENV命令设置的环境变量。你可以只使用Dockerfile中明确设置的环境变量。比如： 1ENV DIRPATH &#x2F;path2WORKDIR $DIRPATH&#x2F;$DIRNAME3RUN pwd 最终pwd命令的输出结果会是/path/$DIRNAME ARG1ARG &lt;name&gt;[&#x3D;&lt;default value&gt;] ARG指令定义了一个变量，用户可以使用--build-arg &lt;varname&gt; = &lt;value&gt;标志在构建器中通过docker build命令将其传递给构建器。如果用户指定了一个在Dockerfile中没有定义的构建参数，那么构建器会输出一个警告。 1[Warning] One or more build-args [foo] were not consumed. 一个Dockerfile可以包含一个或者多个ARG指令，比如，下面是一个合法的Dockerfile: 1FROM busybox2ARG user13ARG buildno4... 注意： 不推荐使用构建阶段的变量传递密钥比如github的keys，用户证书等，构建阶段变量对任何用户都是可见的，只要使用docker history命令。 默认值一个ARG指令可以选择包含一个默认值： 1FROM busybox2ARG user1&#x3D;someuser3ARG buildno&#x3D;14... 如果一个ARG指令有一个默认值，而且没有其他值在构建阶段传递，那么构建器就会使用默认值。 作用范围一个ARG变量的定义从它被定义的哪一行开始生效，而不是从命令行或其他地方使用参数的时候开始。比如，考虑下面这个Dockerfile: 11 FROM busybox22 USER $&#123;user:-some_user&#125;33 ARG user44 USER $user5... 一个用户用下面的命令构建了这个文件： 1$ docker build --build-arg user&#x3D;what_user . 在这种情况下，RUN指令使用v1.0.0而不是用户传递的ARG设置：v2.0.1这种行为类似于一个shell脚本，其中一个本地作用域变量覆盖作为参数传递的变量或从环境继承的变量，定义点。 使用上面的示例，但使用不同的ENV规范，您可以在ARG和ENV指令之间创建更有用的交互： 11 FROM ubuntu22 ARG CONT_IMG_VER33 ENV CONT_IMG_VER $&#123;CONT_IMG_VER:-v1.0.0&#125;44 RUN echo $CONT_IMG_VER 与ARG指令不通，ENV的值会在构建镜像的阶段一直存在。考虑一个docker不使用--build-arg参数的构建： 1$ docker build . 使用这个Dockerfile为例，CONT_IMG_VER仍然在镜像中持续存在，但是它的值会是v1.0.0，如同第三行中ENV指令设定的那样。 这个例子中的变量扩展技术允许你从命令行中传递一个参数，并通过利用ENV指令将其保存在最终镜像中。变量扩展只支持有限的Dockerfile指令。 预定义的ARGsDocker 有一组预定义的ARG变量，您可以在Dockerfile中使用没有相应的ARG指令的变量。 HTTP_PROXY http_proxy HTTPS_PROXY https_proxy FTP_PROXY ftp_proxy NO_PROXY no_proxy 想要使用它们，只需要使用命令行传递它们： 1--build-arg &lt;varname&gt;&#x3D;&lt;value&gt; 默认的，这些预定义变量会被docker history的输出结果排除。将它们排除在外可以减少在HTTP_PROXY变量中意外泄漏敏感身份验证信息的风险。 举个栗子，考虑使用下面的语句构建Dockerfile： 1--build-arg HTTP_PROXY&#x3D;http:&#x2F;&#x2F;user:pass@proxy.lon.example.com 1FROM ubuntu2RUN echo &quot;Hello World&quot; 在这种情况下，HTTP_PROXY变量的值在docker history中不可见，而且没有被缓存。如果你本地作了修改，而且你的代理服务器设置成了http://user:pass@proxy.sfo.example.com，后续的构建不会导致缓存未命中。 如果你需要重写这种状况，你可能需要在Dockerfile中添加一个ARG声明，如下： 1FROM ubuntu2ARG HTTP_PROXY3RUN echo &quot;Hello World&quot; 当构建这个Dockerfile的时候，HTTP_PROXY被保存在了docker history中，而且改变了它的值使构建缓存失效。 对构建缓存的影响作为ENV变量，ARG变量不会保留在构建的镜像中。但是，ARG变量确实会以类似的方式影响构建缓存。如果Dockerfile定义了一个ARG变量，其值与以前的版本不同，那么在第一次使用时会发生“缓存未命中”，而不是其定义。尤其是，ARG指令之后的所有RUN指令隐式使用ARG变量（作为环境变量），因此可能导致缓存未命中。除非在Dockerfile中存在匹配的ARG语句，否则所有预定义的ARG变量都可以免于缓存。 举个栗子，考虑下面两个Dockerfile： 11 FROM ubuntu22 ARG CONT_IMG_VER33 RUN echo $CONT_IMG_VER 11 FROM ubuntu22 ARG CONT_IMG_VER33 RUN echo hello 如果在命令行中指定--build-arg CONT_IMG_VER = &lt;value&gt;，则在这两种情况下，第2行的规范不会导致缓存未命中;第3行确实导致缓存未命中.ARG CONT_IMG_VER导致RUN行被识别为与运行CONT_IMG_VER = &lt;value&gt;echo hello相同，因此如果&lt;value&gt;发生更改，则会导致缓存未命中。 考虑同一个命令行下的另一个例子： 11 FROM ubuntu22 ARG CONT_IMG_VER33 ENV CONT_IMG_VER $CONT_IMG_VER44 RUN echo $CONT_IMG_VER 在这个例子中，高速缓存未命中发生在第3行。发生未命中是因为ENV中的变量值引用了ARG变量，并且该变量通过命令行进行了更改。在这个例子中，ENV命令会使镜像包含该值。 如果ENV指令覆盖同名的ARG指令，就像这个Dockerfile： 11 FROM ubuntu22 ARG CONT_IMG_VER33 ENV CONT_IMG_VER hello44 RUN echo $CONT_IMG_VER 第3行不会导致缓存未命中，因为CONT_IMG_VER的值是常量（hello）。因此，RUN（第4行）上使用的环境变量和值在构建之间不会改变。 ONBUILD1ONBUILD [INSTRUCTION] ONBUILD指令为镜像添加一个触发器指令，稍后将该镜像用作另一个构建的基础。触发器将在下游构建的上下文中执行，就像它已经在下游Dockerfile中的FROM指令之后立即插入一样。 任何构建指令都可以注册为触发器。 如果您正在构建将用作构建其他映像的基础的映像，那么这非常有用，例如可以使用用户特定配置自定义的应用程序构建环境或守护程序。 例如，如果您的映像是可重用的Python应用程序构建器，则需要将应用程序源代码添加到特定目录中，并且可能需要在此之后调用构建脚本。你现在不能只调用ADD和RUN，因为你还没有访问应用程序的源代码，每个应用程序的版本都不一样。您可以简单地向应用程序开发人员提供样板化的Dockerfile以复制粘贴到他们的应用程序中，但效率低下，容易出错，难以更新，因为它与特定于应用程序的代码混合在一起。 解决方案是使用ONBUILD注册先行指令，以便在以后的构建阶段运行。 这是它如何工作的： 当遇到ONBUILD指令时，构建器会为正在构建的映像的元数据添加一个触发器。该指令不会影响当前的构建。 在构建结束时，所有触发器的列表都存储在图像清单中的OnBuild键下。可以使用docker inspect命令检查它们。 稍后，可以使用FROM指令将镜像用作新构建的基础。作为处理FROM指令的一部分，下游构建器会查找ONBUILD触发器，并按照它们所注册的相同顺序执行它们。如果任何触发器失败，则FROM指令将被中止，从而导致构建失败。如果所有触发器都成功，则FROM指令完成，构建将像往常一样继续。 触发器在执行后从最终图像中清除。换句话说，他们不是由“大孩子”构建的遗传。 例如，你可能会添加这样的东西： 1[...]2ONBUILD ADD . &#x2F;app&#x2F;src3ONBUILD RUN &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python-build --dir &#x2F;app&#x2F;src4[...] 注意： 不允许使用ONBUILD ONBUILD链接ONBUILD。 注意： ONBUILD指令可能不会触发FROM或者MAINTAINER指令。 STOPSIGNAL1STOPSIGNAL signal STOPSIGNAL指令设置将被发送到容器的系统调用信号以退出。该信号可以是一个有效的无符号数字，与内核的syscall表中的一个位置（例如9）匹配，或者是一个SIGNAME格式的信号名称，例如SIGKILL。 HEALTHCHECKHEALTHCHECK指令有两种形式： HEALTHCHECK [选项] CMD命令（通过在容器中运行一个命令来检查容器的健康状况） HEALTHCHECK NONE（禁用从基础映像继承的任何健康检查） HEALTHCHECK指令告诉Docker如何测试一个容器来检查它是否还在工作。这可以检测到一些情况，例如一个陷入无限循环的web服务器，即使服务器进程仍在运行，也无法处理新的连接。 当容器指定了健康状况检查时，除了正常状态之外，还有一个健康状态。这个状态是最初开始的。每当健康检查通过，它变得健康（无论以前在哪个状态）。经过一定次数的连续失败后，变得不健康。 可以在CMD之前出现的选项是： --interval = DURATION（默认：30秒） --timeout = DURATION（默认：30s） --start-period = DURATION（默认值：0s） --retries = N（默认值：3） 运行状况检查将首先在容器启动后的间隔秒内运行，然后在每次前一次检查完成后再次间隔几秒。 如果单次运行检查花费的时间超过了超过秒数，则认为检查失败。 多次连续的健康检查失败的容器被认为是不健康的。 启动周期为需要时间启动的容器提供初始化时间。在此期间的探测失败不会计入最大重试次数。但是，如果在启动期间运行状况检查成功，则认为容器已启动，并且所有连续的故障都将计入最大重试次数。 Dockerfile中只能有一个HEALTHCHECK指令。如果您列出多个，则只有最后一个HEALTHCHECK将生效。 CMD关键字之后的命令可以是shell命令（例如，HEALTHCHECK CMD / bin / check-running）或exec阵列（与其他Dockerfile命令一样;例如参见ENTRYPOINT以获得详细信息）。 该命令的退出状态表示容器的健康状态。可能的值是： 0：成功 - 容器很健康而且随时可以使用 1：不健康 - 容器运行状态不正常 2：保留 - 不要使用这个退出状态码 例如，要每隔五分钟检查一次，网络服务器是否能够在三秒内为网站的主页提供服务： 1HEALTHCHECK --interval &#x3D; 5m --timeout &#x3D; 3s \\2 CMD curl -f http：&#x2F;&#x2F; localhost &#x2F; ||exit 1 为了帮助调试失败的探测器，命令在stdout或stderr上写入的任何输出文本（UTF-8编码）都将存储在健康状态中，并且可以通过docker检查进行查询。这样的输出应该保持简短（目前只有前4096个字节被存储）。 当容器的运行状况发生变化时，会以新状态生成一个health_status事件。 在Docker 1.12中添加了HEALTHCHECK功能。 SHELL1SHELL [&quot;executable&quot;, &quot;parameters&quot;] SHELL指令允许覆盖用于shell命令形式的默认shell。 Linux上的默认shell是[&quot;/bin/sh&quot;，&quot;-c&quot;]，在Windows上是[&quot;cmd&quot;，&quot;/S&quot;，&quot;/C&quot;]。 SHELL指令必须以JSON格式写入Dockerfile中。 SHELL指令在Windows中有两个常用和完全不同的本机shell特别有用：cmd和powershell，以及可用的备用shell，包括sh。 SHELL指令可以出现多次。每个SHELL指令将覆盖所有先前的SHELL指令，并影响所有后续指令。例如： 1FROM microsoft&#x2F;windowsservercore23# Executed as cmd &#x2F;S &#x2F;C echo default4RUN echo default56# Executed as cmd &#x2F;S &#x2F;C powershell -command Write-Host default7RUN powershell -command Write-Host default89# Executed as powershell -command Write-Host hello10SHELL [&quot;powershell&quot;, &quot;-command&quot;]11RUN Write-Host hello1213# Executed as cmd &#x2F;S &#x2F;C echo hello14SHELL [&quot;cmd&quot;, &quot;&#x2F;S&quot;&quot;, &quot;&#x2F;C&quot;]15RUN echo hello 如果在Dockerfile中使用SHELL指令，则会影响以下指令：RUN，CMD和ENTRYPOINT。 以下示例是在Windows上可以通过使用SHELL指令简化的常见模式： 1...2RUN powershell -command Execute-MyCmdlet -param1 &quot;c:\\foo.txt&quot;3... docker调用的命令是： 1cmd &#x2F;S &#x2F;C powershell -command Execute-MyCmdlet -param1 &quot;c:\\foo.txt&quot; 这是低效率的，原因有两个。首先，调用一个不必要的cmd.exe命令处理器（又名shell）。其次，shell格式中的每条RUN指令都需要在命令前加上一个额外的powershell命令。 为了提高效率，可以采用两种机制之一。一种是使用RUN命令的JSON格式，例如： 1...2RUN [&quot;powershell&quot;，&quot;-command&quot;，&quot;Execute-MyCmdlet&quot;，&quot;-param1 \\&quot;c:\\\\foo.txt\\&quot;&quot;]3... 虽然JSON格式是明确的，并且不使用不必要的cmd.exe，但是通过双引号和转义确实需要更多的冗长。另一种机制是使用SHELL指令和shell形式，为Windows用户提供更自然的语法，特别是与escape转义指令结合使用时： 1# escape&#x3D;&#96;23FROM microsoft&#x2F;nanoserver4SHELL [&quot;powershell&quot;,&quot;-command&quot;]5RUN New-Item -ItemType Directory C:\\Example6ADD Execute-MyCmdlet.ps1 c:\\example\\7RUN c:\\example\\Execute-MyCmdlet -sample &#39;hello world&#39; 结果是： 1PS E:\\docker\\build\\shell&gt; docker build -t shell .2Sending build context to Docker daemon 4.096 kB3Step 1&#x2F;5 : FROM microsoft&#x2F;nanoserver4 ---&gt; 22738ff49c6d5Step 2&#x2F;5 : SHELL powershell -command6 ---&gt; Running in 6fcdb6855ae27 ---&gt; 6331462d43008Removing intermediate container 6fcdb6855ae29Step 3&#x2F;5 : RUN New-Item -ItemType Directory C:\\Example10 ---&gt; Running in d0eef8386e97111213 Directory: C:\\141516Mode LastWriteTime Length Name17---- ------------- ------ ----18d----- 10&#x2F;28&#x2F;2016 11:26 AM Example192021 ---&gt; 3f2fbf1395d922Removing intermediate container d0eef8386e9723Step 4&#x2F;5 : ADD Execute-MyCmdlet.ps1 c:\\example\\24 ---&gt; a955b2621c3125Removing intermediate container b825593d39fc26Step 5&#x2F;5 : RUN c:\\example\\Execute-MyCmdlet &#39;hello world&#39;27 ---&gt; Running in be6d8e63fe7528hello world29 ---&gt; 8e559e9bf42430Removing intermediate container be6d8e63fe7531Successfully built 8e559e9bf42432PS E:\\docker\\build\\shell&gt; SHELL指令也可以用来修改shell的运行方式。例如，在Windows上使用SHELL cmd /S /C /V:ON|OFF，可以修改延迟的环境变量扩展语义。 SHELL指令也可以在Linux上使用，如果需要备用shell，如zsh，csh，tcsh等。 SHELL功能是在Docker 1.12中添加的。 Dockerfile 示例在你可以看到Dockerfile语法之前，如果你对更多的实例感兴趣，可以参考docker文档。 1# Nginx2#3# VERSION 0.0.145FROM ubuntu6LABEL Description&#x3D;&quot;This image is used to start the foobar executable&quot; Vendor&#x3D;&quot;ACME Products&quot; Version&#x3D;&quot;1.0&quot;7RUN apt-get update &amp;&amp; apt-get install -y inotify-tools nginx apache2 openssh-server 1# Firefox over VNC2#3# VERSION 0.345FROM ubuntu67# Install vnc, xvfb in order to create a &#39;fake&#39; display and firefox8RUN apt-get update &amp;&amp; apt-get install -y x11vnc xvfb firefox9RUN mkdir ~&#x2F;.vnc10# Setup a password11RUN x11vnc -storepasswd 1234 ~&#x2F;.vnc&#x2F;passwd12# Autostart firefox (might not be the best way, but it does the trick)13RUN bash -c &#39;echo &quot;firefox&quot; &gt;&gt; &#x2F;.bashrc&#39;1415EXPOSE 590016CMD [&quot;x11vnc&quot;, &quot;-forever&quot;, &quot;-usepw&quot;, &quot;-create&quot;]17# Multiple images example18#19# VERSION 0.12021FROM ubuntu22RUN echo foo &gt; bar23# Will output something like &#x3D;&#x3D;&#x3D;&gt; 907ad6c2736f2425FROM ubuntu26RUN echo moo &gt; oink27# Will output something like &#x3D;&#x3D;&#x3D;&gt; 695d7793cbe42829# You&#39;ll now have two images, 907ad6c2736f with &#x2F;bar, and 695d7793cbe4 with30# &#x2F;oink. Dockerfile 最佳实践Docker可以通过从Dockerfile中阅读 一般参考和建议容器应该是短暂的你Dockerfile中定义的镜像所产生的容器应该是尽可能短暂的。提到“短暂“，我们的意思是它可以停止和被销毁，而且可以用一个最小化的启动和配置流程来构建一个新的替代它。 使用.dockerignore文件当您发出docker build命令时，您所在的当前工作目录称为构建上下文，并且Dockerfile必须位于此构建上下文中的某个位置。默认情况下，它假定位于当前目录中，但是可以使用-f标志指定不同的位置。无论Dockerfile实际存在于哪里，当前目录中的所有文件和目录的递归内容都会作为构建上下文发送到Docker守护进程。无意中包含构建映像所不需要的文件会产生较大的构建上下文和较大的映像大小。这些反过来可以增加构建时间，拉和推图像的时间以及容器的运行时间大小。要查看构建上下文有多大，在构建Dockerfile时查找如下消息。 1Sending build context to Docker daemon 187.8MB 要排除与构建无关的文件，而不重构源代码库，请使用.dockerignore文件。该文件支持类似于.gitignore文件的排除模式。有关创建一个的信息，请参阅.dockerignore文件。除了使用.dockerignore文件之外，请查看以下关于多阶段构建的信息。 使用多阶段构建如果你使用Docker 17.05或者更高版本，你可以使用多阶段构建来彻底减少最终镜像的大小。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://beritra.github.com/categories/Docker/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://beritra.github.com/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"http://beritra.github.com/tags/Docker/"},{"name":"Dockerfile","slug":"Dockerfile","permalink":"http://beritra.github.com/tags/Dockerfile/"}]},{"title":"Java 常见面试问题","slug":"Java常见面试问题","date":"2017-05-07T08:46:29.000Z","updated":"2020-06-06T16:47:58.951Z","comments":true,"path":"2017/05/07/Java常见面试问题/","link":"","permalink":"http://beritra.github.com/2017/05/07/Java%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/","excerpt":"整理下面试常见的，自己可能掌握不牢固的问题。","text":"整理下面试常见的，自己可能掌握不牢固的问题。 多线程现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行？join方法 在java中wait和sleep方法的不同？最大的不同是wait会在等待时释放锁，而sleep会持有锁。wait通常会被用于线程之间的交互，sleep通常被用于暂停执行。 volatile能提供什么保证？volatile变量提供顺序保证和可见性保证，比如JVM或者JIT会为了更好的性能对语句重排顺序，使用volatile修饰的变量即使在没有同步块的情况下赋值也不会与其他语句重拍序。volatile提供happens-before保证，确保一个线程的修改能对其他线程可见。在某些情况下还能提供原子性，比如long和double类型的数据。 Java中能创建volatile数组吗?能，Java中能创建volatile数组，但是只是一个指向数组的引用，而不是整个数组，也就是说如果多个线程同时改变数组的元素，volatile标识符就不能起到保护作用。、 volatile标识符能使一个非原子操作变成一个原子操作吗？一个典型的例子就是一个long类型。Java中读取long类型变量不是原子的，需要分成两步，如果一个线程正在修改该long变量的值，另一个线程可能就只能看到该值的一半（前32位）。但是如果对一个volatile修饰的long或者double变量的读写是原子的。 Java 基础数据类型怎么将byte转换为String？可以使用String接收byte[]参数的构造器来进行转换，需要注意的是使用正确的编码。 1new String(\"你好啊\".getBytes(), StandardCharsets.UTF_8); 我们能将int强制转换位byte类型的变量吗？如果该值大于byte类型的范围，将会出现什么现象？是的，可以强制转换,但是Java中int是32位的，而byte是8位的，所以，如果强制转化，int类型的高24位将会被丢弃，byte类型的范围是-127到128。 Java中的++操作符是线程安全的吗？不是，这个操作实际上涉及三个指令：读取变量值，增加，再存储会内存，这个过程有可能出现多个线程交叉。 3*0.1 == 0.3 将会返回什么？true 还是 false？false，因为有些浮点数不能精确度的表示出来。可以用BigDecimal转换后进行精确计算： 1BigDecimal bigDecimal = new BigDecimal(\"0.2\");2System.out.println(bigDecimal.multiply(new BigDecimal(\"6\")).doubleValue()); 这里注意要用BigDecimal的String构造器，如果是用double类型，仍然是不精确的。参考：Java中浮点型数据Float和Double进行精确计算的问题 可以用Switch中使用String吗？从Java7开始，我们可以在switch case中使用字符串，但这仅仅是一个语法糖。内部实现在switch中使用字符串的hash code。 String和StringBuilder/StringBufferString是只读字符串，它所引用的字符串内容是不能被改变的。而StringBuffer和StringBuilder表示的字符串对象是可以修改的。StringBuilder是JDK1.5引入的，区别在于它是在单线程环境下使用的，所有方面都没有被synchronized修饰，因此它的效率也比StringBuffer高。关于性能问题，有个知乎问题：JAVA 中的 StringBuilder 和 StringBuffer 适用的场景是什么？ 评论区提到，现在用+号拼接字符串已经没有了性能损失，除非在一个循环里面重复执行，因为这样会多次创建StringBuilder 有一个面试题问：有没有哪种情况用 + 做字符串连接比调用 StringBuffer / StringBuilder 对象的 append 方法性能更好？如果连接后得到的字符串在静态存储区中是早已存在的，那么用+做字符串连接是优于 StringBuffer / StringBuilder 的 append 方法的。 集合ArrayList和LinkedList区别是什么ArrayList底层使用数组，随机访问效率高，插入和删除元素的效率低，需要连续空间；LinkedList底层使用链表，随机访问元素效率低，插入和删除效率高，不需要连续空间。 LinkedHashMap 和 PriorityQueue 的区别是什么？PriorityQueue保证最高或者最低优先级的元素总在队列头部，但是LinkedHashMap维持的顺序是元素插入的顺序。当遍历一个PriorityQueue时，没有任何顺序保证，但是LinkedHashMap保证按照插入顺序。 Java中怎么打印数组？用Arrays.toString()和Arrays.deepToString()方法。 TreeMap是用什么实现的？红黑树。 ArrayList和HashMap的默认大小是多少？ArrayList的默认大小是是个元素，HashMap的默认大小是16个元素（必须是2的幂） Iterator和ListIterator的区别是什么？ Iterator可以用来遍历Set和List集合，但是ListIterator只能用来遍历List。 Iterator对集合只能是前向遍历，但是ListIterator可以前向也可以后向。 ListIterator实现了Iterator接口，并且包含其他功能。比如：增加元素，替换元素，获取前一个和后一个元素的索引等等。 快速失败(fail-fast)和安全失败(fail-safe)的区别是什么？Iterator的安全失败是基于对底层集合做拷贝，因此，它不受源集合上修改的影响。java.util包下面的所有集合都是快速失败的，而java.util.current包下面的所有类都是安全失败的。快速失败的迭代器会抛出CurrentModificationException异常，而安全失败的迭代器永远不会抛出这样的异常。 用什么方法来实现集合的排序？可以使用有序集合比如TreeSet或者TreeMap，你也可以使用Collections.sort()来排序 JVM64位JVM中，int的长度是多少位？Java中，int类型变量的长度是一个固定值，与平台无关，都是32位。 对象分配规则 对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC. 大对象直接进入老年代。这么做的目的是避免频繁在Eden区和两个Survivor区之间发生大量的内存拷贝。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过一次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC对象的年龄就会增加一，直到达到阈值，对象进入老年代。 动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移动到老年区的对象的平均大小，如果这个值大于老年区的剩余值则进行一次Full GC，如果小于就检查HandlePromotionFailure设置，设置为true的话只进行Minor GC，如果false则进行Full GC。","categories":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://beritra.github.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"JavaFX简单入门","slug":"JavaFX简单入门","date":"2017-04-13T11:58:39.000Z","updated":"2020-05-20T11:44:42.157Z","comments":true,"path":"2017/04/13/JavaFX简单入门/","link":"","permalink":"http://beritra.github.com/2017/04/13/JavaFX%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/","excerpt":"这几天鼓捣bilibili的直播，想做一个桌面弹窗，方便查看直播消息，开始用Python和PyQt5，后来觉得Python手实在太生，写出来的代码惨不忍睹，不如就用ava写了，找工作也用得上。Java写GUI一向饱受诟病，看了半天也没啥很好的方案，推荐最多的就是JavaFX了。中文资料比较缺乏，只能慢慢摸索了，做到哪里就跟着写到哪里吧，这篇就总结下JavaFx学到的东西。","text":"这几天鼓捣bilibili的直播，想做一个桌面弹窗，方便查看直播消息，开始用Python和PyQt5，后来觉得Python手实在太生，写出来的代码惨不忍睹，不如就用ava写了，找工作也用得上。Java写GUI一向饱受诟病，看了半天也没啥很好的方案，推荐最多的就是JavaFX了。中文资料比较缺乏，只能慢慢摸索了，做到哪里就跟着写到哪里吧，这篇就总结下JavaFx学到的东西。 概述JavaFX是一个强大的图形和多媒体处理工具包集合，和Java一样是跨平台的，在Java8里面已经内置。JavaFX8新的特性（不知道啥意思跟着抄下来装逼就对了）： JavaFX应用程序的默认主题是新设计的Modena主题。详见“关键特性”一节中的Modena主题部分。 已经加入了对HTML5的支持。详见“向JavaFX应用程序中添加HTML内容”相关章节。 新添加的SwingNode类改进了与Swing的互操作性。参考“在JavaFX应用程序中嵌入Swing内容”相关章节。 新的内置UI控件，DatePicker和TableView，已经可用。参考《使用JavaFX UI控件》一文来获得更多信息。 3D图形库被改进了，增加了一些新的API类。参考“关键特性”一节中的3D图形特性部分和“开始使用JavaFX 3D图形”章节来获得更多信息。 print包现在是可用的，并且提供了公开的JavaFX打印API 加入了富本文支持 对Hi-DPI显示的支持已经变得可用了 CSS样式类变成了公开API 引入了调度服务类 JavaFX架构先放一个现成的架构图： Scene 场景图Scene在顶层部分，是构建JavaFX应用的入口，这是一个层级结构的节点树，表示了所有用户界面的视觉元素，可以处理输入，可以被渲染。场景图中的元素被称为一个节点（Node），每个节点有一个ID，样式类和一个包围盒（bounding volime），除了根节点，其他都有一个父节点，零个或者多个子节点。除此之外还有如下特性： 效果（Effects），比如模糊和阴影 不透明度（Opacity） 变换（Transforms） 事件处理器（Event handlers，例如鼠标、键盘和输入法） 应用相关的状态（Application-specific state） 图形系统JavaFX图形系统（Graphics System）是架构图中蓝色部分，是在JavaFX场景图层之下的实现细节。当系统中的图形硬件无法支持硬件加速渲染时，它将提供软件渲染技术。 在JavaFX平台中实现了两套图形加速流水线： Prism用于处理渲染工作。它可以在硬件和软件渲染器之上工作，包括3D。它负责将JavaFX场景进行光栅化和渲染。下面的各种渲染方式都有可能被用到： 在Windows XP和Vista上的DirectX 9 在Windows 7上的DirectX 11 Mac、Linux、嵌入式设备上的OpenGL 当硬件加速技术不支持时使用软件渲染，如果可能将会优先使用硬件加速，但是如果硬件加速不可用则会使用软件渲染，软件渲染技术已经内置于JRE之中。这点在展示3D场景时尤其重要。当然，使用硬件加速时性能将会更好。 Quantum Toolkit 将Prism和Glass Windowing ToolKit绑在一起，使得它们可以被其上层的JavaFX层使用。它也负责管理与渲染有关的事件处理的线程规则。 用法Hello World最简单的一个例子： 1import javafx.application.Application;2import javafx.scene.Group;3import javafx.scene.Scene;4import javafx.scene.paint.Color;5import javafx.stage.Stage;6 7public class Main extends Application &#123;8 9 @Override10 public void start(Stage stage) &#123;11 Group root = new Group();12 Scene scene = new Scene(root, 500, 500, Color.BLACK);13 stage.setTitle(\"JavaFX Scene Graph Demo\");14 stage.setScene(scene);15 stage.show();16 &#125;17 18 public static void main(String[] args) &#123;19 launch(args);20 &#125;21&#125; 参考资料JavaFX中文资料","categories":[{"name":"JavaFX","slug":"JavaFX","permalink":"http://beritra.github.com/categories/JavaFX/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"JavaFX","slug":"JavaFX","permalink":"http://beritra.github.com/tags/JavaFX/"},{"name":"GUI","slug":"GUI","permalink":"http://beritra.github.com/tags/GUI/"}]},{"title":"Spring配置文件","slug":"Spring配置文件","date":"2017-04-07T08:36:42.000Z","updated":"2019-12-15T15:45:03.512Z","comments":true,"path":"2017/04/07/Spring配置文件/","link":"","permalink":"http://beritra.github.com/2017/04/07/Spring%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/","excerpt":"被Spring配置文件搞得不胜其烦，干脆记下来，也不用到处找教程了。","text":"被Spring配置文件搞得不胜其烦，干脆记下来，也不用到处找教程了。 配置内容web.xml1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;2&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"3 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"4 xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\"5 version=\"3.1\"&gt;6 &lt;servlet&gt;7 &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt;8 &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;9 &lt;init-param&gt;10 &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;11 &lt;param-value&gt;WEB-INF/springmvc-servlet.xml&lt;/param-value&gt;12 &lt;/init-param&gt;13 &lt;/servlet&gt;14 &lt;servlet-mapping&gt;15 &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt;16 &lt;url-pattern&gt;/&lt;/url-pattern&gt;17 &lt;/servlet-mapping&gt;18&lt;/web-app&gt; springspringmvc然后是springmvc的配置： 1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;2&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"3 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"4 xmlns:context=\"http://www.springframework.org/schema/context\"5 xmlns:mvc=\"http://www.springframework.org/schema/mvc\"6 xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\"&gt;7 &lt;context:component-scan base-package=\"index\"/&gt;8 &lt;mvc:default-servlet-handler/&gt;9 &lt;mvc:annotation-driven/&gt;10 &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" id=\"internalResourceViewResolver\"&gt;11 &lt;property name=\"prefix\" value=\"/WEB-INF/pages\"/&gt;12 &lt;property name=\"suffix\" value=\".jsp\"/&gt;13 &lt;/bean&gt;14&lt;/beans&gt; 详细解释web.xmlweb.xml文件元素schemaSun公司定义的，在根元素中，都必须标明web.xml是用的拿个文件模式。其他元素放在内部比如： 1&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"2 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"3 xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\"4 version=\"3.1\"&gt;5&lt;/web-app&gt; icon、display、disciption、分别用来描述web应用的图标、名称和描述，基本没用过… 1&lt;icon&gt;2 &lt;small-icon&gt;/images/app_small.gif&lt;/small-icon&gt;3 &lt;large-icon&gt;/images/app_large.gif&lt;/large-icon&gt;4&lt;/icon&gt;5&lt;display-name&gt;Tomcat Example&lt;/display-name&gt;6&lt;disciption&gt;Tomcat Example servlets and JSP pages.&lt;/disciption&gt; context-param 上下文参数声明应用范围内的初始化参数。向ServletContext提供键值对，即应用程序上下文信息。listener，filter等在初始化的时候会用到这些上下文中的信息。在servlet了里面可以通过getServletContext().getInitParameter(&quot;cibtext/param&quot;)得到 1&lt;context-param&gt;2 &lt;param-name&gt;ContextParameter&lt;/para-name&gt;3 &lt;param-value&gt;test&lt;/param-value&gt;4 &lt;description&gt;It is a test parameter.&lt;/description&gt;5&lt;/context-param&gt; filter 过滤器将一个名字和一个实现javaxs.servlet.Filter接口的类相关联通过Filter技术，对web服务器管理的所有web资源：例如Jsp, Servlet, 静态图片文件或静态 html 文件等进行拦截，从而实现一些特殊的功能。例如实现URL级别的权限访问控制、过滤敏感词汇、压缩响应信息等一些高级功能。使用Filter 的完整流程：Filter 对用户请求进行预处理，接着将请求交给Servlet 进行处理并生成响应，最后Filter 再对服务器响应进行后处理。 功能 在HttpServletRequest 到达 Servlet 之前，拦截客户的 HttpServletRequest 。 根据需要检查 HttpServletRequest ，也可以修改HttpServletRequest 头和数据。 在HttpServletResponse 到达客户端之前，拦截HttpServletResponse 。 根据需要检查 HttpServletResponse ，也可以修改HttpServletResponse头和数据。 filter链在一个web应用中，可以开发编写多个Filter，这些Filter组合起来称之为一个Filter链。web服务器根据Filter在web.xml文件中的注册顺序，决定先调用哪个Filter，当第一个Filter的doFilter方法被调用时，web服务器会创建一个代表Filter链的FilterChain对象传递给该方法。在doFilter方法中，开发人员如果调用了FilterChain对象的doFilter方法，则web服务器会检查FilterChain对象中是否还有filter，如果有，则调用第2个filter，如果没有，则调用目标资源。 生命周期web服务器负责创建和销毁，应用程序启动的时候创建实例，并且调用init方法创建：public void init(FilterConfig filterConfig) throws ServletException;//初始化执行：public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException;//拦截请求销毁：public void destroy();//销毁 FilterConfig接口编写filter的时候可以通过filterConfig对象的方法获取filter初始化参数。 1String getFilterName();//得到filter的名称。2String getInitParameter(String name);//返回在部署描述中指定名称的初始化参数的值。如果不存在返回null.3Enumeration getInitParameterNames();//返回过滤器的所有初始化参数的名字的枚举集合。4public ServletContext getServletContext();//返回Servlet上下文对象的引用。 1&lt;filter&gt;2 &lt;filter-name&gt;setCharacterEncoding&lt;/filter-name&gt;3 &lt;filter-class&gt;com.myTest.setCharacterEncodingFilter&lt;/filter-class&gt;4 &lt;init-param&gt;&lt;!--通过FilterConfig类的getInitParameter(\"paramName\") --&gt;5 &lt;param-name&gt;encoding&lt;/param-name&gt;6 &lt;param-value&gt;UTF-8&lt;/param-value&gt;7 &lt;/init-param&gt;8&lt;/filter&gt;9&lt;filter-mapping&gt;10 &lt;filter-name&gt;setCharacterEncoding&lt;/filter-name&gt;11 &lt;url-pattern&gt;/*&lt;/url-pattern&gt;12 &lt;servlet-name&gt;指定过滤器所拦截的Servlet名称。&lt;/servlet-name&gt;13 &lt;dispatcher&gt;指定过滤器所拦截的资源被 Servlet 容器调用的方式，可以是REQUEST,INCLUDE,FORWARD和ERROR之一，默认REQUEST。用户可以设置多个dispatcher子元素用来指定 Filter 对资源的多种调用方式进行拦截。&lt;/dispatcher&gt;14&lt;/filter-mapping&gt; listener 监听器监听器Listener就是在application,session,request三个对象创建、销毁或者往其中添加修改删除属性时自动执行代码的功能组件。Listener是Servlet的监听器，可以监听客户端的请求，服务端的操作等。 1&lt;listener&gt;2 &lt;listener-class&gt;com.listener.class&lt;/listener-class&gt;3&lt;/listener&gt; ServletContext监听ServletContextListerner接口: 用于对Servlet整个上下文进行监听(创建、销毁) 1public void contextInitialized(ServletContextEvent sce);//上下文初始化2public void contextDestroyed(ServletContextEvent sce);//上下文销毁3public ServletContext getServletContext();//ServletContextEvent事件：取得一个ServletContext（application）对象 ServletContextAttributeListener接口：对Servlet上下文属性的监听(增删改属性) 1public void attributeAdded(ServletContextAttributeEvent scab);//增加属性2public void attributeRemoved(ServletContextAttributeEvent scab);//属性删除3public void attributeRepalced(ServletContextAttributeEvent scab);//属性替换（第二次设置同一属性）4//ServletContextAttributeEvent事件：能取得设置属性的名称与内容5public String getName();//得到属性名称6public Object getValue();//取得属性的值 Session监听Session属于http协议下的内容，接口位于javax.servlet.http.*包下。HttpSessionListener接口：对Session的整体状态的监听。 1public void sessionCreated(HttpSessionEvent se);//session创建2public void sessionDestroyed(HttpSessionEvent se);//session销毁3//HttpSessionEvent事件：4public HttpSession getSession();//取得当前操作的session HttpSessionAttributeListener接口：对session的属性监听。 1public void attributeAdded(HttpSessionBindingEvent se);//增加属性2public void attributeRemoved(HttpSessionBindingEvent se);//删除属性3public void attributeReplaced(HttpSessionBindingEvent se);//替换属性4//HttpSessionBindingEvent事件：5public String getName();//取得属性的名称6public Object getValue();//取得属性的值7public HttpSession getSession();//取得当前的session request监听ServletRequestListener： 用于Request请求进行监听(创建、销毁) 1public void requestInitialized(ServletRequestEvent sre);//request初始化2public void requestDestroyed(ServletRequestEvent sre);//request销毁3//ServletRequestEvent事件：4public ServletRequest getServletRequest();//取得一个ServletRequest对象5public ServletContext getServletContext();//取得一个ServletContext（application）对象 ServletRequestAttributeListener: 对Request属性的监听(增删改属性) 1public void attributeAdded(ServletRequestAttributeEvent srae);//增加属性2public void attributeRemoved(ServletRequestAttributeEvent srae);//属性删除3public void attributeReplaced(ServletRequestAttributeEvent srae);//属性替换（第二次设置同一属性）4//ServletRequestAttributeEvent事件：能取得设置属性的名称与内容5public String getName();//得到属性名称6public Object getValue();//取得属性的值 servletservlet标签只要有以下子元素： servlet-name指定servlet的名称 servlet-class制定servlet的类名称 jsp-file制定某个jsp网页的完整路径 init-param用来定义参数，可以有多个init-param。在servlet类中通过getInitParamenter(String name)方法初始化参数 load-on-startup 指定当Web应用启动的时候，装载Servlet的次序,当值为正数或零时：Servlet容器先加载数值小的servlet，再依次加载其他数值大的servlet。当值为负或未定义：Servlet容器将在Web客户首次访问这个servlet时加载它。 servlet-mapping 用来定义servlet所对应的URL包含两个子元素 servlet-name 指定servlet的名称 url-parttern 指定servlet所对应的URL1&lt;!-- 基本配置 --&gt;2&lt;servlet&gt;3 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;4 &lt;servlet-class&gt;SnoopServlet&lt;/servlet-class&gt;5&lt;/servlet&gt;6&lt;servlet-mapping&gt;7 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;8 &lt;url-pattern&gt;/snoop&lt;/url-pattern&gt;9&lt;/servlet-mapping&gt;10&lt;!-- 高级配置 --&gt;11&lt;servlet&gt;12 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;13 &lt;servlet-class&gt;SnoopServlet&lt;/servlet-class&gt;14 &lt;init-param&gt;15 &lt;param-name&gt;foo&lt;/param-name&gt;16 &lt;param-value&gt;bar&lt;/param-value&gt;17 &lt;/init-param&gt;18 &lt;run-as&gt;19 &lt;description&gt;Security role for anonymous access&lt;/description&gt;20 &lt;role-name&gt;tomcat&lt;/role-name&gt;21 &lt;/run-as&gt;22&lt;/servlet&gt;23&lt;servlet-mapping&gt;24 &lt;servlet-name&gt;snoop&lt;/servlet-name&gt;25 &lt;url-pattern&gt;/snoop&lt;/url-pattern&gt;26&lt;/servlet-mapping&gt; session-config 会话超时配置单位是分钟 1&lt;session-config&gt;2 &lt;session-timeout&gt;120&lt;/session-timeout&gt;3&lt;/session-config&gt; mime-mapping、welcome-file-list、error-page错误页面1&lt;mime-mapping&gt;2 &lt;extension&gt;htm&lt;/extension&gt;3 &lt;mime-type&gt;text/html&lt;/mime-type&gt;4&lt;/mime-mapping&gt;5&lt;welcome-file-list&gt;6 &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;7 &lt;welcome-file&gt;index.html&lt;/welcome-file&gt;8 &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt;9&lt;/welcome-file-list&gt;10&lt;error-page&gt;11 &lt;error-code&gt;404&lt;/error-code&gt;12 &lt;location&gt;/NotFound.jsp&lt;/location&gt;13&lt;/error-page&gt;14&lt;error-page&gt;15 &lt;exception-type&gt;java.lang.NullException&lt;/exception-type&gt;16 &lt;location&gt;/error.jsp&lt;/location&gt;17&lt;/error-page&gt; jsp-config 设置jsp1&lt;jsp-config&gt;2 &lt;taglib&gt;3 &lt;taglib-uri&gt;Taglib&lt;/taglib-uri&gt;&lt;!--jstl标签配置--&gt;4 &lt;taglib-location&gt;/WEB-INF/tlds/MyTaglib.tld&lt;/taglib-location&gt;5 &lt;/taglib&gt;6 &lt;jsp-property-group&gt;7 &lt;description&gt;Special property group for JSP Configuration JSP example.&lt;/description&gt;&lt;!--设定的说明--&gt;8 &lt;display-name&gt;JSPConfiguration&lt;/display-name&gt;9 &lt;url-pattern&gt;/jsp/* &lt;/url-pattern&gt;10 &lt;el-ignored&gt;true&lt;/el-ignored&gt;11 &lt;page-encoding&gt;GB2312&lt;/page-encoding&gt;12 &lt;scripting-invalid&gt;true&lt;/scripting-invalid&gt;13 &lt;include-prelude&gt;/include/prelude.jspf&lt;/include-prelude&gt;14 &lt;include-coda&gt;/include/coda.jspf&lt;/include-coda&gt;15 &lt;/jsp-property-group&gt;16&lt;/jsp-config&gt; 容器加载web.xml和启动过程当要启动某个j2ee项目的时候，服务器软件或者容器会第一步加载项目中的web.xml文件，通过其中的各种配置来启动项目，只有其中配置各项均无误的时候，项目才正确启动。web.xml有多响标签，在加载过程中顺序依次为：context-param &gt; listener &gt; filter &gt; servlet web.xml先读取context-param和listen这两个节点； 然后容器创建一个ServletContext，应用于整个项目； 将读取到的context-param转化为键值对并且存入servletContext 根据listener创建监听 容器读取，根据指定的类路径来实例化过滤器 项目初始化完成 发起第一次请求的时候，servlet节点被加载实例化。 参考：web.xml文件详解、Web启动过程及web.xml配置、web.xml官方文档、Filter过滤器、Listener监听器、JSP标准标签库","categories":[{"name":"Spring","slug":"Spring","permalink":"http://beritra.github.com/categories/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://beritra.github.com/tags/Spring/"},{"name":"framework","slug":"framework","permalink":"http://beritra.github.com/tags/framework/"}]},{"title":"Java关键字","slug":"Java关键字","date":"2017-04-05T13:33:59.000Z","updated":"2020-05-27T15:37:42.030Z","comments":true,"path":"2017/04/05/Java关键字/","link":"","permalink":"http://beritra.github.com/2017/04/05/Java%E5%85%B3%E9%94%AE%E5%AD%97/","excerpt":"Java里面的关键字和相关的知识点","text":"Java里面的关键字和相关的知识点 final 关键字final 可以声明成员变、方法、类和本地变量。一旦将引用声明为 final，这个引用就不能改变了。 final 变量凡是对成员变量或者本地变量(在方法中的或者代码块中的变量称为本地变量)声明为 final 的都叫作 final 变量。final 变量经常和 static 关键字一起使用，作为常量。 final 方法声明为 final 的方法不能被子类的方法重写。final 方法比非 final 方法要快，因为编译的时候已经静态绑定，不需要运行时再动态绑定。 final类final 修饰的类不能被继承。Java 中有 String，Integer 和其他包装类都是 final 类。 final关键字的好处 提高性能，JVM 和 Java 应用都会缓存final变量。 可以在安全的多线程环境下进行共享，不需要额外的同步开销。 使用 final 关键字，JVM 会对方法、变量和类进行优化。 不可变类不可变类是指对象一旦创建就不能更改，要使用 final 关键字创建。 为什么字符串类需要是不可变的？ 实现字符串池，节省 heap 空间。如果是可变的那么 String interning 就不能实现了，因为这样的话如果变量改变了他的值，那么这个值的变量的值也会一起改变。 安全问题：比如数据库的用户名密码都是字符串形式传入数据库的连接，可变的话就有风险被篡改。 多线程安全，字符串可以被多个线程共享。 类加载器要用到字符串。比如你想加载 java.sql.Connection 类，而这个值被改成了 myhacked.Connection，那么会对你的数据库造成不可知的破坏。 创建的时候 hashcode 就被缓存了，不需要重新计算。这使得字符串很适合作为Map中的键，字符串的处理速度要快过其他的键对象。 如何写一个不可变类其他要点 final 成员变量必须在声明的时候初始化或者在构造器中初始化，否则编译报错。 匿名类所有变量必须是 final 的。 接口中所有变量本身是 final 的 final 和 abstract 是冲突的，不能同时使用。 集合对象声明为 final，引用不能被更改，但是不影响增删改查内部元素。 (来源:深入理解Java中的final关键字、为什么String类是不可变的)","categories":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://beritra.github.com/tags/Java/"},{"name":"API","slug":"API","permalink":"http://beritra.github.com/tags/API/"}]},{"title":"遇到的小问题合集","slug":"遇到的小问题合集","date":"2017-03-17T04:12:19.000Z","updated":"2019-12-15T13:24:11.791Z","comments":true,"path":"2017/03/17/遇到的小问题合集/","link":"","permalink":"http://beritra.github.com/2017/03/17/%E9%81%87%E5%88%B0%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98%E5%90%88%E9%9B%86/","excerpt":"记录平时遇到的小问题和踩过的坑，当做备忘。","text":"记录平时遇到的小问题和踩过的坑，当做备忘。 框架Spring MVC表单传值中文乱码在web.xml中增加拦截器： 1&lt;filter&gt;2 &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;3 &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;4 &lt;init-param&gt;5 &lt;param-name&gt;encoding&lt;/param-name&gt;6 &lt;param-value&gt;UTF-8&lt;/param-value&gt;7 &lt;/init-param&gt;8 &lt;/filter&gt;9 &lt;filter-mapping&gt;10 &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;11 &lt;url-pattern&gt;/*&lt;/url-pattern&gt;12 &lt;/filter-mapping&gt; Spring MVC返回JSON数据只需要添加依赖： 1&lt;dependency&gt;2 &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;3 &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;4 &lt;version&gt;2.8.8&lt;/version&gt;5&lt;/dependency&gt; 其实试了好几个解析JSON数据的包，但是就这个行了，没搞明白咋回事。 Linux系统Docker免sudo使用docker： 1$sudo addgroup --system docker2$sudo adduser $USER docker3$newgrp docker 在容器中创建虚拟IP的时候，有可能遇到错误： 1IPVS: Can&#39;t initialize ipvs: Protocol not available 开启的必要条件有两个： 容器放开权限，即添加参数--privileged 宿主机同样需要开启ipvasdm 在容器中使用Systemctl命令经常会遇到提示： 1systemctl start fdfs_trckerd2Failed to get D-Bus connection: Operation not permitted 解决办法，用特权模式启动容器： 1docker run -d -name centos7 --privileged=true centos:7 /usr/sbin/init ssh设置免密码ssh登陆的时候，把公钥放到authorized_keys里面，重启ssh service都不生效，百度得知home目录，.ssh目录和authorized_keys文件需要分别为700,700,600权限，错一个都不行，更改权限后可以免密登陆了。 apt-get经常遇到apt-get遇到问题卡住之后，出错的地方常年在那摆着，无论干啥都提示一遍，神烦。 1cd &#x2F;var&#x2F;lib&#x2F;dpkg2sudo mv info info.bak3sudo mkdir info info删掉就搞定了，原理有待深入研究 数据库Linux下的MySQL安装顺序1rpm -ivh mysql-community-common-5.7.17-1.el7.x86_64.rpm 2rpm -ivh mysql-community-libs-5.7.17-1.el7.x86_64.rpm 3rpm -ivh mysql-community-client-5.7.17-1.el7.x86_64.rpm 4rpm -ivh mysql-community-server-5.7.17-1.el7.x86_64.rpm 5rpm -ivh mysql-community-devel-5.7.17-1.el7.x86_64.rpm 安装最后一个的时候有可能出现依赖openssl的问题，重新安装openssl无效，输入参数-e --nodeps可以解决，暂不清楚原理 MySQL 8修改用户密码命令1ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;新密码&#39;; 如果提示ERROR 1819 (HY000): Your password does not satisfy the current policy requirements 说明密码太简单，经过测试大小写英文字母+数字+标点符号可以满足要求。 IDEA和java中文字符问题运行Tomcat的时候，Messages Build出现了错误提示：然后中文因为字体的关系不能显示，这里由于实在是喜欢这个字体，不想换成很丑的中文，而且JVM的错误提示还是英文搜索起来比较方便，所以想换成中文。然后发现在终端输入Java命令得到的是中文：这两个应该是相关的。然后修改系统字体：发现可以显示成英文，但是目的是只是Java为英文，所以不想这样。尝试过的方法： 在Tomcat的VM option里面添加-Duser.language=en -Duser.country=US 在Setting-&gt;BUild,Execution,Deployment-&gt;Compiler中的两个地方添加-Duser.language=en -Duser.country=US 在IDEA的安装路径/bin/idea.vmoptions中添加-Duser.language=en -Duser.country=US 最后发现上面那个配置文件是32位程序的，64位程序不用这个….点击 Help-&gt;Edit Custom VM Option 会生成一个 64 位的配置文件，然后再添加-D 巴拉拉巴拉就行了….害得我V2EX号还被封了….. JavaFX窗口闪烁问题用JavaFX绘制窗口的时候，由于Scene填充成了比较深的颜色，所以每次窗口初始化和大小变化的时候，背景都会出现白色闪烁： 代码如下： 1public class Panel extends Application &#123;2 private Group root;3 private Scene scene;4 private static final Color BACKGROUND_COLOR = Color.rgb(0, 0, 0, 0.5);5 private Stage stage;67 @Override8 public void start(Stage primaryStage) throws Exception &#123;9 root = new Group();10 stage = primaryStage;11 primaryStage.initStyle(StageStyle.TRANSPARENT);12 scene = new Scene(root, 1000, 250, BACKGROUND_COLOR);13 Screen screen = Screen.getScreens().get(0);14 double windowHeight = screen.getBounds().getHeight();15 double windowWidth = screen.getBounds().getWidth();16 stage.setY(windowHeight + 1000);17 stage.setX(windowWidth - 1000);18 stage.setScene(scene);19 stage.show();20 Timer timer = new Timer();21 final int[] index = &#123;0&#125;;22 timer.schedule(new TimerTask() &#123;23 @Override24 public void run() &#123;25 System.out.println(\"111\");26 index[0]++;27 primaryStage.setY(primaryStage.getY() - 1);28 primaryStage.setHeight(primaryStage.getHeight() + 1);29 if (index[0] == 300) &#123;30 this.cancel();31 &#125;3233 &#125;34 &#125;, 1000, 1);35 &#125;36&#125; 浪费了大概一天的时间…查找各种文档，教学，试了好多方法还是不行。一开始以为我填充的区域不对，不是最底层的，后来以为是填充背景色时间不对，跟底层绘制时间差距太大导致出现闪烁，但是不能解决。最后还是在万能的Google上找到了一个向jdk开发人员反馈的帖子：White flashing when opening a stage with dark background原来是个Bug…窗口初始化的时候默认填充了白色。第一次碰到jdk级别的Bug还有点小激动。跟着帖子看到的修复信息：changeset 10131:bfe35c702696 jdk-9+145解决方法是取得了子节点的颜色再填充的，修复的版本是9…所以只好下了个预览版的jdk9最后要记住这个网站：https://bugs.openjdk.java.net，可以向jdk的开发组反馈遇到的问题～ 修改Maven中央仓库.m2文件夹下新建文件settings.xml 1&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"2 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"3 xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.04 https://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt;5&lt;mirrors&gt;6 &lt;mirror&gt;7 &lt;id&gt;alimaven&lt;/id&gt;8 &lt;name&gt;aliyun maven&lt;/name&gt;9 &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;10 &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;11 &lt;/mirror&gt;12&lt;/mirrors&gt;13&lt;/settings&gt; 其他设置参考官方文档 其他工具Hexo的git问题写完博客，上传到服务器的时候，出现了错误： 1FATAL Permission denied (publickey).2fatal: Could not read from remote repository.34Please make sure you have the correct access rights5and the repository exists.67Error: Permission denied (publickey).8fatal: Could not read from remote repository.910Please make sure you have the correct access rights11and the repository exists.1213 at ChildProcess.&lt;anonymous&gt; (&#x2F;home&#x2F;ray&#x2F;文档&#x2F;笔记&#x2F;Blogs&#x2F;node_modules&#x2F;hexo-util&#x2F;lib&#x2F;spawn.js文档&#x2F;笔记&#x2F;Blogs&#x2F;node_modules&#x2F;hexo-util&#x2F;lib&#x2F;spawn.js:37:17)14 at emitTwo (events.js:87:13)15 at ChildProcess.emit (events.js:172:7)16 at maybeClose (internal&#x2F;child_process.js:821:16)17 at Socket.&lt;anonymous&gt; (internal&#x2F;child_process.js:319:11)18 at emitOne (events.js:77:13)19 at Socket.emit (events.js:169:7)20 at Pipe._onclose (net.js:469:12) 试了试github能登上，也能push，把hexo的配置文件里面关于git的配置，ssh改成了https,就能成功上传代码了，原因不明。每次都得输入账号密码，这也不是办法。","categories":[{"name":"问题","slug":"问题","permalink":"http://beritra.github.com/categories/%E9%97%AE%E9%A2%98/"}],"tags":[{"name":"问题","slug":"问题","permalink":"http://beritra.github.com/tags/%E9%97%AE%E9%A2%98/"}]},{"title":"字符串匹配算法","slug":"字符串匹配算法","date":"2017-03-07T05:45:22.000Z","updated":"2019-12-15T13:17:57.414Z","comments":true,"path":"2017/03/07/字符串匹配算法/","link":"","permalink":"http://beritra.github.com/2017/03/07/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95/","excerpt":"刷题的时候遇到的几个与字符串匹配相关的算法。","text":"刷题的时候遇到的几个与字符串匹配相关的算法。 搜索子字符串KMP首先是比较出名的KMP算法 Sunday算法主要思想：匹配失败的时候关注的是参与匹配的最末位字符的下一个字符。这个字符如果没有出现在下面字符串里，就直接向右移动字符串长度+1位。如果出现，就移动字符串长度-出现位置位。比如，要在substring searching algorithm中搜索algorithm，先从头匹配：显然不符合，然后直接跳到最末位的下一个，空格没有出现在下面字符串里不符合，接着找下一个g出现了，在下标为2的位置，所以右移9-2=7位显然还是不行，然后再去找下一个，i出现了，在下标5的位置，所以右移4个经验证符合要求。java代码如下： 1public int searchStr(String patternString, String str) &#123;2 if (patternString.equals(\"\") || str.equals(\"\")||str.length()&gt;patternString.length())3 return -1;4 char[] pattern = patternString.toCharArray();5 char[] s = str.toCharArray();6 int strIndex;7 for (int i = 0; i &lt; pattern.length;) &#123;8 strIndex = 0;9 while (strIndex &lt; s.length) &#123;10 if(pattern[i + strIndex] != s[strIndex]) &#123;11 i+=strIndex;12 break;13 &#125;14 if(strIndex==s.length-1)&#123;15 return i;16 &#125;17 strIndex++;18 &#125;19 if (i + 1 &lt; pattern.length) &#123;20 int charIndex=findChar(s,pattern[i+s.length]);21 if(charIndex==-1)&#123;22 i+=s.length+1;23 &#125;else&#123;24 i+=s.length-charIndex;25 &#125;26 &#125; else &#123;27 return -1;28 &#125;29 &#125;30 return -1;31 &#125;32 private int findChar(char[] c,char search)&#123;33 for(int i=0;i&lt;c.length;i++)&#123;34 if(c[i]==search)35 return i;36 &#125;37 return -1;38 &#125; 最长回文子串暴力求解三层循环，外层遍历字符串，中层遍历子串，三层判定是否回文 中心扩散法先循环找到相邻且相等的，或者间隔一个且相等两个字符，然后向外扩展找到最长时间复杂度n² 1public String longestPalindrome(String s) &#123;2 // Write your code here3 if (s.equals(\"\") || s.length() == 1)4 return s;5 char[] result = &#123;&#125;;6 char[] charArray = s.toCharArray();7 for (int i = 0; i &lt; s.length(); i++) &#123;8 if (i + 1 &lt; s.length() &amp;&amp; charArray[i] == charArray[i + 1]) &#123;9 result = result.length &lt;= 2 ? Arrays.copyOfRange(charArray, i, i + 2) : result;10 for (int j = 0; j &lt;= s.length() / 2; j++) &#123;11 if (i - j+1 &gt; 0 &amp;&amp; i + j + 1 &lt; s.length() &amp;&amp; charArray[i - j] == charArray[i + j + 1]) &#123;12 result = result.length &lt; (2 * j + 2) ? Arrays.copyOfRange(charArray, i - j, i + j + 2) : result;13 &#125; else &#123;14 break;15 &#125;16 &#125;17 &#125;18 if (i + 2 &lt; s.length() &amp;&amp; charArray[i] == charArray[i + 2]) &#123;19 result = result.length &lt;= 3 ? Arrays.copyOfRange(charArray, i, i + 3) : result;20 for (int j = 0; j &lt;= s.length() / 2; j++) &#123;21 if (i - j+1 &gt; 0 &amp;&amp; i + j + 2 &lt; s.length() &amp;&amp; charArray[i - j] == charArray[i + j + 2]) &#123;22 result = result.length &lt; (2 * j + 3) ? Arrays.copyOfRange(charArray, i - j, i + j + 3) : result;23 &#125; else &#123;24 break;25 &#125;26 &#125;27 &#125;28 &#125;29 return new String(result);30 &#125; Manacher算法先对字符串预处理，每个字符中间添加特殊符号比如#然后引入数组P[i]代表以字符S[i]为中心的最长回文子串向左/右扩张的长度（包括S[i]）S[]ghaweuifhwe123454321fahwu-&gt;S[]#g#h#a#w#e#u#i#f#h#w#e#1#2#1#4#4#1#2#1#f#a#h#w#u#P[] 1212121212121212121212121412129212141212121212121可以看出P[i]-1就是原字符串中回文子串的长度，比如12144241中对应的9-1，121对应4-1，所以需要求P[]最大值","categories":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"字符串","slug":"字符串","permalink":"http://beritra.github.com/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"}]},{"title":"LintCode记录","slug":"LintCode记录","date":"2017-03-01T15:45:17.000Z","updated":"2019-12-15T14:27:22.264Z","comments":true,"path":"2017/03/01/LintCode记录/","link":"","permalink":"http://beritra.github.com/2017/03/01/LintCode%E8%AE%B0%E5%BD%95/","excerpt":"记录刷的那些麻烦的题","text":"记录刷的那些麻烦的题 二叉树的所有路径：1/**2 * Definition of TreeNode:3 * public class TreeNode &#123;4 * public int val;5 * public TreeNode left, right;6 * public TreeNode(int val) &#123;7 * this.val = val;8 * this.left = this.right = null;9 * &#125;10 * &#125;11 */12//递归解法：13public List binaryTreePaths(TreeNode root) &#123;14 List&lt;String&gt; leftList=new ArrayList&lt;&gt;();15 if(root==null)16 return leftList;17 if(root.left!=null)&#123;18 for(String list:binaryTreePaths(root.left))19 leftList.add(root.val+\"-&gt;\"+list);20 &#125;21 if(root.right!=null)&#123;22 for(String list:binaryTreePaths(root.right))23 leftList.add(root.val+\"-&gt;\"+list);24 &#125;25 if(root.right==null&amp;&amp;root.left==null)&#123;26 leftList.add(String.valueOf(root.val));27 &#125;28 return leftList;29&#125;3031//非递归解法：32public class Solution &#123;33 public List binaryTreePaths(TreeNode root) &#123;34 List list=new ArrayList&lt;&gt;();35 if(root==null)36 return list;37 boolean down=true;38 Stack stack=new Stack&lt;&gt;();39 Stack result=new Stack&lt;&gt;();40 TreeNode test=root;41 while(true)&#123;42 if(test.left!=null&amp;&amp;test.right!=null)&#123;43 if(down)&#123;44 if(test==root)&#123;45 result.push(String.valueOf(test.val));46 result.push(String.valueOf(test.val));47 &#125;48 else&#123;49 String str=result.pop();50 result.push(str+\"-&gt;\"+String.valueOf(test.val));51 result.push(str+\"-&gt;\"+String.valueOf(test.val));52 &#125;53 stack.push(test);54 test=test.left;55 down=true;56 &#125;else&#123;57 result.push(String.valueOf(result.pop()));58 test=test.right;59 down=true;60 &#125;61 &#125;else if(test.left!=null)&#123;62 if(test==root)&#123;63 result.push(String.valueOf(test.val));64 &#125;else65 result.push(result.pop()+\"-&gt;\"+String.valueOf(test.val));66 test=test.left;67 &#125;else if(test.right!=null)&#123;68 if(test==root)&#123;69 result.push(String.valueOf(test.val));70 &#125;else71 result.push(result.pop()+\"-&gt;\"+String.valueOf(test.val));72 test=test.right;73 &#125;else&#123;74 if(test==root)&#123;75 list.add(String.valueOf(test.val));76 &#125;else&#123;77 list.add(result.pop()+\"-&gt;\"+String.valueOf(test.val));78 &#125;79 if(stack.empty())80 break;81 test=stack.pop();82 down=false;83 &#125;84 &#125;85 return list;86 &#125;87&#125; 动态规划：上楼问题。一次一个或者两个台阶，上N层楼。1// 递归：消耗太大，简直变态2public class Solution &#123;3 /**4 * @param n: An integer5 * @return: An integer6 */7 public int climbStairs(int n) &#123;8 if(n==0)&#123;9 return 0;10 &#125;else if(n==1)&#123;11 return 1;12 &#125;else if(n==2)&#123;13 return 2;14 &#125;else&#123;15 // write your code here16 return climbStairs(n-1)+climbStairs(n-2);17 &#125;18 &#125;19&#125;2021// 非递归解法：22public class Solution &#123;23 /**24 * @param n: An integer25 * @return: An integer26 */27 public int climbStairs(int n) &#123;28 if(n==0)&#123;29 return 1;30 &#125;else if(n==1)&#123;31 return 1;32 &#125;else if(n==2)&#123;return 2;&#125;else&#123;33 int last=1;34 int swap;35 int result=2;36 for(int i=0;i&lt;n-2;i++)&#123;37 swap=result;38 result+=last;39 last=swap;40 &#125;41 return result;42 &#125;43 &#125;44&#125; 非递归的二叉树后序遍历应该还有更简洁的方式 1/**2 * Definition of TreeNode:3 * public class TreeNode &#123;4 * public int val;5 * public TreeNode left, right;6 * public TreeNode(int val) &#123;7 * this.val = val;8 * this.left = this.right = null;9 * &#125;10 * &#125;11 */12public class Solution &#123;13 /**14 * @param root: The root of binary tree.15 * @return: Postorder in ArrayList which contains node values.16 */17 public ArrayList&lt;Integer&gt; postorderTraversal(TreeNode root) &#123;18 ArrayList list=new ArrayList();19 TreeNode node=root;20 if(root==null)21 return list;22 Stack&lt;TreeNode&gt; stack=new Stack&lt;&gt;();23 boolean down=true;24 Stack&lt;TreeNode&gt; last=new Stack&lt;&gt;();25 last.push(new TreeNode(0));26 while(true)&#123;27 if(node.left!=null&amp;&amp;node.right!=null)&#123;28 if(down)&#123;29 stack.push(node);30 stack.push(node);31 node=node.left;32 down=true;33 &#125;else&#123;34 if(node==last.peek())&#123;35 list.add(node.val);36 if(stack.empty())37 break;38 node=stack.pop();39 last.pop();40 &#125;else&#123;41 last.push(node);42 node=node.right;43 down=true;44 &#125;45 &#125;46 &#125;47 else if(node.left!=null)&#123;48 if(down)&#123;49 stack.push(node);50 node=node.left;down=true;51 &#125;else&#123;52 list.add(node.val);53 if(stack.empty())54 break;55 node=stack.pop();56 &#125;57 &#125;else if(node.right!=null)&#123;58 if(down)&#123;59 stack.push(node);60 node=node.right;down=true;61 &#125;else&#123;62 list.add(node.val);63 if(stack.empty())64 break;65 node=stack.pop();66 &#125;67 &#125;else&#123;68 list.add(node.val);69 if(stack.empty())70 break;71 node=stack.pop();72 down=false;73 &#125;74 &#125;75 return list;76 &#125;77&#125; 二叉树的复制 非递归1public class Solution &#123;2 /**3 * @param root: The root of binary tree4 * @return root of new tree5 */6 public TreeNode cloneTree(TreeNode root) &#123;7 if(root==null)8 return null;9 TreeNode another=new TreeNode(root.val);10 TreeNode test=root;11 TreeNode test2=another;12 boolean down=true;13 Stack&lt;TreeNode&gt; stack1=new Stack&lt;&gt;();14 Stack&lt;TreeNode&gt; stack2=new Stack&lt;&gt;();15 while(true)&#123;16 if(test.left!=null&amp;&amp;test.right!=null)&#123;17 if(down)&#123;18 stack1.push(test);19 stack2.push(test2);20 test2.left=new TreeNode(test.left.val);21 test=test.left;22 test2=test2.left;23 continue;24 &#125;else&#123;25 test2.right=new TreeNode(test.right.val);26 test=test.right;27 test2=test2.right;28 down=true;29 continue;30 &#125;31 &#125;32 else if(test.left!=null)&#123;33 test2.left=new TreeNode(test.left.val);34 test=test.left;35 test2=test2.left;36 &#125;else if(test.right!=null)&#123;37 test2.right=new TreeNode(test.right.val);38 test=test.right;39 test2=test2.right;40 &#125;else&#123;41 if(stack1.empty())42 break;43 test=stack1.pop();44 test2=stack2.pop();45 down=false;46 &#125;47 &#125;48 return another;49 &#125;50&#125; 二分查找本来是很简单的问题，但是不知道position为啥要减一，回头再看. 1public int binarySearch(int[] nums, int target) &#123;2 if (nums == null)3 return -1;4 int position = nums.length;5 int[] temp = nums;67 while (true) &#123;8 int len = temp.length;9 if (len == 2) &#123;10 if (temp[0] == target)11 return position - 2;12 else if(temp[1] == target)13 return position-1;14 else15 return -1;16 &#125; else if (len == 1) &#123;17 if(temp[0]==target)18 return position-1;19 else20 return -1;21 &#125;22 len = len / 2;23 if (temp[len] &gt;= target) &#123;24 position = position-temp.length+len+1;25 temp = Arrays.copyOfRange(temp, 0, len + 1);26 &#125; else &#123;27 temp = Arrays.copyOfRange(temp, len + 1, temp.length);28 &#125;29 &#125; 二叉树的层次遍历一层一层的输出二叉树： 基本解法 1public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;2 // write your code here3 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result=new ArrayList&lt;&gt;();4 ArrayList&lt;Integer&gt; list=new ArrayList&lt;&gt;();5 if(root==null)6 return result;7 TreeNode currentNode=root;8 Queue queue=new LinkedList();9 int index=0;10 int index2=0;11 while(true)&#123;12 list.add(currentNode.val);13 if(currentNode.left!=null)&#123;14 queue.add(currentNode.left);15 index2++;16 &#125;17 if(currentNode.right!=null)&#123;18 queue.add(currentNode.right);19 index2++;20 &#125;21 if(index==0)&#123;22 index=index2;23 index2=0;24 result.add(new ArrayList(list));25 list.clear();26 if(queue.isEmpty())27 break;28 &#125;29 currentNode=(TreeNode)queue.remove();30 index--;31 &#125;32 return result;33 &#125; 二叉树的路径和我觉得我的智商有问题… 1public List&lt;List&lt;Integer&gt;&gt; binaryTreePathSum(TreeNode root, int target) &#123;2 TreeNode node = root;3 List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;();4 if (root == null)5 return list;6 if (target == node.val) &#123;7 if (node.left == null &amp;&amp; node.right == null) &#123;8 List l = new ArrayList();9 l.add(node.val);10 list.add(l);11 return list;12 &#125;13 &#125; else if (target &lt; node.val&amp;&amp;node.left == null &amp;&amp; node.right == null) &#123;14 return null;15 &#125;16 if (node.left != null) &#123;17 List l = binaryTreePathSum(node.left, target - node.val);18 if (l != null)19 for (int i = 0; i &lt; l.size(); i++) &#123;20 List ll = (List) l.get(i);21 List lll = new ArrayList();22 lll.add(node.val);23 lll.addAll(ll);24 list.add(lll);25 &#125;26 &#125;27 if (node.right != null) &#123;28 List l = binaryTreePathSum(node.right, target - node.val);29 if (l != null)30 for (int i = 0; i &lt; l.size(); i++) &#123;31 List ll = (List) l.get(i);32 List lll = new ArrayList();33 lll.add(node.val);34 lll.addAll(ll);35 list.add(lll);36 &#125;3738 &#125;39 return list;40 &#125; 几个出错了的点： 没考虑负数 new ArrayList(i)的参数i不是初始值，是initCapacity初始容量 二叉查找树的删除元素1public TreeNode removeNode(TreeNode root, int value) &#123;2 // write your code here3 if(root==null)4 return null;5 if(root.val==value)&#123;6 if(root.left!=null)&#123;7 TreeNode retuNode=root;8 root=root.left;9 TreeNode max=root;10 while(max.right!=null)&#123;11 max=max.right;12 &#125;13 max.right=retuNode.right;14 return root;15 &#125;else if(root.right!=null)&#123;16 root=root.right;17 return root;18 &#125;19 return null;20 &#125;else if(value&lt;root.val)&#123;21 root.left=removeNode(root.left,value);22 return root;23 &#125;else if(value&gt;root.val)&#123;24 root.right=removeNode(root.right,value);25 return root;26 &#125;27 return null;28 &#125; 最长无重复字符的子串这个简单，不过就是不知道使用了hashmap的话,时间复杂度应该怎么算 1public int lengthOfLongestSubstring(String s) &#123;2 // write your code here3 if (s.equals(\"\")) &#123;4 return 0;5 &#125;6 char[] c = s.toCharArray();7 int start = 0;8 int end = 0;9 int length = 1;10 HashSet set = new HashSet();11 while (end &lt; c.length) &#123;12 boolean added = set.add(c[end]);13 length = set.size() &gt; length ? set.size() : length;1415 if (!added) &#123;16 while (c[start] != c[end]) &#123;17 set.remove(c[start]);18 start++;19 &#125;20 start++;21 &#125;22 end++;23 &#125;24 return length;25 &#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"计算机基础","slug":"计算机基础","permalink":"http://beritra.github.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"数据结构","slug":"数据结构","permalink":"http://beritra.github.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"http://beritra.github.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"奇奇怪怪的面试题记录","slug":"奇奇怪怪的面试题记录","date":"2017-02-26T17:58:51.000Z","updated":"2019-12-15T13:21:44.299Z","comments":true,"path":"2017/02/27/奇奇怪怪的面试题记录/","link":"","permalink":"http://beritra.github.com/2017/02/27/%E5%A5%87%E5%A5%87%E6%80%AA%E6%80%AA%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"好几次碰到已经刷过的奇怪的题，还是想不起来，所以还是好记性不如烂笔头，都记下来面试之前看一遍好了。","text":"好几次碰到已经刷过的奇怪的题，还是想不起来，所以还是好记性不如烂笔头，都记下来面试之前看一遍好了。 语言特性不借助中间变量，交换字符串要用到三次亦或： 1int a=5555;2int b=6666;3a^=b;4b^=a;5a^=b;6System.out.println(a+\" \"+b); Java不能字符串的亦或操作，用数字表示了，好像c可以？？ Java类型转换今天犯了个错，char转int直接强转，然后得到了Unicode。可以Integer.parseInt(&#39;x&#39;+&quot;&quot;)转成string再转换或者Character的静态方法:Character.getNumericValue((int)&#39;5&#39;) 今天发现个好玩的事情，测试一个回文字符串算法，重复十亿次看时间，发现二十个字符和十个字符好时差别不大，四十个和二十个却有十倍的差距，我猜应该是虚拟机对段字符串有优化，留个坑改天详细研究。 Java语言基础final关键字static关键字静态变量同一个类在内存中只有同一个拷贝，可以用类名直接访问，也可以通过对象访问（不推荐） 静态方法可以通过类名调用，任何势力都可以调用，所以静态方法中不能有this和super关键字。静态方法必须被实现，不能是abstract。 静态代码块JVM加载类的时候按照出现的先后顺序执行，每个代码块只被执行一次。 静态内部类 一个内部类如果想要有静态的成员变量或者成员方法，那他本身必须是static关键字修饰的。 静态内部类只能访问外部的静态方法和变量，非静态的不能访问，这是静态内部类最大的使用限制，普通的非静态内部类没有这个限制。静态导包导入静态方法，简化操作，比如：1import static java.lang.System.out2out.println(\"text\"); 算法","categories":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"http://beritra.github.com/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"用github.pages和Hexo搭建个人博客","slug":"Article","date":"2017-02-24T13:26:06.000Z","updated":"2020-05-20T11:45:58.519Z","comments":true,"path":"2017/02/24/Article/","link":"","permalink":"http://beritra.github.com/2017/02/24/Article/","excerpt":"最近深感进步速度太慢，希望能养成写博客的习惯，不断总结学习。之前都是想在 Atom 上本地写写，后来用手机或者在别的地方看起来不方便，于是决定自己用比较现成的 github.pages+Hexo 解决，顺便玩玩 github。","text":"最近深感进步速度太慢，希望能养成写博客的习惯，不断总结学习。之前都是想在 Atom 上本地写写，后来用手机或者在别的地方看起来不方便，于是决定自己用比较现成的 github.pages+Hexo 解决，顺便玩玩 github。 基本的搭建过程网上教程一大把，不在赘述，也没啥技术含量，主要记录个人遇到的几个坑 Repository name 一定要是 username.github.id 生成ssh-key的时候，因为之前gitlab上用了一套，然后开始以为不能用同一个，就找了条命令：ssh-keygen -t rsa -C &quot;xxx@xxx.com&quot; -f ~/.ssh/github(后来发现默认的命令也会让你填写保存的文件名称)，然后发现没卵用…仍然是 permission denied 然后看到人说是 root 用户的问题，看了下自己用的，果然是 root 用户，马上切回到普通用户，生成一遍，还是不行… 再回头翻了翻 github 的帮助，发现可以不用生成新的 key： 1、Start the ssh-agent in the background.$ eval “$(ssh-agent -s)”$ Agent pid 595662、Add your SSH key to the ssh-agent. If you are using an existing SSH key rather than generating a new SSH key, you’ll need to replace id_rsa in the command with the name of your existing private key file.$ ssh-add ~/.ssh/id_rsa 终于搞定基本的连接，开始搭Hexo，deployer的时候,出现了错误:error deployer not found:git在 v2ex 上找到了解决办法：搭建 hexo，在执行 hexo deploy 后,出现 error deployer not found:github 的错误。 npm install hexo-deployer-git –save 改了之后执行，然后再部署试试 顺便安利下这个 v2ex 还是个挺不错的程序员社区 搞了大半夜折腾了个 Next.mist 主题…我发誓再也不做美化界面的事…","categories":[{"name":"Blog","slug":"Blog","permalink":"http://beritra.github.com/categories/Blog/"}],"tags":[{"name":"Github","slug":"Github","permalink":"http://beritra.github.com/tags/Github/"},{"name":"Blog","slug":"Blog","permalink":"http://beritra.github.com/tags/Blog/"},{"name":"Hexo","slug":"Hexo","permalink":"http://beritra.github.com/tags/Hexo/"}]}]}